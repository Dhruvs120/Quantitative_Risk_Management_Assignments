{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdedd50",
   "metadata": {},
   "source": [
    "### Initial package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from typing import Dict, List, Union\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from GARCH optimization)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79e1b1",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e703234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(indices_path='Data/Cleaned_Indices_Assignment1.csv', \n",
    "                          interest_rate_path='Data/ECB_Data_10yr_Treasury_bond.csv',\n",
    "                          start_date_str='2012-01-04'):\n",
    "    \"\"\"Loads, merges, cleans, and prepares the initial DataFrame.\"\"\"\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    # Read the data\n",
    "    main_df = pd.read_csv(indices_path, sep=';')\n",
    "    interest_rate_bond_df = pd.read_csv(interest_rate_path, sep=',')\n",
    "\n",
    "    # Convert date columns to datetime format\n",
    "    main_df['Date'] = pd.to_datetime(main_df['Date'], format='%d-%m-%Y')\n",
    "    interest_rate_bond_df['Date'] = pd.to_datetime(interest_rate_bond_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Merge dataframes\n",
    "    main_df = pd.merge(main_df, interest_rate_bond_df, on='Date', how='left')\n",
    "\n",
    "    # Clean data\n",
    "    main_df = main_df.dropna(axis=0, subset=['Yield curve spot rate, 10-year maturity - Government bond'])\n",
    "    main_df = main_df[main_df['Date'] >= start_date_str]\n",
    "\n",
    "    # Set Date as index and sort\n",
    "    main_df = main_df.set_index('Date')\n",
    "    main_df = main_df.sort_index()\n",
    "    \n",
    "    # Calculate initial returns (needed before portfolio metrics)\n",
    "    for col in ['S&P500_Closing', 'Dax40_Closing', 'Nikkei_Closing']:\n",
    "        main_df[f'C_{col.replace(\"_Closing\", \"\")}_Returns'] = main_df[col].pct_change()\n",
    "        # Fill the first NaN return with 0\n",
    "        main_df[f'C_{col.replace(\"_Closing\", \"\")}_Returns'].iloc[0] = 0.0\n",
    "        \n",
    "    return main_df\n",
    "\n",
    "# Load data initially\n",
    "main_df_initial = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8521a68",
   "metadata": {},
   "source": [
    "## Portfolio Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb880cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio weights and initial investment\n",
    "weights_dict = {\n",
    "    'S&P500': 0.4,\n",
    "    'DAX40': 0.3,\n",
    "    'NIKKEI': 0.15,\n",
    "    'EU-BOND': 0.15,\n",
    "}\n",
    "weights_array = np.array([weights_dict['S&P500'], weights_dict['DAX40'], weights_dict['NIKKEI'], weights_dict['EU-BOND']])\n",
    "\n",
    "starting_investment = 10000000  # 10 million euros\n",
    "interest_bond_initial = starting_investment * weights_dict['EU-BOND'] # Initial bond value\n",
    "start_date = pd.to_datetime('2012-01-04')\n",
    "\n",
    "def get_initial_invested_amounts(df, starting_investment, weights_dict, start_date):\n",
    "    \"\"\"Calculates initial amounts invested in each asset's currency.\"\"\"\n",
    "    starting_row = df.loc[start_date]\n",
    "    usd_to_eur = float(starting_row['USD/EUR'])\n",
    "    jpy_to_eur = float(starting_row['JPY/EUR'])\n",
    "    \n",
    "    invested_amount_SP500 = starting_investment * weights_dict['S&P500'] / usd_to_eur\n",
    "    invested_amount_DAX40 = starting_investment * weights_dict['DAX40']\n",
    "    invested_amount_NIKKEI = starting_investment * weights_dict['NIKKEI'] / jpy_to_eur\n",
    "    invested_amount_EU_BOND = starting_investment * weights_dict['EU-BOND']\n",
    "    \n",
    "    return [invested_amount_SP500, invested_amount_DAX40, invested_amount_NIKKEI, invested_amount_EU_BOND]\n",
    "\n",
    "invested_amounts_initial = get_initial_invested_amounts(main_df_initial, starting_investment, weights_dict, start_date)\n",
    "print(\"Initial Invested Amounts (Local Currency):\")\n",
    "print(invested_amounts_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7900041",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f32de1",
   "metadata": {},
   "source": [
    "### Portfolio Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bond_metrics(df, interest_bond_initial):\n",
    "    \"\"\"Calculates daily bond value, profit/loss, and daily rate.\"\"\"\n",
    "    df_copy = df.copy() # Work on a copy\n",
    "    days_per_annum = 365\n",
    "    \n",
    "    interest_bond_vector = np.zeros(len(df_copy))\n",
    "    interest_bond_profit_vector = np.zeros(len(df_copy))\n",
    "    interest_bond_loss_vector = np.zeros(len(df_copy))\n",
    "    daily_rates = np.zeros(len(df_copy))\n",
    "    \n",
    "    # Find the index corresponding to the start date\n",
    "    start_idx_loc = df_copy.index.get_loc(start_date)\n",
    "    \n",
    "    # Set initial value at the correct starting index\n",
    "    if start_idx_loc < len(df_copy):\n",
    "         interest_bond_vector[start_idx_loc] = interest_bond_initial\n",
    "    else:\n",
    "         print(\"Warning: Start date not found in DataFrame index for bond calculation.\")\n",
    "         return df_copy # Return original if start date is wrong\n",
    "\n",
    "    # Calculate bond values day by day\n",
    "    for i in range(len(df_copy)):\n",
    "        # Adding 1.5% credit risk spread, adjust rate for weekends/holidays (7/5)\n",
    "        daily_rate = (((df_copy['Yield curve spot rate, 10-year maturity - Government bond'].iloc[i] + 1.5) / days_per_annum) * (7/5)) / 100\n",
    "        daily_rates[i] = daily_rate\n",
    "        \n",
    "        # Calculate current value based on previous day, starting from day after start_date\n",
    "        if i > start_idx_loc:\n",
    "            previous_value = interest_bond_vector[i-1]\n",
    "            current_value = previous_value * (1 + daily_rate)\n",
    "            interest_bond_vector[i] = current_value\n",
    "            \n",
    "            change = current_value - previous_value\n",
    "            interest_bond_profit_vector[i] = change\n",
    "            interest_bond_loss_vector[i] = -change\n",
    "        elif i < start_idx_loc:\n",
    "             # Handle days before the start date if necessary (e.g., set to NaN or 0)\n",
    "             interest_bond_vector[i] = np.nan # Or 0, depending on desired behavior\n",
    "             interest_bond_profit_vector[i] = np.nan\n",
    "             interest_bond_loss_vector[i] = np.nan\n",
    "             daily_rates[i] = np.nan\n",
    "             \n",
    "    # Add vectors to the dataframe\n",
    "    df_copy['Interest_Bond'] = interest_bond_vector\n",
    "    df_copy['Interest_Bond_Profit'] = interest_bond_profit_vector\n",
    "    df_copy['Interest_Bond_Loss'] = interest_bond_loss_vector\n",
    "    df_copy['Interest_Bond_daily_rate'] = daily_rates\n",
    "    \n",
    "    # Fill NaNs before start date if they exist\n",
    "    df_copy.fillna(method='bfill', inplace=True) # Backfill might be appropriate here\n",
    "    df_copy.fillna(0.0, inplace=True) # Fill any remaining NaNs with 0\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c852dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_investment_values(df, invested_amounts, start_date):\n",
    "    \"\"\"Calculates the daily value of each equity investment based on returns.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    invested_amount_SP500, invested_amount_DAX40, invested_amount_NIKKEI, _ = invested_amounts\n",
    "    \n",
    "    # Initialize columns\n",
    "    df_copy['SP500_Investment'] = np.nan\n",
    "    df_copy['DAX40_Investment'] = np.nan\n",
    "    df_copy['NIKKEI_Investment'] = np.nan\n",
    "\n",
    "    # Set initial investment values at the start date\n",
    "    if start_date in df_copy.index:\n",
    "        df_copy.loc[start_date, 'SP500_Investment'] = invested_amount_SP500\n",
    "        df_copy.loc[start_date, 'DAX40_Investment'] = invested_amount_DAX40\n",
    "        df_copy.loc[start_date, 'NIKKEI_Investment'] = invested_amount_NIKKEI\n",
    "    else:\n",
    "        print(\"Warning: Start date not found for setting initial investment values.\")\n",
    "        return df_copy\n",
    "\n",
    "    # Calculate daily investment values using pct_change (returns)\n",
    "    # Ensure returns columns exist and handle potential NaNs\n",
    "    equity_returns_cols = ['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns']\n",
    "    investment_cols = ['SP500_Investment', 'DAX40_Investment', 'NIKKEI_Investment']\n",
    "    \n",
    "    for ret_col, inv_col in zip(equity_returns_cols, investment_cols):\n",
    "        if ret_col not in df_copy.columns:\n",
    "             print(f\"Warning: Return column {ret_col} not found. Skipping {inv_col}.\")\n",
    "             continue\n",
    "             \n",
    "        # Forward fill initial investment value to allow calculation\n",
    "        # df_copy[inv_col] = df_copy[inv_col].ffill() # Careful with ffill if data gaps exist\n",
    "        \n",
    "        # Calculate using cumulative product of (1 + return)\n",
    "        # Find the index location of the start date\n",
    "        start_idx_loc = df_copy.index.get_loc(start_date)\n",
    "        initial_value = df_copy.loc[start_date, inv_col]\n",
    "        \n",
    "        # Calculate cumulative returns factor starting from the day after start_date\n",
    "        cumulative_factor = (1 + df_copy[ret_col].iloc[start_idx_loc+1:]).cumprod()\n",
    "        \n",
    "        # Apply the cumulative factor to the initial investment value\n",
    "        df_copy.loc[df_copy.index[start_idx_loc+1:], inv_col] = initial_value * cumulative_factor\n",
    "        \n",
    "        # Handle potential NaNs if returns had NaNs\n",
    "        df_copy[inv_col].fillna(method='ffill', inplace=True) # Forward fill gaps\n",
    "        df_copy[inv_col].fillna(0.0, inplace=True) # Fill remaining NaNs\n",
    "        \n",
    "    # EU Government Bond value is taken from the 'Interest_Bond' column\n",
    "    if 'Interest_Bond' in df_copy.columns:\n",
    "        df_copy['EU_BOND_Investment'] = df_copy['Interest_Bond']\n",
    "    else:\n",
    "        print(\"Warning: 'Interest_Bond' column not found for EU_BOND_Investment.\")\n",
    "        df_copy['EU_BOND_Investment'] = np.nan\n",
    "        df_copy['EU_BOND_Investment'].fillna(method='ffill', inplace=True)\n",
    "        df_copy['EU_BOND_Investment'].fillna(0.0, inplace=True)\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7268a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_portfolio_metrics(df, starting_investment, start_date):\n",
    "    \"\"\"Calculates total portfolio value, daily change, loss, and returns.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Ensure required columns exist, fillna just in case\n",
    "    required_cols = ['SP500_Investment', 'USD/EUR', 'DAX40_Investment', \n",
    "                     'NIKKEI_Investment', 'JPY/EUR', 'EU_BOND_Investment']\n",
    "    for col in required_cols:\n",
    "        if col not in df_copy.columns:\n",
    "             print(f\"Warning: Required column {col} missing for portfolio metrics.\")\n",
    "             df_copy[col] = 0.0 # Assign default value or handle appropriately\n",
    "        else:\n",
    "             df_copy[col] = df_copy[col].fillna(0.0)\n",
    "             \n",
    "    # Calculate total portfolio value in EUR\n",
    "    df_copy['Portfolio_Value_EUR'] = (\n",
    "        df_copy['SP500_Investment'] * df_copy['USD/EUR'] +\n",
    "        df_copy['DAX40_Investment'] +\n",
    "        df_copy['NIKKEI_Investment'] * df_copy['JPY/EUR'] +\n",
    "        df_copy['EU_BOND_Investment']\n",
    "    )\n",
    "\n",
    "    # Set the first day's value to the initial investment\n",
    "    if start_date in df_copy.index:\n",
    "        df_copy.loc[start_date, 'Portfolio_Value_EUR'] = starting_investment\n",
    "    \n",
    "    # Calculate daily change, loss, and returns\n",
    "    df_copy['Portfolio_Change_EUR'] = df_copy['Portfolio_Value_EUR'].diff()\n",
    "    df_copy['Portfolio_loss'] = -df_copy['Portfolio_Change_EUR']\n",
    "    df_copy['Portfolio_Daily_Returns'] = df_copy['Portfolio_Value_EUR'].pct_change()\n",
    "\n",
    "    # Set the first day's change, loss, and return to 0\n",
    "    if start_date in df_copy.index:\n",
    "        df_copy.loc[start_date, ['Portfolio_Change_EUR', 'Portfolio_loss', 'Portfolio_Daily_Returns']] = 0.0\n",
    "        \n",
    "    # Fill any NaNs created by diff/pct_change (e.g., first row)\n",
    "    df_copy.fillna({'Portfolio_Change_EUR': 0.0, 'Portfolio_loss': 0.0, 'Portfolio_Daily_Returns': 0.0}, inplace=True)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e112e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_dependent_columns(df, starting_investment, weights_dict, start_date, interest_bond_initial):\n",
    "    \"\"\"Recalculates all dependent columns after a stress event.\"\"\"\n",
    "    df_recalc = df.copy()\n",
    "    \n",
    "    # 1. Recalculate Returns based on potentially stressed closing prices\n",
    "    print(\"Recalculating returns...\")\n",
    "    for col in ['S&P500_Closing', 'Dax40_Closing', 'Nikkei_Closing']:\n",
    "        return_col = f'C_{col.replace(\"_Closing\", \"\")}_Returns'\n",
    "        if col in df_recalc.columns:\n",
    "            df_recalc[return_col] = df_recalc[col].pct_change()\n",
    "            # Handle the first NaN value\n",
    "            if not df_recalc.empty:\n",
    "                 df_recalc[return_col].iloc[0] = 0.0\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found for return recalculation.\")\n",
    "            \n",
    "    # 2. Recalculate Bond Metrics based on potentially stressed yield\n",
    "    print(\"Recalculating bond metrics...\")\n",
    "    df_recalc = calculate_bond_metrics(df_recalc, interest_bond_initial)\n",
    "    \n",
    "    # 3. Recalculate Investment Values based on new returns and bond value\n",
    "    print(\"Recalculating investment values...\")\n",
    "    # Need initial invested amounts in local currency\n",
    "    invested_amounts = get_initial_invested_amounts(df_recalc, starting_investment, weights_dict, start_date)\n",
    "    df_recalc = calculate_investment_values(df_recalc, invested_amounts, start_date)\n",
    "    \n",
    "    # 4. Recalculate Portfolio Metrics based on new investment values and exchange rates\n",
    "    print(\"Recalculating portfolio metrics...\")\n",
    "    df_recalc = calculate_portfolio_metrics(df_recalc, starting_investment, start_date)\n",
    "    \n",
    "    print(\"Recalculation complete.\")\n",
    "    return df_recalc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cb138",
   "metadata": {},
   "source": [
    "### VaR and ES Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Get the VaR of the normal or student-t model.\n",
    "    Assumes VaR is for LOSSES (positive value).\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        # Normal distribution: VaR = mu + sigma * Z_alpha\n",
    "        # Since we model losses, VaR = E[Loss] + std(Loss) * Z_alpha\n",
    "        dVaR0 = st.norm.ppf(alpha)\n",
    "        dVaR = r + s*dVaR0\n",
    "    else:\n",
    "        # Student-t distribution\n",
    "        dVaR0 = st.t.ppf(alpha, df= df)\n",
    "        # Scale factor to match volatility\n",
    "        dS2t = df/(df-2) # Variance of standard t-distribution\n",
    "        if dS2t <= 0: # Handle df <= 2\n",
    "             return np.nan\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dVaR = r + c*dVaR0\n",
    "    return dVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d781bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Get the ES of the normal/student model for LOSSES.\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        # Normal distribution: ES = mu + sigma * pdf(Z_alpha) / (1-alpha)\n",
    "        dVaR0 = st.norm.ppf(alpha)\n",
    "        dES0 = st.norm.pdf(dVaR0) / (1-alpha)\n",
    "        dES = r + s*dES0\n",
    "    else:\n",
    "        # Student-t distribution\n",
    "        dVaR0 = st.t.ppf(alpha, df= df)\n",
    "        # ES formula for t-distribution\n",
    "        if df <= 1: # ES not defined for df=1\n",
    "             return np.nan\n",
    "        dES0 = st.t.pdf(dVaR0, df= df)*((df + dVaR0**2)/(df-1)) / (1-alpha)\n",
    "        # Scale factor\n",
    "        if df <= 2: # Variance not defined for df<=2\n",
    "             return np.nan\n",
    "        dS2t = df/(df-2)\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dES = r + c*dES0\n",
    "    return dES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83191a",
   "metadata": {},
   "source": [
    "### Risk Calculation Method Functions (Historical, Var/Cov, Multi-day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b72468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_loss_variables(time_window, current_date):\n",
    "    # Calculate the mean and standard deviation of portfolio loss from the time windows\n",
    "    loss_dict = {\n",
    "        \"Date\": current_date,\n",
    "        \"Portfolio_mean_loss\": np.nanmean(time_window['Portfolio_loss']),\n",
    "        \"Portfolio_std_loss\": np.nanstd(time_window['Portfolio_loss'])\n",
    "    }\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var_cov(window, current_date, vAlpha, mean_loss, portfolio_std_loss, df=0):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk and ES using variance-covariance method.\n",
    "    Returns VaR/ES for LOSSES.\n",
    "    \"\"\"\n",
    "    var_results = []\n",
    "    es_results = []\n",
    "    for alpha in vAlpha:\n",
    "        var_results.append(VaR(alpha, mean_loss, portfolio_std_loss, df=df))\n",
    "        es_results.append(ES(alpha, mean_loss, portfolio_std_loss, df=df))\n",
    "    \n",
    "    # Set label for distribution type\n",
    "    if df == 0:\n",
    "        dist_label = \"Normal\"\n",
    "    else:\n",
    "        dist_label = f\"T{df}\"\n",
    "        \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        f'VaR {dist_label}': np.array(var_results),\n",
    "        f'ES {dist_label}': np.array(es_results)\n",
    "    }\n",
    "\n",
    "def calculate_historical_var_es(window, current_date, vAlpha):\n",
    "    \"\"\"\n",
    "    Calculate VaR and ES using historical simulation method for LOSSES.\n",
    "    \"\"\"\n",
    "    # Extract portfolio loss values from the window\n",
    "    historical_losses = window['Portfolio_loss'].dropna()\n",
    "    \n",
    "    if len(historical_losses) == 0:\n",
    "         return {\n",
    "            'Date': current_date,\n",
    "            'VaR Historical': np.full(len(vAlpha), np.nan),\n",
    "            'ES Historical': np.full(len(vAlpha), np.nan)\n",
    "        }\n",
    "        \n",
    "    # Sort losses in ascending order (higher losses are larger positive numbers)\n",
    "    sorted_losses = np.sort(historical_losses)\n",
    "    \n",
    "    # Calculate VaR for alpha levels\n",
    "    var_hist = np.percentile(sorted_losses, vAlpha * 100)\n",
    "    \n",
    "    # Calculate ES for each alpha level\n",
    "    es_hist = []\n",
    "    for i, alpha in enumerate(vAlpha):\n",
    "        # ES is the mean of losses greater than or equal to VaR\n",
    "        losses_above_var = sorted_losses[sorted_losses >= var_hist[i]]\n",
    "        es_val = losses_above_var.mean() if len(losses_above_var) > 0 else np.nan # Handle case where no losses >= VaR\n",
    "        es_hist.append(es_val)\n",
    "    \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        'VaR Historical': np.array(var_hist),\n",
    "        'ES Historical': np.array(es_hist)\n",
    "    }\n",
    "\n",
    "def calculate_multiday_risk(main_df_indexed, vAlpha, interval, sample_size):\n",
    "    \"\"\"\n",
    "    Calculate multi-day VaR and ES using the historical simulation method.\n",
    "    Returns VaR/ES for LOSSES.\n",
    "    \n",
    "    Parameters:\n",
    "    - main_df_indexed: DataFrame with DatetimeIndex\n",
    "    - vAlpha: Confidence levels (array)\n",
    "    - interval: Number of days for the multi-day calculation (e.g., 5 or 10)\n",
    "    - sample_size: Rolling window size for daily VaR calculation (used for sqrt rule)\n",
    "    \n",
    "    Returns:\n",
    "    - var_multi_df, es_multi_df: DataFrames with multi-day VaR and ES\n",
    "    \"\"\"\n",
    "    if sample_size >= len(main_df_indexed):\n",
    "        print(f\"Warning: sample_size ({sample_size}) >= data length ({len(main_df_indexed)}). Cannot calculate multi-day risk.\")\n",
    "        empty_df = pd.DataFrame(index=main_df_indexed.index)\n",
    "        for alpha in vAlpha:\n",
    "             empty_df[f'VaR_{interval}d_Hist_Reg_{int(alpha*100)}'] = np.nan\n",
    "             empty_df[f'VaR_{interval}d_Hist_Sqrt_{int(alpha*100)}'] = np.nan\n",
    "             empty_df[f'ES_{interval}d_Hist_Reg_{int(alpha*100)}'] = np.nan\n",
    "             empty_df[f'ES_{interval}d_Hist_Sqrt_{int(alpha*100)}'] = np.nan\n",
    "        empty_df[f'Actual_Loss_{interval}d'] = np.nan\n",
    "        return empty_df, empty_df.copy()\n",
    "        \n",
    "    # Filter data for the period we want to analyze (excluding initial sample)\n",
    "    analysis_start_date = main_df_indexed.index[sample_size]\n",
    "    time_window_multi = main_df_indexed[main_df_indexed.index >= analysis_start_date].copy()\n",
    "    \n",
    "    # Calculate rolling sum of losses over the interval\n",
    "    loss_col = f'Portfolio_loss_{interval}d'\n",
    "    time_window_multi[loss_col] = time_window_multi['Portfolio_loss'].rolling(window=interval).sum()\n",
    "    \n",
    "    # Drop NaNs created by rolling sum\n",
    "    multi_day_losses_df = time_window_multi.dropna(subset=[loss_col]).copy()\n",
    "    \n",
    "    if multi_day_losses_df.empty:\n",
    "        print(f\"Warning: No valid {interval}-day losses found after rolling sum and dropna.\")\n",
    "        # Return empty DataFrames with expected columns\n",
    "        empty_df = pd.DataFrame(index=main_df_indexed.index[sample_size:])\n",
    "        for alpha in vAlpha:\n",
    "             empty_df[f'VaR_{interval}d_Hist_Reg_{int(alpha*100)}'] = np.nan\n",
    "             empty_df[f'VaR_{interval}d_Hist_Sqrt_{int(alpha*100)}'] = np.nan\n",
    "             empty_df[f'ES_{interval}d_Hist_Reg_{int(alpha*100)}'] = np.nan\n",
    "             empty_df[f'ES_{interval}d_Hist_Sqrt_{int(alpha*100)}'] = np.nan\n",
    "        empty_df[f'Actual_Loss_{interval}d'] = np.nan\n",
    "        return empty_df, empty_df.copy()\n",
    "        \n",
    "    # --- Historical Multi-Day VaR/ES (Regular Method) ---\n",
    "    var_reg_list = []\n",
    "    es_reg_list = []\n",
    "    # Use expanding window for multi-day historical simulation\n",
    "    for i in range(1, len(multi_day_losses_df) + 1):\n",
    "        current_losses = multi_day_losses_df[loss_col].iloc[:i]\n",
    "        if current_losses.empty:\n",
    "             var_vals = np.full(len(vAlpha), np.nan)\n",
    "             es_vals = np.full(len(vAlpha), np.nan)\n",
    "        else:\n",
    "            sorted_losses = np.sort(current_losses.dropna())\n",
    "            if len(sorted_losses) == 0:\n",
    "                 var_vals = np.full(len(vAlpha), np.nan)\n",
    "                 es_vals = np.full(len(vAlpha), np.nan)\n",
    "            else:\n",
    "                var_vals = np.percentile(sorted_losses, vAlpha * 100)\n",
    "                es_vals = []\n",
    "                for j, alpha in enumerate(vAlpha):\n",
    "                    losses_above_var = sorted_losses[sorted_losses >= var_vals[j]]\n",
    "                    es_val = losses_above_var.mean() if len(losses_above_var) > 0 else np.nan\n",
    "                    es_vals.append(es_val)\n",
    "        var_reg_list.append(var_vals)\n",
    "        es_reg_list.append(es_vals)\n",
    "        \n",
    "    # Add results to DataFrame\n",
    "    for k, alpha in enumerate(vAlpha):\n",
    "        multi_day_losses_df[f'VaR_{interval}d_Hist_Reg_{int(alpha*100)}'] = [res[k] for res in var_reg_list]\n",
    "        multi_day_losses_df[f'ES_{interval}d_Hist_Reg_{int(alpha*100)}'] = [res[k] for res in es_reg_list]\n",
    "\n",
    "    # --- Historical Multi-Day VaR/ES (Sqrt Rule) ---\n",
    "    # Calculate rolling 1-day historical VaR/ES first\n",
    "    daily_var_hist = []\n",
    "    daily_es_hist = []\n",
    "    daily_dates = []\n",
    "    for i in range(sample_size, len(main_df_indexed)):\n",
    "        window = main_df_indexed.iloc[i - sample_size:i]\n",
    "        current_date = main_df_indexed.index[i]\n",
    "        hist_res = calculate_historical_var_es(window, current_date, vAlpha)\n",
    "        daily_var_hist.append(hist_res['VaR Historical'])\n",
    "        daily_es_hist.append(hist_res['ES Historical'])\n",
    "        daily_dates.append(current_date)\n",
    "        \n",
    "    daily_risk_dict = {}\n",
    "    for k, alpha in enumerate(vAlpha):\n",
    "        daily_risk_dict[f'VaR_1d_Hist_{int(alpha*100)}'] = [res[k] for res in daily_var_hist]\n",
    "        daily_risk_dict[f'ES_1d_Hist_{int(alpha*100)}'] = [res[k] for res in daily_es_hist]\n",
    "        \n",
    "    daily_risk_df = pd.DataFrame(daily_risk_dict, index=pd.Index(daily_dates, name='Date'))\n",
    "\n",
    "    # Merge daily risk with multi-day losses df\n",
    "    multi_day_losses_df = multi_day_losses_df.merge(daily_risk_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Apply sqrt rule\n",
    "    for alpha in vAlpha:\n",
    "        alpha_perc = int(alpha*100)\n",
    "        multi_day_losses_df[f'VaR_{interval}d_Hist_Sqrt_{alpha_perc}'] = multi_day_losses_df[f'VaR_1d_Hist_{alpha_perc}'] * np.sqrt(interval)\n",
    "        multi_day_losses_df[f'ES_{interval}d_Hist_Sqrt_{alpha_perc}'] = multi_day_losses_df[f'ES_1d_Hist_{alpha_perc}'] * np.sqrt(interval)\n",
    "\n",
    "    # Prepare output DataFrames\n",
    "    var_cols = [f'VaR_{interval}d_Hist_Reg_{int(a*100)}' for a in vAlpha] + \\\n",
    "               [f'VaR_{interval}d_Hist_Sqrt_{int(a*100)}' for a in vAlpha]\n",
    "    es_cols = [f'ES_{interval}d_Hist_Reg_{int(a*100)}' for a in vAlpha] + \\\n",
    "              [f'ES_{interval}d_Hist_Sqrt_{int(a*100)}' for a in vAlpha]\n",
    "    \n",
    "    var_multi_df = multi_day_losses_df[var_cols].copy()\n",
    "    es_multi_df = multi_day_losses_df[es_cols].copy()\n",
    "    # Add the actual loss column for plotting/backtesting\n",
    "    var_multi_df[f'Actual_Loss_{interval}d'] = multi_day_losses_df[loss_col]\n",
    "    es_multi_df[f'Actual_Loss_{interval}d'] = multi_day_losses_df[loss_col]\n",
    "\n",
    "    return var_multi_df, es_multi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bde6f",
   "metadata": {},
   "source": [
    "### Risk Calculation Method Functions (EWMA, FHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ad7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewma_volatility(\n",
    "    returns: pd.DataFrame, \n",
    "    lambdas: List[float] = [0.94, 0.97]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute EWMA volatility for each risk factor using different lambda values.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns, pd.DataFrame):\n",
    "        raise TypeError(\"returns must be a pandas DataFrame\")\n",
    "    \n",
    "    # Ensure returns are numeric and handle potential NaNs\n",
    "    returns_numeric = returns.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        n_obs, n_assets = returns_numeric.shape\n",
    "        ewma_var = np.zeros((n_obs, n_assets))\n",
    "        \n",
    "        # Initialize first variance with sample variance of initial portion or overall\n",
    "        initial_var = returns_numeric.var()\n",
    "        ewma_var[0] = initial_var.values\n",
    "        \n",
    "        # Loop through time to apply EWMA variance formula\n",
    "        for t in range(1, n_obs):\n",
    "            ewma_var[t] = lambda_ * ewma_var[t-1] + (1 - lambda_) * returns_numeric.iloc[t-1].values**2\n",
    "        \n",
    "        # Convert variance to volatility (standard deviation)\n",
    "        ewma_vol = np.sqrt(ewma_var)\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        results[f'lambda_{lambda_}'] = pd.DataFrame(\n",
    "            ewma_vol,\n",
    "            index=returns.index,\n",
    "            columns=returns.columns\n",
    "        )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_returns(returns: pd.DataFrame, ewma_vol: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Compute standardized (filtered) returns for each lambda value.\n",
    "    \"\"\"\n",
    "    filtered_returns = {}\n",
    "    returns_numeric = returns.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Filter returns for each lambda value\n",
    "    for lambda_key, vol_df in ewma_vol.items():\n",
    "        # Align indices before division\n",
    "        vol_df_aligned, returns_aligned = vol_df.align(returns_numeric, join='inner', axis=0)\n",
    "        # Avoid division by zero or near-zero volatility\n",
    "        safe_vol_df = vol_df_aligned.replace(0, np.nan)\n",
    "        filtered = returns_aligned / safe_vol_df\n",
    "        # Handle potential NaNs resulting from division or original NaNs\n",
    "        filtered_returns[lambda_key] = filtered.fillna(0.0) # Fill NaNs with 0\n",
    "    \n",
    "    return filtered_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd23d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_historical_simulation_multivariate(filtered_returns_dict: dict, ewma_vol_dict: dict,\n",
    "                                              n_simulations: int = 10000, random_seed: int = None, \n",
    "                                              weights: np.ndarray = None) -> dict:\n",
    "    \"\"\"\n",
    "    Perform Filtered Historical Simulation for a multi-asset portfolio for different lambda values.\n",
    "    Calculates 1-day VaR and ES.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process each lambda value\n",
    "    for lambda_key in filtered_returns_dict.keys():\n",
    "        filtered_returns = filtered_returns_dict[lambda_key]\n",
    "        ewma_vol = ewma_vol_dict[lambda_key]\n",
    "        \n",
    "        if filtered_returns.empty or ewma_vol.empty:\n",
    "             print(f\"Warning: Empty filtered returns or EWMA vol for {lambda_key}. Skipping FHS.\")\n",
    "             results[lambda_key] = {'VaR': np.full(len(vAlpha), np.nan), 'ES': np.full(len(vAlpha), np.nan)}\n",
    "             continue\n",
    "             \n",
    "        assets = filtered_returns.columns\n",
    "        n_assets = len(assets)\n",
    "        \n",
    "        # Get the latest volatility forecast\n",
    "        sigma_t_vector = ewma_vol.iloc[-1].values\n",
    "        \n",
    "        # Initialize simulated return matrix (n_simulations x n_assets)\n",
    "        sim_returns_assets = np.zeros((n_simulations, n_assets))\n",
    "\n",
    "        # Sample from standardized residuals for each asset\n",
    "        for i, asset in enumerate(assets):\n",
    "            z_asset = filtered_returns[asset].dropna().values\n",
    "            if len(z_asset) == 0:\n",
    "                 print(f\"Warning: No residuals to sample for asset {asset} in {lambda_key}. Using zeros.\")\n",
    "                 z_star = np.zeros(n_simulations)\n",
    "            else:\n",
    "                 z_star = np.random.choice(z_asset, size=n_simulations, replace=True)\n",
    "            \n",
    "            # Rescale with the latest volatility forecast\n",
    "            sim_returns_assets[:, i] = sigma_t_vector[i] * z_star\n",
    "\n",
    "        # Calculate simulated portfolio returns\n",
    "        if weights is not None:\n",
    "            portfolio_simulated_returns = sim_returns_assets @ weights\n",
    "        else:\n",
    "            # If no weights, maybe return asset returns or mean? Assume portfolio context.\n",
    "            print(\"Warning: FHS called without weights. Returning NaNs.\")\n",
    "            portfolio_simulated_returns = np.full(n_simulations, np.nan)\n",
    "            \n",
    "        # Calculate VaR and ES from simulated portfolio returns\n",
    "        var_fhs = []\n",
    "        es_fhs = []\n",
    "        vAlpha = np.array([0.95, 0.99]) # Hardcoding alpha levels for now\n",
    "        sorted_sim_losses = np.sort(-portfolio_simulated_returns) # Simulate losses\n",
    "        \n",
    "        for alpha in vAlpha:\n",
    "            var_val = np.percentile(sorted_sim_losses, alpha * 100)\n",
    "            losses_above_var = sorted_sim_losses[sorted_sim_losses >= var_val]\n",
    "            es_val = losses_above_var.mean() if len(losses_above_var) > 0 else np.nan\n",
    "            var_fhs.append(var_val)\n",
    "            es_fhs.append(es_val)\n",
    "            \n",
    "        results[lambda_key] = {'VaR': np.array(var_fhs), 'ES': np.array(es_fhs)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519947e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_historical_simulation_multiday(\n",
    "    filtered_returns_dict: dict,\n",
    "    ewma_vol_dict: dict,\n",
    "    lambda_key: str,\n",
    "    n_days: int = 1,\n",
    "    n_simulations: int = 10000,\n",
    "    random_seed: int = None,\n",
    "    weights: np.ndarray = None,\n",
    "    vAlpha = np.array([0.95, 0.99])\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simulate N-day portfolio returns using Filtered Historical Simulation.\n",
    "    Calculates N-day VaR and ES.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    filtered_returns = filtered_returns_dict.get(lambda_key)\n",
    "    ewma_vol = ewma_vol_dict.get(lambda_key)\n",
    "\n",
    "    if filtered_returns is None or ewma_vol is None or filtered_returns.empty or ewma_vol.empty:\n",
    "        print(f\"Warning: Missing or empty data for {lambda_key} in FHS multiday. Returning NaNs.\")\n",
    "        return {'VaR': np.full(len(vAlpha), np.nan), 'ES': np.full(len(vAlpha), np.nan)}\n",
    "\n",
    "    assets = filtered_returns.columns\n",
    "    n_assets = len(assets)\n",
    "    # Store simulated N-day total returns for the portfolio\n",
    "    sim_portfolio_nday_returns = np.zeros(n_simulations)\n",
    "\n",
    "    # Get the latest volatility forecast vector\n",
    "    sigma_t_vector = ewma_vol.iloc[-1].values\n",
    "\n",
    "    # Pre-sample residuals for efficiency\n",
    "    residuals_sampled = {}\n",
    "    valid_sampling = True\n",
    "    for i, asset in enumerate(assets):\n",
    "        z_asset = filtered_returns[asset].dropna().values\n",
    "        if len(z_asset) == 0:\n",
    "            print(f\"Warning: No residuals for asset {asset} in {lambda_key}. Using zeros.\")\n",
    "            residuals_sampled[asset] = np.zeros((n_simulations, n_days))\n",
    "            # If any asset has no residuals, the simulation might be invalid\n",
    "            # valid_sampling = False \n",
    "            # Depending on requirements, might want to stop or continue with zeros\n",
    "        else:\n",
    "            # Sample N-day paths by sampling N times independently for each simulation\n",
    "            residuals_sampled[asset] = np.random.choice(z_asset, size=(n_simulations, n_days), replace=True)\n",
    "            \n",
    "    # if not valid_sampling:\n",
    "    #     print(f\"Error: Cannot perform valid FHS simulation for {lambda_key} due to missing residuals.\")\n",
    "    #     return {'VaR': np.full(len(vAlpha), np.nan), 'ES': np.full(len(vAlpha), np.nan)}\n",
    "\n",
    "    # Simulate N-day returns for each asset and calculate portfolio return\n",
    "    sim_asset_nday_returns = np.zeros((n_simulations, n_assets))\n",
    "    for i, asset in enumerate(assets):\n",
    "        # Rescale sampled residuals by the constant forecast volatility over the horizon\n",
    "        # Sum the daily simulated returns to get N-day return for the asset\n",
    "        sim_asset_nday_returns[:, i] = (residuals_sampled[asset] * sigma_t_vector[i]).sum(axis=1)\n",
    "        \n",
    "    # Calculate N-day portfolio return using weights\n",
    "    if weights is not None:\n",
    "        sim_portfolio_nday_returns = sim_asset_nday_returns @ weights\n",
    "    else:\n",
    "        print(\"Warning: FHS multiday called without weights. Returning NaNs.\")\n",
    "        sim_portfolio_nday_returns = np.full(n_simulations, np.nan)\n",
    "        \n",
    "    # Calculate VaR and ES from simulated N-day portfolio returns\n",
    "    var_fhs_nday = []\n",
    "    es_fhs_nday = []\n",
    "    sorted_sim_losses = np.sort(-sim_portfolio_nday_returns) # Losses\n",
    "    \n",
    "    for alpha in vAlpha:\n",
    "        var_val = np.percentile(sorted_sim_losses, alpha * 100)\n",
    "        losses_above_var = sorted_sim_losses[sorted_sim_losses >= var_val]\n",
    "        es_val = losses_above_var.mean() if len(losses_above_var) > 0 else np.nan\n",
    "        var_fhs_nday.append(var_val)\n",
    "        es_fhs_nday.append(es_val)\n",
    "        \n",
    "    return {'VaR': np.array(var_fhs_nday), 'ES': np.array(es_fhs_nday)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278d7d5",
   "metadata": {},
   "source": [
    "### Risk Calculation Method Functions (GARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum likelihood estimation of GARCH(1,1) parameters\n",
    "def garch_likelihood(params, returns):\n",
    "    omega, alpha, beta = params\n",
    "    # Constraints check\n",
    "    if omega <= 0 or alpha < 0 or beta < 0 or alpha + beta >= 1:\n",
    "        return np.inf  # Return infinity for invalid parameters\n",
    "        \n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "    \n",
    "    # Initialize variance (e.g., with unconditional variance or sample variance)\n",
    "    var[0] = omega / (1 - alpha - beta) # Unconditional variance\n",
    "    if var[0] <= 0:\n",
    "         var[0] = np.var(returns) # Fallback to sample variance if unconditional is non-positive\n",
    "         if var[0] <= 0:\n",
    "              var[0] = 1e-6 # Small positive number if sample variance is also non-positive\n",
    "              \n",
    "    log_likelihood = 0\n",
    "    \n",
    "    # Iterate through returns to calculate conditional variance and log-likelihood\n",
    "    for t in range(1, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "        # Ensure variance is positive\n",
    "        if var[t] <= 0:\n",
    "            return np.inf # Variance must be positive\n",
    "        \n",
    "        # Log-likelihood contribution for time t (assuming normality)\n",
    "        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(var[t]) + returns[t]**2 / var[t])\n",
    "        \n",
    "    # Return negative log-likelihood for minimization\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) parameter estimation using MLE\n",
    "def estimate_garch_params(returns):\n",
    "    \"\"\"Estimates GARCH(1,1) parameters for a given return series.\"\"\"\n",
    "    returns_clean = returns.dropna().values\n",
    "    if len(returns_clean) < 10: # Need sufficient data\n",
    "         print(\"Warning: Insufficient data for GARCH estimation.\")\n",
    "         return None\n",
    "         \n",
    "    # Initial guess (can influence convergence)\n",
    "    initial_guess = [np.var(returns_clean) * 0.01, 0.1, 0.85]\n",
    "    \n",
    "    # Bounds for parameters: omega > 0, 0 <= alpha < 1, 0 <= beta < 1\n",
    "    bounds = [(1e-7, None), (0, 0.999), (0, 0.999)]\n",
    "    \n",
    "    # Constraint: alpha + beta < 1 (for stationarity)\n",
    "    constraints = ({'type': 'ineq', 'fun': lambda params: 1 - params[1] - params[2]})\n",
    "    \n",
    "    # Run the optimization\n",
    "    result = minimize(\n",
    "        garch_likelihood,\n",
    "        x0=initial_guess,\n",
    "        args=(returns_clean,),\n",
    "        method='SLSQP', # Sequential Least Squares Programming is good for constrained optimization\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'disp': False, 'ftol': 1e-7} # Suppress output, set tolerance\n",
    "    )\n",
    "\n",
    "    if result.success and result.x[1] + result.x[2] < 1:\n",
    "        return result.x # Return optimized parameters [omega, alpha, beta]\n",
    "    else:\n",
    "        print(f\"GARCH Optimization failed or non-stationary: {result.message}\")\n",
    "        # Fallback or default parameters if optimization fails\n",
    "        # return [np.var(returns_clean) * 0.01, 0.05, 0.9] # Example fallback\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the GARCH(1,1) volatility forecast\n",
    "def forecast_garch_volatility(params, returns):\n",
    "    \"\"\"Forecasts the next day's volatility using estimated GARCH params.\"\"\"\n",
    "    if params is None:\n",
    "        # Handle case where estimation failed - use sample std dev as fallback\n",
    "        print(\"Warning: GARCH params not available, using sample std dev for forecast.\")\n",
    "        return np.std(returns.dropna())\n",
    "        \n",
    "    omega, alpha, beta = params\n",
    "    returns_clean = returns.dropna().values\n",
    "    T = len(returns_clean)\n",
    "    if T < 1:\n",
    "         print(\"Warning: No returns data for GARCH forecast.\")\n",
    "         return np.nan\n",
    "         \n",
    "    var = np.zeros(T)\n",
    "    # Initialize variance\n",
    "    var[0] = omega / (1 - alpha - beta) if (1 - alpha - beta) > 0 else np.var(returns_clean)\n",
    "    if var[0] <= 0: var[0] = 1e-6\n",
    "    \n",
    "    # Calculate historical conditional variances\n",
    "    for t in range(1, T):\n",
    "        var[t] = omega + alpha * returns_clean[t-1]**2 + beta * var[t-1]\n",
    "        if var[t] <= 0: var[t] = var[t-1] # Prevent non-positive variance\n",
    "        \n",
    "    # Forecast next day's variance\n",
    "    forecast_var = omega + alpha * returns_clean[-1]**2 + beta * var[-1]\n",
    "    if forecast_var <= 0: forecast_var = var[-1] # Ensure positive forecast\n",
    "    \n",
    "    return np.sqrt(forecast_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d23c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate constant conditional correlation matrix\n",
    "def calculate_ccc_matrix(returns_df):\n",
    "    \"\"\"Calculates the constant conditional correlation matrix from returns.\"\"\"\n",
    "    returns_clean = returns_df.dropna()\n",
    "    if returns_clean.empty or returns_clean.shape[1] < 2:\n",
    "         print(\"Warning: Insufficient data for correlation matrix.\")\n",
    "         # Return identity matrix or handle as error\n",
    "         n = returns_df.shape[1]\n",
    "         return np.identity(n) if n > 0 else np.array([[]])\n",
    "         \n",
    "    # Calculate the correlation matrix using pandas .corr() method\n",
    "    corr_matrix = returns_clean.corr().values\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the GARCH-CCC covariance matrix forecast\n",
    "def forecast_garch_ccc_covariance(returns_df, garch_params_dict, corr_matrix):\n",
    "    \"\"\"Forecasts the covariance matrix using GARCH volatilities and CCC.\"\"\"\n",
    "    asset_cols = returns_df.columns\n",
    "    n_assets = len(asset_cols)\n",
    "    vol_forecasts = np.zeros(n_assets)\n",
    "    \n",
    "    # Get volatility forecast for each asset\n",
    "    for i, col in enumerate(asset_cols):\n",
    "        params = garch_params_dict.get(col)\n",
    "        vol_forecasts[i] = forecast_garch_volatility(params, returns_df[col])\n",
    "        \n",
    "    # Create diagonal matrix of volatility forecasts\n",
    "    D_t = np.diag(vol_forecasts)\n",
    "    \n",
    "    # Calculate the forecasted covariance matrix: H_t = D_t * R * D_t\n",
    "    cov_matrix_forecast = D_t @ corr_matrix @ D_t\n",
    "    \n",
    "    return cov_matrix_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40911c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio variance and volatility from covariance matrix\n",
    "def calculate_portfolio_variance_volatility(weights, cov_matrix):\n",
    "    \"\"\"Calculates portfolio variance and volatility given weights and covariance matrix.\"\"\"\n",
    "    port_variance = weights.T @ cov_matrix @ weights\n",
    "    # Ensure variance is non-negative\n",
    "    port_variance = max(port_variance, 0)\n",
    "    port_volatility = np.sqrt(port_variance)\n",
    "    return port_variance, port_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VaR and ES using GARCH-CCC forecast (assuming normality)\n",
    "def calculate_garch_ccc_var_es(weights, cov_matrix_forecast, vAlpha):\n",
    "    \"\"\"Calculates VaR and ES based on GARCH-CCC forecast, assuming normal distribution.\"\"\"\n",
    "    _, port_volatility_forecast = calculate_portfolio_variance_volatility(weights, cov_matrix_forecast)\n",
    "    \n",
    "    var_garch = []\n",
    "    es_garch = []\n",
    "    \n",
    "    for alpha in vAlpha:\n",
    "        # VaR = Portfolio_Volatility * Z_alpha (Losses are positive)\n",
    "        var_val = port_volatility_forecast * st.norm.ppf(alpha)\n",
    "        # ES = Portfolio_Volatility * E[Z | Z > Z_alpha] (Losses are positive)\n",
    "        es_val = port_volatility_forecast * st.norm.pdf(st.norm.ppf(alpha)) / (1 - alpha)\n",
    "        var_garch.append(var_val)\n",
    "        es_garch.append(es_val)\n",
    "        \n",
    "    return {'VaR': np.array(var_garch), 'ES': np.array(es_garch)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3d062",
   "metadata": {},
   "source": [
    "### Backtesting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_violations(actual_losses, var_predictions):\n",
    "    \"\"\"Checks for VaR violations (actual loss > predicted VaR).\"\"\"\n",
    "    # Ensure inputs are aligned Series\n",
    "    actual_losses_aligned, var_predictions_aligned = actual_losses.align(var_predictions, join='inner')\n",
    "    return actual_losses_aligned > var_predictions_aligned\n",
    "\n",
    "def backtest_var(violations, alpha):\n",
    "    \"\"\"Performs basic VaR backtesting: counts violations and compares yearly rates.\"\"\"\n",
    "    if violations.empty:\n",
    "        print(\"Warning: No violations data to backtest.\")\n",
    "        return pd.DataFrame(columns=['Actual Violations', 'Expected Violations', 'Total Observations', 'Violation Rate (%)', 'Expected Rate (%)'])\n",
    "        \n",
    "    violations_df = pd.DataFrame({'Violations': violations, 'Year': violations.index.year})\n",
    "    yearly_violations = violations_df.groupby('Year')['Violations'].sum()\n",
    "    yearly_counts = violations_df.groupby('Year')['Violations'].count()\n",
    "    \n",
    "    expected_violations = yearly_counts * (1 - alpha)\n",
    "    violation_rate = (yearly_violations / yearly_counts) * 100\n",
    "    expected_rate = (1 - alpha) * 100\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Actual Violations': yearly_violations,\n",
    "        'Expected Violations': expected_violations,\n",
    "        'Total Observations': yearly_counts,\n",
    "        'Violation Rate (%)': violation_rate,\n",
    "        'Expected Rate (%)': expected_rate\n",
    "    })\n",
    "    # Add overall summary row\n",
    "    overall_actual = summary['Actual Violations'].sum()\n",
    "    overall_total = summary['Total Observations'].sum()\n",
    "    overall_expected = overall_total * (1 - alpha)\n",
    "    overall_rate = (overall_actual / overall_total) * 100 if overall_total > 0 else 0\n",
    "    summary.loc['Overall'] = [overall_actual, overall_expected, overall_total, overall_rate, expected_rate]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def backtest_es(actual_losses, violations, es_predictions):\n",
    "    \"\"\"Performs basic ES backtesting: compares average shortfall in violation periods to predicted ES.\"\"\"\n",
    "    # Align inputs\n",
    "    actual_losses_aligned, violations_aligned, es_predictions_aligned = actual_losses.align(violations, es_predictions, join='inner')\n",
    "    \n",
    "    if violations_aligned.empty:\n",
    "        print(\"Warning: No violations data for ES backtest.\")\n",
    "        return pd.DataFrame(columns=['Avg Actual Shortfall', 'Avg Predicted ES', 'Violation Count'])\n",
    "        \n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_Loss': actual_losses_aligned,\n",
    "        'Violation': violations_aligned,\n",
    "        'Predicted_ES': es_predictions_aligned,\n",
    "        'Year': actual_losses_aligned.index.year\n",
    "    })\n",
    "    \n",
    "    # Filter for violations\n",
    "    violation_data = results_df[results_df['Violation']].copy()\n",
    "    \n",
    "    if violation_data.empty:\n",
    "         print(\"Info: No violations occurred for ES backtest.\")\n",
    "         return pd.DataFrame(columns=['Avg Actual Shortfall', 'Avg Predicted ES', 'Violation Count'])\n",
    "         \n",
    "    # Calculate yearly averages\n",
    "    yearly_avg_actual_shortfall = violation_data.groupby('Year')['Actual_Loss'].mean()\n",
    "    yearly_avg_predicted_es = violation_data.groupby('Year')['Predicted_ES'].mean()\n",
    "    yearly_violation_count = violation_data.groupby('Year').size()\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Avg Actual Shortfall': yearly_avg_actual_shortfall,\n",
    "        'Avg Predicted ES': yearly_avg_predicted_es,\n",
    "        'Violation Count': yearly_violation_count\n",
    "    })\n",
    "    # Add overall summary row\n",
    "    overall_avg_shortfall = violation_data['Actual_Loss'].mean()\n",
    "    overall_avg_es = violation_data['Predicted_ES'].mean()\n",
    "    overall_count = violation_data.shape[0]\n",
    "    summary.loc['Overall'] = [overall_avg_shortfall, overall_avg_es, overall_count]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def plot_violations(violations, dates, title):\n",
    "    \"\"\"Plots VaR violations over time.\"\"\"\n",
    "    if violations.empty or dates.empty:\n",
    "         print(f\"Warning: Cannot plot violations for '{title}' - data is empty.\")\n",
    "         return\n",
    "         \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    # Ensure we only plot points where violation is True (or 1)\n",
    "    violation_points = violations[violations == True]\n",
    "    if not violation_points.empty:\n",
    "        plt.plot(violation_points.index, np.ones(len(violation_points)), 'ro', markersize=4, alpha=0.7, label='Violation')\n",
    "    else:\n",
    "         # Plot an empty graph if no violations, but keep structure\n",
    "         plt.plot([], [], 'ro', markersize=4, alpha=0.7, label='Violation') \n",
    "         \n",
    "    # Set x-axis limits based on the full date range\n",
    "    plt.xlim(dates.min(), dates.max())\n",
    "    plt.ylim(-0.1, 1.1) # Set y-limits to accommodate 0 and 1\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Violation (1=Yes)')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea4e73",
   "metadata": {},
   "source": [
    "### Stress Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stress(df, column, start_idx, duration, magnitude, stress_type='percentage'):\n",
    "    \"\"\"Applies a stress shock to a DataFrame column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify (should be a copy).\n",
    "        column (str): The name of the column to stress.\n",
    "        start_idx (int): The starting index for the stress period.\n",
    "        duration (int): The number of days the stress lasts.\n",
    "        magnitude (float): The size of the shock.\n",
    "        stress_type (str): 'percentage' or 'absolute'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the stress applied.\n",
    "    \"\"\"\n",
    "    df_stressed = df.copy()\n",
    "    end_idx = min(start_idx + duration, len(df_stressed))\n",
    "    target_indices = df_stressed.index[start_idx:end_idx]\n",
    "    \n",
    "    if target_indices.empty:\n",
    "        print(f\"Warning: No indices found for stress period starting at {start_idx}.\")\n",
    "        return df_stressed\n",
    "        \n",
    "    original_values = df_stressed.loc[target_indices, column]\n",
    "    \n",
    "    if stress_type == 'percentage':\n",
    "        stressed_values = original_values * (1 + magnitude)\n",
    "    elif stress_type == 'absolute':\n",
    "        stressed_values = original_values + magnitude\n",
    "    else:\n",
    "        raise ValueError(\"stress_type must be 'percentage' or 'absolute'\")\n",
    "        \n",
    "    df_stressed.loc[target_indices, column] = stressed_values\n",
    "    print(f\"Applied {stress_type} stress ({magnitude:.2f}) to {column} from {target_indices[0]} to {target_indices[-1]}\")\n",
    "    return df_stressed\n",
    "\n",
    "def stress_equity(df_original, repetitions=5, max_duration=4):\n",
    "    \"\"\"Applies random stress shocks to equity index closing prices.\"\"\"\n",
    "    df = df_original.copy()\n",
    "    equity_cols = ['S&P500_Closing', 'Dax40_Closing', 'Nikkei_Closing']\n",
    "    magnitudes = [-0.4, -0.2, 0.2, 0.4]\n",
    "    n = len(df)\n",
    "    \n",
    "    if n <= max_duration:\n",
    "        print(\"DataFrame too small for stress testing.\")\n",
    "        return df\n",
    "        \n",
    "    print(\"\\n--- Applying Equity Stress ---\")\n",
    "    for _ in range(repetitions):\n",
    "        col_to_stress = random.choice(equity_cols)\n",
    "        duration = random.randint(1, max_duration)\n",
    "        start_idx = random.randint(0, n - duration)\n",
    "        magnitude = random.choice(magnitudes)\n",
    "        df = apply_stress(df, col_to_stress, start_idx, duration, magnitude, 'percentage')\n",
    "        \n",
    "    return df\n",
    "\n",
    "def stress_currency(df_original, repetitions=5, max_duration=4):\n",
    "    \"\"\"Applies random stress shocks to currency exchange rates.\"\"\"\n",
    "    df = df_original.copy()\n",
    "    currency_mags = {\n",
    "        'USD/EUR': [-0.1, 0.1], \n",
    "        'JPY/EUR': [-0.2, 0.2]\n",
    "    }\n",
    "    currency_cols = list(currency_mags.keys())\n",
    "    n = len(df)\n",
    "    \n",
    "    if n <= max_duration:\n",
    "        print(\"DataFrame too small for stress testing.\")\n",
    "        return df\n",
    "        \n",
    "    print(\"\\n--- Applying Currency Stress ---\")\n",
    "    for _ in range(repetitions):\n",
    "        col_to_stress = random.choice(currency_cols)\n",
    "        duration = random.randint(1, max_duration)\n",
    "        start_idx = random.randint(0, n - duration)\n",
    "        magnitude = random.choice(currency_mags[col_to_stress])\n",
    "        df = apply_stress(df, col_to_stress, start_idx, duration, magnitude, 'percentage')\n",
    "        \n",
    "    return df\n",
    "\n",
    "def stress_commodity(df_original, repetitions=5, max_duration=4):\n",
    "    \"\"\"Placeholder for commodity stress testing. Returns original DataFrame.\"\"\"\n",
    "    print(\"\\n--- Applying Commodity Stress (Placeholder) ---\")\n",
    "    print(\"Stress Test Info: No commodity columns found. Skipping commodity stress test.\")\n",
    "    return df_original.copy()\n",
    "\n",
    "def stress_interest_rate(df_original, repetitions=5, max_duration=4):\n",
    "    \"\"\"Applies random absolute shifts to the government bond yield.\"\"\"\n",
    "    df = df_original.copy()\n",
    "    interest_col = 'Yield curve spot rate, 10-year maturity - Government bond'\n",
    "    magnitudes = [-3.0, -2.0, 2.0, 3.0]\n",
    "    n = len(df)\n",
    "    \n",
    "    if n <= max_duration:\n",
    "        print(\"DataFrame too small for stress testing.\")\n",
    "        return df\n",
    "        \n",
    "    if interest_col not in df.columns:\n",
    "        print(f\"Stress Test Info: Column '{interest_col}' not found. Skipping interest rate stress test.\")\n",
    "        return df\n",
    "        \n",
    "    print(\"\\n--- Applying Interest Rate Stress ---\")\n",
    "    for _ in range(repetitions):\n",
    "        duration = random.randint(1, max_duration)\n",
    "        start_idx = random.randint(0, n - duration)\n",
    "        magnitude = random.choice(magnitudes)\n",
    "        df = apply_stress(df, interest_col, start_idx, duration, magnitude, 'absolute')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c77c07",
   "metadata": {},
   "source": [
    "### Main Analysis Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_analysis(df_input, vAlpha=np.array([0.95, 0.99]), sample_size=500, \n",
    "                      degrees_of_freedom=[0, 3, 4, 5, 6], fhs_lambdas=[0.94, 0.97], \n",
    "                      fhs_simulations=1000, garch_weights=weights_array):\n",
    "    \"\"\"Runs all VaR/ES calculations and backtesting for a given DataFrame.\"\"\"\n",
    "    print(f\"\\n=== Starting Full Analysis for DataFrame ending {df_input.index[-1]} ===\")\n",
    "    df_analysis = df_input.copy()\n",
    "    \n",
    "    # --- 1. 1-Day VaR/ES Calculations (Var/Cov, Historical) ---\n",
    "    print(\"Calculating 1-Day VaR/ES (Var/Cov, Historical)...\")\n",
    "    VaR_results_1d = []\n",
    "    ES_results_1d = []\n",
    "    dates_1d = []\n",
    "    \n",
    "    # Ensure sample_size is valid\n",
    "    if sample_size >= len(df_analysis):\n",
    "         print(f\"Warning: sample_size ({sample_size}) too large for DataFrame length ({len(df_analysis)}). Skipping rolling calculations.\")\n",
    "         # Return empty or partially filled results\n",
    "         return {}, {}, {}, {}, {}, {}, {}\n",
    "         \n",
    "    for i in range(sample_size, len(df_analysis)):\n",
    "        window = df_analysis.iloc[i - sample_size:i]\n",
    "        current_date = df_analysis.index[i]\n",
    "        dates_1d.append(current_date)\n",
    "        \n",
    "        loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "        mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "        portfolio_std_loss = loss_stats[\"Portfolio_std_loss\"]\n",
    "        \n",
    "        var_row = {'Date': current_date}\n",
    "        es_row = {'Date': current_date}\n",
    "        \n",
    "        # Var/Cov Methods\n",
    "        for df_t in degrees_of_freedom:\n",
    "            results = calculate_var_cov(window, current_date, vAlpha, mean_loss, portfolio_std_loss, df=df_t)\n",
    "            dist_label = f\"T{df_t}\" if df_t > 0 else \"Normal\"\n",
    "            var_row[f'VaR {dist_label}'] = results[f'VaR {dist_label}']\n",
    "            es_row[f'ES {dist_label}'] = results[f'ES {dist_label}']\n",
    "        \n",
    "        # Historical Method\n",
    "        hist_results = calculate_historical_var_es(window, current_date, vAlpha)\n",
    "        var_row['VaR Historical'] = hist_results['VaR Historical']\n",
    "        es_row['ES Historical'] = hist_results['ES Historical']\n",
    "        \n",
    "        VaR_results_1d.append(var_row)\n",
    "        ES_results_1d.append(es_row)\n",
    "        \n",
    "    var_results_df = pd.DataFrame(VaR_results_1d).set_index('Date') if VaR_results_1d else pd.DataFrame()\n",
    "    es_results_df = pd.DataFrame(ES_results_1d).set_index('Date') if ES_results_1d else pd.DataFrame()\n",
    "    \n",
    "    # --- 2. Multi-Day VaR/ES (Historical - Regular & Sqrt) ---\n",
    "    print(\"Calculating Multi-Day VaR/ES (Historical)...\")\n",
    "    var_5d_df, es_5d_df = calculate_multiday_risk(df_analysis, vAlpha, 5, sample_size)\n",
    "    var_10d_df, es_10d_df = calculate_multiday_risk(df_analysis, vAlpha, 10, sample_size)\n",
    "    \n",
    "    # --- 3. FHS Calculations (Placeholder - Requires more integration) ---\n",
    "    # print(\"Calculating 1-Day VaR/ES (FHS - Placeholder)...\")\n",
    "    # fhs_results = {} # Store FHS VaR/ES here\n",
    "    # Add FHS calculation logic if needed, similar structure to GARCH below\n",
    "    # Need to run EWMA, filter returns, then simulate within the loop or separately\n",
    "    \n",
    "    # --- 4. GARCH-CCC Calculations (Placeholder - Requires more integration) ---\n",
    "    # print(\"Calculating 1-Day VaR/ES (GARCH-CCC - Placeholder)...\")\n",
    "    # garch_results = {} # Store GARCH VaR/ES here\n",
    "    # Add GARCH estimation and forecasting logic if needed\n",
    "    # Needs careful handling within the rolling window\n",
    "    \n",
    "    # --- 5. Backtesting ---\n",
    "    print(\"Running Backtesting...\")\n",
    "    backtest_summaries = {}\n",
    "    actual_losses_full = df_analysis['Portfolio_loss']\n",
    "    \n",
    "    # Backtest 1-Day Models\n",
    "    for model_col in var_results_df.columns:\n",
    "        if model_col == 'Date': continue\n",
    "        es_col = model_col.replace('VaR', 'ES')\n",
    "        if es_col not in es_results_df.columns:\n",
    "             print(f\"Skipping backtest for {model_col}, no matching ES column.\")\n",
    "             continue\n",
    "             \n",
    "        print(f\"  Backtesting {model_col}...\")\n",
    "        model_backtest = {}\n",
    "        for i, alpha in enumerate(vAlpha):\n",
    "            alpha_perc = int(alpha * 100)\n",
    "            # Extract VaR/ES series for this alpha\n",
    "            try:\n",
    "                var_preds = var_results_df[model_col].apply(lambda x: x[i] if isinstance(x, (list, np.ndarray)) and len(x) > i else np.nan)\n",
    "                es_preds = es_results_df[es_col].apply(lambda x: x[i] if isinstance(x, (list, np.ndarray)) and len(x) > i else np.nan)\n",
    "            except Exception as e:\n",
    "                 print(f\"    Error extracting predictions for {model_col} alpha={alpha}: {e}\")\n",
    "                 continue\n",
    "                 \n",
    "            # Align actual losses with predictions\n",
    "            actual_losses, var_preds_aligned = actual_losses_full.align(var_preds, join='inner')\n",
    "            _, es_preds_aligned = actual_losses_full.align(es_preds, join='inner')\n",
    "            \n",
    "            if actual_losses.empty or var_preds_aligned.empty:\n",
    "                 print(f\"    Skipping alpha={alpha}, no aligned data.\")\n",
    "                 continue\n",
    "                 \n",
    "            violations = calculate_violations(actual_losses, var_preds_aligned)\n",
    "            var_summary = backtest_var(violations, alpha)\n",
    "            es_summary = backtest_es(actual_losses, violations, es_preds_aligned)\n",
    "            \n",
    "            model_backtest[f'VaR_{alpha_perc}'] = var_summary\n",
    "            model_backtest[f'ES_{alpha_perc}'] = es_summary\n",
    "            # plot_violations(violations, violations.index, f'{model_col} Violations (alpha={alpha})')\n",
    "            \n",
    "        backtest_summaries[model_col.replace('VaR ', '')] = model_backtest\n",
    "        \n",
    "    # Backtest Multi-Day Models (Example for 5-day Historical Regular)\n",
    "    # Add similar loops for other multi-day models if needed\n",
    "    print(\"  Backtesting Multi-Day Historical (Example: 5d Reg)...\")\n",
    "    for alpha in vAlpha:\n",
    "        alpha_perc = int(alpha*100)\n",
    "        var_col_5d = f'VaR_5d_Hist_Reg_{alpha_perc}'\n",
    "        es_col_5d = f'ES_5d_Hist_Reg_{alpha_perc}'\n",
    "        actual_col_5d = 'Actual_Loss_5d'\n",
    "        \n",
    "        if var_col_5d in var_5d_df.columns and actual_col_5d in var_5d_df.columns:\n",
    "            var_preds_5d = var_5d_df[var_col_5d]\n",
    "            actual_losses_5d = var_5d_df[actual_col_5d]\n",
    "            es_preds_5d = es_5d_df[es_col_5d] if es_col_5d in es_5d_df.columns else None\n",
    "            \n",
    "            actual_losses_5d_aligned, var_preds_5d_aligned = actual_losses_5d.align(var_preds_5d, join='inner')\n",
    "            violations_5d = calculate_violations(actual_losses_5d_aligned, var_preds_5d_aligned)\n",
    "            var_summary_5d = backtest_var(violations_5d, alpha)\n",
    "            \n",
    "            model_key = f'Historical_5d_Reg'\n",
    "            if model_key not in backtest_summaries: backtest_summaries[model_key] = {}\n",
    "            backtest_summaries[model_key][f'VaR_{alpha_perc}'] = var_summary_5d\n",
    "            \n",
    "            if es_preds_5d is not None:\n",
    "                 _, es_preds_5d_aligned = actual_losses_5d.align(es_preds_5d, join='inner')\n",
    "                 es_summary_5d = backtest_es(actual_losses_5d_aligned, violations_5d, es_preds_5d_aligned)\n",
    "                 backtest_summaries[model_key][f'ES_{alpha_perc}'] = es_summary_5d\n",
    "                 \n",
    "            # plot_violations(violations_5d, violations_5d.index, f'{model_key} Violations (alpha={alpha})')\n",
    "        else:\n",
    "            print(f\"    Skipping 5d Hist Reg alpha={alpha}, missing columns.\")\n",
    "            \n",
    "    print(\"=== Full Analysis Complete ===\")\n",
    "    return var_results_df, es_results_df, var_5d_df, es_5d_df, var_10d_df, es_10d_df, backtest_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09ae67",
   "metadata": {},
   "source": [
    "## Initial Data Calculation and Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial calculations on the loaded data\n",
    "print(\"Performing initial calculations on baseline data...\")\n",
    "main_df_calculated = calculate_bond_metrics(main_df_initial, interest_bond_initial)\n",
    "main_df_calculated = calculate_investment_values(main_df_calculated, invested_amounts_initial, start_date)\n",
    "main_df_calculated = calculate_portfolio_metrics(main_df_calculated, starting_investment, start_date)\n",
    "\n",
    "print(\"\\nBaseline Data Head with Portfolio Metrics:\")\n",
    "display(main_df_calculated[['Portfolio_Value_EUR', 'Portfolio_Change_EUR', 'Portfolio_loss', 'Portfolio_Daily_Returns']].head())\n",
    "\n",
    "# Run the full analysis on the baseline data\n",
    "var_1d_base, es_1d_base, var_5d_base, es_5d_base, var_10d_base, es_10d_base, backtest_base = run_full_analysis(main_df_calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca9485",
   "metadata": {},
   "source": [
    "## Baseline Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Baseline 1-Day VaR Results (Historical) ---\")\n",
    "if 'VaR Historical' in var_1d_base:\n",
    "    # Extract 95% and 99% VaR from the arrays\n",
    "    var_hist_95 = var_1d_base['VaR Historical'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "    var_hist_99 = var_1d_base['VaR Historical'].apply(lambda x: x[1] if isinstance(x, (list, np.ndarray)) and len(x)>1 else np.nan)\n",
    "    display(pd.DataFrame({'VaR 95%': var_hist_95, 'VaR 99%': var_hist_99}).tail())\n",
    "else:\n",
    "    print(\"Historical VaR not found in baseline results.\")\n",
    "\n",
    "print(\"\\n--- Baseline 5-Day VaR Results (Historical Regular) ---\")\n",
    "if 'VaR_5d_Hist_Reg_95' in var_5d_base:\n",
    "    display(var_5d_base[['VaR_5d_Hist_Reg_95', 'VaR_5d_Hist_Reg_99', 'Actual_Loss_5d']].tail())\n",
    "else:\n",
    "    print(\"5-Day Historical Regular VaR not found.\")\n",
    "\n",
    "print(\"\\n--- Baseline 10-Day VaR Results (Historical Regular) ---\")\n",
    "if 'VaR_10d_Hist_Reg_95' in var_10d_base:\n",
    "    display(var_10d_base[['VaR_10d_Hist_Reg_95', 'VaR_10d_Hist_Reg_99', 'Actual_Loss_10d']].tail())\n",
    "else:\n",
    "    print(\"10-Day Historical Regular VaR not found.\")\n",
    "\n",
    "print(\"\\n--- Baseline Backtest Summary (Historical VaR 99%) ---\")\n",
    "if 'Historical' in backtest_base and 'VaR_99' in backtest_base['Historical']:\n",
    "    display(backtest_base['Historical']['VaR_99'])\n",
    "else:\n",
    "    print(\"Historical VaR 99% backtest summary not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a41bd1",
   "metadata": {},
   "source": [
    "# Stress Testing\n",
    "\n",
    "Apply stress scenarios and re-run the analysis to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70757d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stress Scenarios\n",
    "stressed_df_equity = stress_equity(main_df_calculated)\n",
    "stressed_df_currency = stress_currency(main_df_calculated)\n",
    "stressed_df_interest = stress_interest_rate(main_df_calculated)\n",
    "# stressed_df_commodity = stress_commodity(main_df_calculated) # Placeholder\n",
    "\n",
    "# Recalculate dependent columns for each stressed DataFrame\n",
    "print(\"\\nRecalculating metrics for EQUITY stressed data...\")\n",
    "stressed_df_equity_recalc = recalculate_dependent_columns(stressed_df_equity, starting_investment, weights_dict, start_date, interest_bond_initial)\n",
    "\n",
    "print(\"\\nRecalculating metrics for CURRENCY stressed data...\")\n",
    "stressed_df_currency_recalc = recalculate_dependent_columns(stressed_df_currency, starting_investment, weights_dict, start_date, interest_bond_initial)\n",
    "\n",
    "print(\"\\nRecalculating metrics for INTEREST RATE stressed data...\")\n",
    "stressed_df_interest_recalc = recalculate_dependent_columns(stressed_df_interest, starting_investment, weights_dict, start_date, interest_bond_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28aebb",
   "metadata": {},
   "source": [
    "## Run Analysis on Stressed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf18ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full analysis on each stressed dataset\n",
    "# Note: Backtesting results on artificially stressed data might not be meaningful,\n",
    "# but we run the full function for consistency.\n",
    "var_1d_eq, es_1d_eq, var_5d_eq, es_5d_eq, var_10d_eq, es_10d_eq, backtest_eq = run_full_analysis(stressed_df_equity_recalc)\n",
    "var_1d_cur, es_1d_cur, var_5d_cur, es_5d_cur, var_10d_cur, es_10d_cur, backtest_cur = run_full_analysis(stressed_df_currency_recalc)\n",
    "var_1d_ir, es_1d_ir, var_5d_ir, es_5d_ir, var_10d_ir, es_10d_ir, backtest_ir = run_full_analysis(stressed_df_interest_recalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a922bd7",
   "metadata": {},
   "source": [
    "## Comparison of Baseline vs. Stressed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latest_var(var_df, model_col, alpha_idx=1): # Default to 99% VaR (index 1)\n",
    "    \"\"\"Extracts the latest VaR value for a given model and alpha index.\"\"\"\n",
    "    if var_df is None or var_df.empty or model_col not in var_df.columns:\n",
    "        return np.nan\n",
    "    try:\n",
    "        latest_val_array = var_df[model_col].iloc[-1]\n",
    "        if isinstance(latest_val_array, (list, np.ndarray)) and len(latest_val_array) > alpha_idx:\n",
    "            return latest_val_array[alpha_idx]\n",
    "        else:\n",
    "             # Handle cases where it might not be an array (e.g., multi-day results)\n",
    "             # Check if column name itself implies alpha\n",
    "             if f'_{int(vAlpha[alpha_idx]*100)}' in model_col:\n",
    "                  return latest_val_array\n",
    "             else:\n",
    "                  return np.nan # Cannot determine correct alpha value\n",
    "    except (IndexError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "# Compare latest 1-Day 99% Historical VaR\n",
    "latest_var_base_1d = extract_latest_var(var_1d_base, 'VaR Historical')\n",
    "latest_var_eq_1d = extract_latest_var(var_1d_eq, 'VaR Historical')\n",
    "latest_var_cur_1d = extract_latest_var(var_1d_cur, 'VaR Historical')\n",
    "latest_var_ir_1d = extract_latest_var(var_1d_ir, 'VaR Historical')\n",
    "\n",
    "# Compare latest 5-Day 99% Historical VaR (Regular)\n",
    "latest_var_base_5d = extract_latest_var(var_5d_base, 'VaR_5d_Hist_Reg_99', alpha_idx=None) # Alpha in col name\n",
    "latest_var_eq_5d = extract_latest_var(var_5d_eq, 'VaR_5d_Hist_Reg_99', alpha_idx=None)\n",
    "latest_var_cur_5d = extract_latest_var(var_5d_cur, 'VaR_5d_Hist_Reg_99', alpha_idx=None)\n",
    "latest_var_ir_5d = extract_latest_var(var_5d_ir, 'VaR_5d_Hist_Reg_99', alpha_idx=None)\n",
    "\n",
    "# Compare latest 10-Day 99% Historical VaR (Regular)\n",
    "latest_var_base_10d = extract_latest_var(var_10d_base, 'VaR_10d_Hist_Reg_99', alpha_idx=None) # Alpha in col name\n",
    "latest_var_eq_10d = extract_latest_var(var_10d_eq, 'VaR_10d_Hist_Reg_99', alpha_idx=None)\n",
    "latest_var_cur_10d = extract_latest_var(var_10d_cur, 'VaR_10d_Hist_Reg_99', alpha_idx=None)\n",
    "latest_var_ir_10d = extract_latest_var(var_10d_ir, 'VaR_10d_Hist_Reg_99', alpha_idx=None)\n",
    "\n",
    "comparison_data = {\n",
    "    'Scenario': ['Baseline', 'Equity Stress', 'Currency Stress', 'Interest Rate Stress'],\n",
    "    '1-Day VaR 99% (Hist)': [latest_var_base_1d, latest_var_eq_1d, latest_var_cur_1d, latest_var_ir_1d],\n",
    "    '5-Day VaR 99% (Hist Reg)': [latest_var_base_5d, latest_var_eq_5d, latest_var_cur_5d, latest_var_ir_5d],\n",
    "    '10-Day VaR 99% (Hist Reg)': [latest_var_base_10d, latest_var_eq_10d, latest_var_cur_10d, latest_var_ir_10d]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).set_index('Scenario')\n",
    "\n",
    "print(\"\\n--- Comparison of Latest 99% Historical VaR across Scenarios ---\")\n",
    "display(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133e012",
   "metadata": {},
   "source": [
    "### Stress Test Interpretation\n",
    "\n",
    "Compare the VaR values in the table above:\n",
    "*   **Equity Stress:** How much did the VaR increase when equity prices experienced sharp moves? This shows sensitivity to market crashes or rallies.\n",
    "*   **Currency Stress:** How did VaR change when exchange rates moved significantly? This highlights the portfolio's FX risk.\n",
    "*   **Interest Rate Stress:** What was the impact of large shifts in interest rates on VaR? This indicates sensitivity to changes in bond yields.\n",
    "\n",
    "The magnitude of the change in VaR under each stress scenario reveals the portfolio's vulnerability to that specific risk factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317de7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add plots comparing VaR/ES time series for baseline vs stressed scenarios\n",
    "# Example: Plot 1-Day Historical VaR 99%\n",
    "plt.figure(figsize=(14, 7))\n",
    "if 'VaR Historical' in var_1d_base:\n",
    "    var_1d_base['VaR Historical'].apply(lambda x: x[1] if isinstance(x, (list, np.ndarray)) and len(x)>1 else np.nan).plot(label='Baseline VaR 99%', alpha=0.7)\n",
    "if 'VaR Historical' in var_1d_eq:\n",
    "    var_1d_eq['VaR Historical'].apply(lambda x: x[1] if isinstance(x, (list, np.ndarray)) and len(x)>1 else np.nan).plot(label='Equity Stress VaR 99%', alpha=0.7, linestyle='--')\n",
    "if 'VaR Historical' in var_1d_cur:\n",
    "    var_1d_cur['VaR Historical'].apply(lambda x: x[1] if isinstance(x, (list, np.ndarray)) and len(x)>1 else np.nan).plot(label='Currency Stress VaR 99%', alpha=0.7, linestyle=':')\n",
    "if 'VaR Historical' in var_1d_ir:\n",
    "    var_1d_ir['VaR Historical'].apply(lambda x: x[1] if isinstance(x, (list, np.ndarray)) and len(x)>1 else np.nan).plot(label='Interest Rate Stress VaR 99%', alpha=0.7, linestyle='-.')\n",
    "\n",
    "plt.title('Comparison of 1-Day 99% Historical VaR over Time')\n",
    "plt.ylabel('VaR (EUR)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93acf26f",
   "metadata": {},
   "source": [
    "### Original Portfolio Details (Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421255dd",
   "metadata": {},
   "source": [
    "#### Instruments:\n",
    "- **S&P500**\n",
    "- **DAX40**\n",
    "- **NIKKEI**\n",
    "- **EU Government Bond (10-year maturity, AAA-rated)**\n",
    "\n",
    "#### Invested amount:\n",
    "- **10,000,000 EURO**\n",
    "\n",
    "#### Period:\n",
    "- **01/01/2012 - 31/12/2022**\n",
    "\n",
    "#### Weights:\n",
    "- **S&P500**: 0.4  \n",
    "- **DAX40**: 0.3  \n",
    "- **NIKKEI**: 0.15  \n",
    "- **EU Government Bond**: 0.15  \n",
    "\n",
    "#### Measures:\n",
    "- **Value at Risk (VaR)**: 1, 5, 10 days  \n",
    "- **Expected Shortfall (ES)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74dd27",
   "metadata": {},
   "source": [
    "## Appendix: Original Code Cells (Removed/Integrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's logic is now integrated into run_full_analysis ---\n",
    "# loss_values = main_df['Portfolio_loss'].values\n",
    "# ... (min, max, mean calculation) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe414a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's logic is replaced by the Baseline Results Display section ---\n",
    "# var_results_df, es_results_df, var_5d, var_10d = main()\n",
    "# print(\"VaR results\")\n",
    "# display(var_results_df.head())\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's plotting logic can be adapted or is partially covered by backtesting plots ---\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=False)\n",
    "# ... (Plotting 5d/10d VaR vs Actual Loss and Violations) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's plotting logic can be adapted or is partially covered by backtesting plots ---\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# ... (Plotting Historical VaR/ES over time) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b028c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell plots return distributions, can be kept if desired ---\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "# ... (Plotting return distributions vs Normal/T) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc14ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's plotting logic is integrated into backtesting or comparison plots ---\n",
    "# def plot_var_es_vs_actual_given_actuals(...):\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89509d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's logic is integrated into multi-day risk calculation ---\n",
    "# def compute_actual_portfolio_returns(...):\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's logic is integrated into run_full_analysis (FHS/EWMA part, currently placeholder) ---\n",
    "# var_df, es_df = rolling_fhs_multiday_var_es(...)\n",
    "# plot_var_es_vs_actual_given_actuals(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's logic is integrated into run_full_analysis (GARCH part, currently placeholder) ---\n",
    "# def main_analysis(time_window_size):\n",
    "# ... (Rolling GARCH calculation and plotting) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Informational cell, can be kept or removed ---\n",
    "print(\"Original DataFrame Columns:\")\n",
    "print(main_df_initial.columns)\n",
    "print(\"\\nOriginal DataFrame Head:\")\n",
    "print(main_df_initial.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006647f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell checks mean return assumption, can be kept if desired ---\n",
    "# main_df['Portfolio_Daily_Returns'] = ...\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# ... (Plotting rolling mean returns) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This cell's logic is now called within run_full_analysis ---\n",
    "# run_backtesting(main_df, var_results_df, es_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comment cell, integrated into stress testing functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comment cell, integrated into stress testing functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81783367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comment cell, integrated into stress testing functions ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
