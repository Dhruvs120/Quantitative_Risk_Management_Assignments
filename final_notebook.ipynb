{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdedd50",
   "metadata": {},
   "source": [
    "### Initial package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from typing import Dict, List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79e1b1",
   "metadata": {},
   "source": [
    "## Initial files being read in and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e703234",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "# Read the data\n",
    "main_df = pd.read_csv('Data/Cleaned_Indices_Assignment1.csv', sep=';')\n",
    "\n",
    "# Read the interest rate data\n",
    "interest_rate_bond_df = pd.read_csv('Data/ECB_Data_10yr_Treasury_bond.csv', sep=',')\n",
    "\n",
    "# Convert date columns to datetime format for proper merging\n",
    "main_df['Date'] = pd.to_datetime(main_df['Date'], format='%d-%m-%Y')\n",
    "interest_rate_bond_df['Date'] = pd.to_datetime(interest_rate_bond_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Merge the dataframes on the Date column\n",
    "main_df = pd.merge(main_df, interest_rate_bond_df, on='Date', how='left')\n",
    "\n",
    "# Remove rows where the bond does not have a yield curve spot rate (Market closed?)\n",
    "main_df = main_df.dropna(axis=0, subset=['Yield curve spot rate, 10-year maturity - Government bond'])\n",
    "\n",
    "# Filter the dataframe to start from 2012-01-04\n",
    "main_df = main_df[main_df['Date'] >= '2012-01-04']\n",
    "\n",
    "# Set Date as index\n",
    "main_df = main_df.set_index('Date')\n",
    "main_df = main_df.sort_index() # Ensure chronological order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557630d",
   "metadata": {},
   "source": [
    "### government bond column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for the interest bond value per day\n",
    "days_per_annum = 365\n",
    "interest_bond = 1500000\n",
    "\n",
    "# Initialize the arrays with appropriate lengths matching the DataFrame\n",
    "interest_bond_vector = np.zeros(len(main_df))\n",
    "interest_bond_profit_vector = np.zeros(len(main_df))\n",
    "interest_bond_loss_vector = np.zeros(len(main_df))\n",
    "daily_rates = np.zeros(len(main_df))\n",
    "\n",
    "# Set initial value\n",
    "interest_bond_vector[0] = interest_bond\n",
    "\n",
    "# Calculate bond values day by day based on the daily yield rate\n",
    "for i in range(len(main_df)):\n",
    "    # Adding 1.5% to account for the credit risk spread\n",
    "    daily_rate = (((main_df['Yield curve spot rate, 10-year maturity - Government bond'].iloc[i] + 1.5) / (days_per_annum)) * (7/5)) / 100\n",
    "    daily_rates[i] = daily_rate\n",
    "    \n",
    "    if i > 0:\n",
    "        previous_value = interest_bond_vector[i-1]\n",
    "        current_value = previous_value * (1 + daily_rate)\n",
    "        interest_bond_vector[i] = current_value\n",
    "        \n",
    "        # Calculate change, profit/loss and return\n",
    "        change = current_value - previous_value\n",
    "        interest_bond_profit_vector[i] = change\n",
    "        interest_bond_loss_vector[i] = -change\n",
    "\n",
    "# Add vectors to the dataframe\n",
    "main_df['Interest_Bond'] = interest_bond_vector\n",
    "main_df['Interest_Bond_Profit'] = interest_bond_profit_vector\n",
    "main_df['Interest_Bond_Loss'] = interest_bond_loss_vector\n",
    "main_df['Interest_Bond_daily_rate'] = daily_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7900041",
   "metadata": {},
   "source": [
    "## Portfolio details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1ac6f",
   "metadata": {},
   "source": [
    "### details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ae7dd",
   "metadata": {},
   "source": [
    "#### Instruments:\n",
    "- **S&P500**\n",
    "- **DAX40**\n",
    "- **NIKKEI**\n",
    "- **EU Government Bond (10-year maturity, AAA-rated)**\n",
    "\n",
    "#### Invested amount:\n",
    "- **10,000,000 EURO**\n",
    "\n",
    "#### Period:\n",
    "- **01/01/2012 - 31/12/2022**\n",
    "\n",
    "#### Weights:\n",
    "- **S&P500**: 0.4  \n",
    "- **DAX40**: 0.3  \n",
    "- **NIKKEI**: 0.15  \n",
    "- **EU Government Bond**: 0.15  \n",
    "\n",
    "#### Measures:\n",
    "- **Value at Risk (VaR)**: 1, 5, 10 days  \n",
    "- **Expected Shortfall (ES)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3e482",
   "metadata": {},
   "source": [
    "### weights and currency correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial investment \n",
    "weights_dict = {\n",
    "    'S&P500': 0.4,\n",
    "    'DAX40': 0.3,\n",
    "    'NIKKEI': 0.15,\n",
    "    'EU-BOND': 0.15,\n",
    "}\n",
    "weights_array = np.array([weights_dict['S&P500'], weights_dict['DAX40'], weights_dict['NIKKEI'], weights_dict['EU-BOND']])\n",
    "\n",
    "starting_investment = 10000000  # 10 million euros\n",
    "starting_date = pd.to_datetime('2012-01-04')\n",
    "\n",
    "# Get the starting row using the index\n",
    "starting_row = main_df.loc[starting_date]\n",
    "\n",
    "# Extract the exchange rates for the starting date\n",
    "usd_to_eur = float(starting_row['USD/EUR'])\n",
    "jpy_to_eur = float(starting_row['JPY/EUR'])\n",
    "\n",
    "# Calculate the invested amounts\n",
    "invested_amount_SP500 = starting_investment * weights_dict['S&P500'] / usd_to_eur\n",
    "invested_amount_DAX40 = starting_investment * weights_dict['DAX40']\n",
    "invested_amount_NIKKEI = starting_investment * weights_dict['NIKKEI'] / jpy_to_eur\n",
    "invested_amount_EU_BOND = starting_investment * weights_dict['EU-BOND']\n",
    "\n",
    "invested_amounts = [\n",
    "    invested_amount_SP500, #in USD\n",
    "    invested_amount_DAX40, #in EUR\n",
    "    invested_amount_NIKKEI, #in JPY\n",
    "    invested_amount_EU_BOND #in EUR\n",
    "]\n",
    "\n",
    "print(invested_amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac90d7a",
   "metadata": {},
   "source": [
    "### Returns Portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to track investments for each asset\n",
    "# Initialize the first day with the initial invested amounts\n",
    "main_df.loc[starting_date, 'SP500_Investment'] = invested_amount_SP500\n",
    "main_df.loc[starting_date, 'DAX40_Investment'] = invested_amount_DAX40\n",
    "main_df.loc[starting_date, 'NIKKEI_Investment'] = invested_amount_NIKKEI\n",
    "main_df.loc[starting_date, 'EU_BOND_Investment'] = invested_amount_EU_BOND\n",
    "\n",
    "# Calculate daily investment values for subsequent days\n",
    "# This uses cumulative returns to track the value growth\n",
    "for i in range(1, len(main_df)):\n",
    "    current_date = main_df.index[i]\n",
    "    prev_date = main_df.index[i-1]\n",
    "    # S&P 500 in USD\n",
    "    main_df.loc[current_date, 'SP500_Investment'] = main_df.loc[prev_date, 'SP500_Investment'] * (1 + main_df.loc[current_date, 'C_S&P500_Returns'])\n",
    "    \n",
    "    # DAX 40 in EUR\n",
    "    main_df.loc[current_date, 'DAX40_Investment'] = main_df.loc[prev_date, 'DAX40_Investment'] * (1 + main_df.loc[current_date, 'C_Dax40_Returns'])\n",
    "    \n",
    "    # NIKKEI in JPY\n",
    "    main_df.loc[current_date, 'NIKKEI_Investment'] = main_df.loc[prev_date, 'NIKKEI_Investment'] * (1 + main_df.loc[current_date, 'C_Nikkei_Returns'])\n",
    "    \n",
    "# EU Government Bond value is already calculated in the Interest_Bond column\n",
    "main_df['EU_BOND_Investment'] = main_df['Interest_Bond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a055208",
   "metadata": {},
   "source": [
    "# Methods input values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eba751",
   "metadata": {},
   "source": [
    "### Portfolio change Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total portfolio value in EUR for each day\n",
    "main_df['Portfolio_Value_EUR'] = (\n",
    "    main_df['SP500_Investment'].fillna(0) * main_df['USD/EUR'] +\n",
    "    main_df['DAX40_Investment'].fillna(0) +\n",
    "    main_df['NIKKEI_Investment'].fillna(0) * main_df['JPY/EUR'] +\n",
    "    main_df['EU_BOND_Investment'].fillna(0)\n",
    ")\n",
    "\n",
    "# First day should be the initial investment amount\n",
    "main_df.loc[starting_date, 'Portfolio_Value_EUR'] = starting_investment\n",
    "\n",
    "# Calculate the daily change in portfolio value (profit/loss)\n",
    "main_df['Portfolio_Change_EUR'] = main_df['Portfolio_Value_EUR'].diff()\n",
    "main_df.loc[starting_date, 'Portfolio_Change_EUR'] = 0.0  # Set the first day's change to 0\n",
    "\n",
    "# Portfolio loss is the negative of the daily change\n",
    "main_df['Portfolio_loss'] = -main_df['Portfolio_Change_EUR']\n",
    "\n",
    "# Set the first day's loss to 0 (there's no previous day to compare with)\n",
    "main_df.loc[starting_date, 'Portfolio_loss'] = 0.0\n",
    "\n",
    "# Calculate portfolio daily returns (used later)\n",
    "main_df['Portfolio_Daily_Returns'] = main_df['Portfolio_Value_EUR'].pct_change()\n",
    "main_df.loc[starting_date, 'Portfolio_Daily_Returns'] = 0.0\n",
    "\n",
    "# Display the relevant columns to verify\n",
    "display(main_df[['SP500_Investment', 'DAX40_Investment', 'NIKKEI_Investment', \n",
    "                'EU_BOND_Investment', 'USD/EUR', 'JPY/EUR', 'Portfolio_Value_EUR', \n",
    "                'Portfolio_Change_EUR', 'Portfolio_loss', 'Portfolio_Daily_Returns']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8244cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = main_df['Portfolio_loss'].values\n",
    "\n",
    "# Calculate and print the minimum, maximum, and mean of portfolio loss values\n",
    "min_loss = np.nanmin(loss_values)\n",
    "max_loss = np.nanmax(loss_values)\n",
    "mean_loss = np.nanmean(loss_values)\n",
    "\n",
    "print(f\"Portfolio Loss Statistics:\")\n",
    "print(f\"Minimum Loss: {min_loss:.4f}\")\n",
    "print(f\"Maximum Loss: {max_loss:.4f}\")\n",
    "print(f\"Mean Loss: {mean_loss:.4f}\")\n",
    "\n",
    "# Also print the number of valid loss values (non-NaN)\n",
    "valid_count = np.sum(~np.isnan(loss_values))\n",
    "print(f\"Number of valid loss values: {valid_count} out of {len(loss_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_loss_variables(time_window, current_date):\n",
    "    # Calculate the mean and standard deviation of portfolio loss from the time windows\n",
    "    loss_dict = {\n",
    "        \"Date\": current_date,\n",
    "        \"Portfolio_mean_loss\": np.nanmean(time_window['Portfolio_loss']),\n",
    "        \"Portfolio_std_loss\": np.nanstd(time_window['Portfolio_loss'])\n",
    "    }\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8a8a5",
   "metadata": {},
   "source": [
    "### Portfolio variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02ec2f",
   "metadata": {},
   "source": [
    "## Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Get the VaR of the normal or student-t model.\n",
    "    Assumes VaR is for LOSSES (positive value).\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        # Normal distribution: VaR = mu + sigma * Z_alpha\n",
    "        # Since we model losses, VaR = E[Loss] + std(Loss) * Z_alpha\n",
    "        dVaR0 = st.norm.ppf(alpha)\n",
    "        dVaR = r + s*dVaR0\n",
    "    else:\n",
    "        # Student-t distribution\n",
    "        dVaR0 = st.t.ppf(alpha, df= df)\n",
    "        # Scale factor to match volatility\n",
    "        dS2t = df/(df-2) # Variance of standard t-distribution\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dVaR = r + c*dVaR0\n",
    "    return dVaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e23cc",
   "metadata": {},
   "source": [
    "## Expected Shortfall (ES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed569094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Get the ES of the normal/student model for LOSSES.\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        # Normal distribution: ES = mu + sigma * pdf(Z_alpha) / (1-alpha)\n",
    "        dVaR0 = st.norm.ppf(alpha)\n",
    "        dES0 = st.norm.pdf(dVaR0) / (1-alpha)\n",
    "        dES = r + s*dES0\n",
    "    else:\n",
    "        # Student-t distribution\n",
    "        dVaR0 = st.t.ppf(alpha, df= df)\n",
    "        # ES formula for t-distribution\n",
    "        dES0 = st.t.pdf(dVaR0, df= df)*((df + dVaR0**2)/(df-1)) / (1-alpha)\n",
    "        # Scale factor\n",
    "        dS2t = df/(df-2)\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dES = r + c*dES0\n",
    "    return dES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960186b",
   "metadata": {},
   "source": [
    "# performing different methods\n",
    "\n",
    "write method for variance covariance where the sample period is an input parameter alongside other parameters that are needed for the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85279b5e",
   "metadata": {},
   "source": [
    "## 1. var/cov multivar normal dist & T-distribution & Historical\n",
    "\n",
    "Functions to calculate components of Var/cov method and Historical method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa540d",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f75ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var_cov(window, current_date, vAlpha, mean_loss, portfolio_std_loss, df=0):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk and ES using variance-covariance method.\n",
    "    Returns VaR/ES for LOSSES.\n",
    "    \"\"\"\n",
    "    var_results = []\n",
    "    es_results = []\n",
    "    for alpha in vAlpha:\n",
    "        var_results.append(VaR(alpha, mean_loss, portfolio_std_loss, df=df))\n",
    "        es_results.append(ES(alpha, mean_loss, portfolio_std_loss, df=df))\n",
    "    \n",
    "    # Set label for distribution type\n",
    "    if df == 0:\n",
    "        dist_label = \"Normal\"\n",
    "    else:\n",
    "        dist_label = f\"T{df}\"\n",
    "        \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        f'VaR {dist_label}': np.array(var_results),\n",
    "        f'ES {dist_label}': np.array(es_results)\n",
    "    }\n",
    "\n",
    "def calculate_historical_var_es(window, current_date, vAlpha):\n",
    "    \"\"\"\n",
    "    Calculate VaR and ES using historical simulation method for LOSSES.\n",
    "    \"\"\"\n",
    "    # Extract portfolio loss values from the window\n",
    "    historical_losses = window['Portfolio_loss'].dropna()\n",
    "    \n",
    "    # Sort losses in ascending order (higher losses are larger positive numbers)\n",
    "    sorted_losses = np.sort(historical_losses)\n",
    "    \n",
    "    # Calculate VaR for alpha levels\n",
    "    var_hist = np.percentile(sorted_losses, vAlpha * 100)\n",
    "    \n",
    "    # Calculate ES for each alpha level\n",
    "    es_hist = []\n",
    "    for i, alpha in enumerate(vAlpha):\n",
    "        # ES is the mean of losses greater than or equal to VaR\n",
    "        es_val = sorted_losses[sorted_losses >= var_hist[i]].mean()\n",
    "        es_hist.append(es_val)\n",
    "    \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        'VaR Historical': np.array(var_hist),\n",
    "        'ES Historical': np.array(es_hist)\n",
    "    }\n",
    "\n",
    "def calculate_multiday_risk(main_df_indexed, vAlpha, interval, sample_size):\n",
    "    \"\"\"\n",
    "    Calculate multi-day VaR and ES using the historical simulation method.\n",
    "    Returns VaR/ES for LOSSES.\n",
    "    \n",
    "    Parameters:\n",
    "    - main_df_indexed: DataFrame with DatetimeIndex\n",
    "    - vAlpha: Confidence levels (array)\n",
    "    - interval: Number of days for the multi-day calculation (e.g., 5 or 10)\n",
    "    - sample_size: Rolling window size for daily VaR calculation (used for sqrt rule)\n",
    "    \n",
    "    Returns:\n",
    "    - var_multi_df, es_multi_df: DataFrames with multi-day VaR and ES\n",
    "    \"\"\"\n",
    "    # Filter data for the period we want to analyze (excluding initial sample)\n",
    "    analysis_start_date = main_df_indexed.index[sample_size]\n",
    "    time_window_multi = main_df_indexed[main_df_indexed.index >= analysis_start_date].copy()\n",
    "    \n",
    "    # Calculate rolling sum of losses over the interval\n",
    "    time_window_multi[f'Portfolio_loss_{interval}d'] = time_window_multi['Portfolio_loss'].rolling(window=interval).sum()\n",
    "    \n",
    "    # Drop NaNs created by rolling sum\n",
    "    multi_day_losses_df = time_window_multi.dropna(subset=[f'Portfolio_loss_{interval}d'])\n",
    "    \n",
    "    # --- Historical Multi-Day VaR/ES (Regular Method) ---\n",
    "    var_reg_list = []\n",
    "    es_reg_list = []\n",
    "    # Use expanding window for multi-day historical simulation\n",
    "    for i in range(1, len(multi_day_losses_df) + 1):\n",
    "        current_losses = multi_day_losses_df[f'Portfolio_loss_{interval}d'][:i]\n",
    "        sorted_losses = np.sort(current_losses)\n",
    "        var_vals = np.percentile(sorted_losses, vAlpha * 100)\n",
    "        es_vals = []\n",
    "        for j, alpha in enumerate(vAlpha):\n",
    "            es_val = sorted_losses[sorted_losses >= var_vals[j]].mean()\n",
    "            es_vals.append(es_val)\n",
    "        var_reg_list.append(var_vals)\n",
    "        es_reg_list.append(es_vals)\n",
    "        \n",
    "    multi_day_losses_df[f'VaR_{interval}d_Hist_Reg'] = var_reg_list\n",
    "    multi_day_losses_df[f'ES_{interval}d_Hist_Reg'] = es_reg_list\n",
    "\n",
    "    # --- Historical Multi-Day VaR/ES (Sqrt Rule) ---\n",
    "    # Calculate rolling 1-day historical VaR/ES first\n",
    "    daily_var_hist = []\n",
    "    daily_es_hist = []\n",
    "    for i in range(sample_size, len(main_df_indexed)):\n",
    "        window = main_df_indexed.iloc[i - sample_size:i]\n",
    "        hist_res = calculate_historical_var_es(window, main_df_indexed.index[i], vAlpha)\n",
    "        daily_var_hist.append(hist_res['VaR Historical'])\n",
    "        daily_es_hist.append(hist_res['ES Historical'])\n",
    "        \n",
    "    daily_risk_df = pd.DataFrame({\n",
    "        'VaR_1d_Hist': daily_var_hist,\n",
    "        'ES_1d_Hist': daily_es_hist\n",
    "    }, index=main_df_indexed.index[sample_size:])\n",
    "\n",
    "    # Merge daily risk with multi-day losses df\n",
    "    multi_day_losses_df = multi_day_losses_df.merge(daily_risk_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Apply sqrt rule\n",
    "    multi_day_losses_df[f'VaR_{interval}d_Hist_Sqrt'] = multi_day_losses_df['VaR_1d_Hist'] * np.sqrt(interval)\n",
    "    multi_day_losses_df[f'ES_{interval}d_Hist_Sqrt'] = multi_day_losses_df['ES_1d_Hist'] * np.sqrt(interval)\n",
    "\n",
    "    # Prepare output DataFrames\n",
    "    var_cols = [f'VaR_{interval}d_Hist_Reg', f'VaR_{interval}d_Hist_Sqrt']\n",
    "    es_cols = [f'ES_{interval}d_Hist_Reg', f'ES_{interval}d_Hist_Sqrt']\n",
    "    \n",
    "    var_multi_df = multi_day_losses_df[var_cols].copy()\n",
    "    es_multi_df = multi_day_losses_df[es_cols].copy()\n",
    "    # Add the actual loss column for plotting/backtesting\n",
    "    var_multi_df[f'Actual_Loss_{interval}d'] = multi_day_losses_df[f'Portfolio_loss_{interval}d']\n",
    "    es_multi_df[f'Actual_Loss_{interval}d'] = multi_day_losses_df[f'Portfolio_loss_{interval}d']\n",
    "\n",
    "    return var_multi_df, es_multi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize lists to store results\n",
    "    VaR_results = []\n",
    "    ES_results = []\n",
    "    # Define time window\n",
    "    time_window = main_df[(main_df['Date'] >= '2012-01-05') & (main_df['Date'] <= '2021-12-31')]\n",
    "\n",
    "    # Define confidence levels\n",
    "    vAlpha = np.array([0.95, 0.99])\n",
    "    \n",
    "    # Define sample size and t-distribution degrees of freedom\n",
    "    sample_size = 500\n",
    "    degrees_of_freedom = [0, 3, 4, 5, 6]  # 0 represents normal distribution\n",
    "    \n",
    "    for i in range(sample_size, len(time_window)):\n",
    "        # Extract the rolling window\n",
    "        window = time_window.iloc[i - sample_size:i]\n",
    "        current_date = time_window.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate loss statistics\n",
    "        loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "        mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "        portfolio_std_loss = loss_stats[\"Portfolio_std_loss\"]\n",
    "        \n",
    "        # Initialize result dictionaries for this date\n",
    "        var_row = {'Date': current_date}\n",
    "        es_row = {'Date': current_date}\n",
    "\n",
    "        \n",
    "        # Calculate VaR and ES using various distributions\n",
    "        for df in degrees_of_freedom:\n",
    "            results = calculate_var_cov(window, current_date, vAlpha, mean_loss, portfolio_std_loss, df)\n",
    "            \n",
    "            # Get the distribution label\n",
    "            if df == 0:\n",
    "                dist_label = \"Normal\"\n",
    "            else:\n",
    "                dist_label = f\"T{df}\"\n",
    "                \n",
    "            # Add results to the dictionaries\n",
    "            var_row[f'VaR {dist_label}'] = results[f'VaR {dist_label}']\n",
    "            es_row[f'ES {dist_label}'] = results[f'ES {dist_label}']\n",
    "        \n",
    "        # Calculate VaR and ES using historical simulation\n",
    "        hist_results = calculate_historical_var_es(window, current_date, vAlpha)\n",
    "        var_row['VaR Historical'] = hist_results['VaR Historical']\n",
    "        es_row['ES Historical'] = hist_results['ES Historical']\n",
    "        \n",
    "        # Add results for this date\n",
    "        VaR_results.append(var_row)\n",
    "        ES_results.append(es_row)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    var_results_df = pd.DataFrame(VaR_results)\n",
    "    es_results_df = pd.DataFrame(ES_results)\n",
    "\n",
    "    var_5d = calculate_multiday_var(vAlpha, 5, sample_size)\n",
    "    var_10d = calculate_multiday_var(vAlpha, 10, sample_size)\n",
    "\n",
    "    return var_results_df, es_results_df, var_5d, var_10d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_results_df, es_results_df, var_5d, var_10d = main()\n",
    "print(\"VaR results\")\n",
    "display(var_results_df.head())\n",
    "print(\"ES results\")\n",
    "display(es_results_df.head())\n",
    "print(\"5-day VaR results\")\n",
    "display(var_5d.head())\n",
    "print(\"10-day VaR results\")\n",
    "display(var_10d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot for 5-day and 10-day VaR over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=False)\n",
    "\n",
    "# ------- 5-day VaR Plot ------- \n",
    "# Extract regular and sqrt VaR values for 95% and 99% levels\n",
    "var5d_reg_95 = [row[0] for row in var_5d['VaR_5d_reg']]\n",
    "var5d_reg_99 = [row[1] for row in var_5d['VaR_5d_reg']]\n",
    "var5d_sqrt_95 = [row[0] for row in var_5d['VaR_5d_sqrt']]\n",
    "var5d_sqrt_99 = [row[1] for row in var_5d['VaR_5d_sqrt']]\n",
    "\n",
    "# Plot actual portfolio losses\n",
    "ax1.plot(var_5d['Date'], var_5d['Portfolio_loss'], 'k-', alpha=0.3, label='Actual 5-day Loss')\n",
    "\n",
    "# Plot regular and sqrt VaR values\n",
    "ax1.plot(var_5d['Date'], var5d_reg_95, 'b-', label='5-day VaR 95% (Regular)')\n",
    "ax1.plot(var_5d['Date'], var5d_reg_99, 'r-', label='5-day VaR 99% (Regular)')\n",
    "ax1.plot(var_5d['Date'], var5d_sqrt_95, 'b--', label='5-day VaR 95% (Sqrt Rule)')\n",
    "ax1.plot(var_5d['Date'], var5d_sqrt_99, 'r--', label='5-day VaR 99% (Sqrt Rule)')\n",
    "\n",
    "# Customize 5-day plot\n",
    "ax1.set_title('5-Day Value at Risk Over Time', fontsize=14)\n",
    "ax1.set_ylabel('Value (EUR)', fontsize=12)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ------- 10-day VaR Plot ------- \n",
    "# Extract regular and sqrt VaR values for 95% and 99% levels\n",
    "var10d_reg_95 = [row[0] for row in var_10d['VaR_10d_reg']]\n",
    "var10d_reg_99 = [row[1] for row in var_10d['VaR_10d_reg']]\n",
    "var10d_sqrt_95 = [row[0] for row in var_10d['VaR_10d_sqrt']]\n",
    "var10d_sqrt_99 = [row[1] for row in var_10d['VaR_10d_sqrt']]\n",
    "\n",
    "# Plot actual portfolio losses\n",
    "ax2.plot(var_10d['Date'], var_10d['Portfolio_loss'], 'k-', alpha=0.3, label='Actual 10-day Loss')\n",
    "\n",
    "# Plot regular and sqrt VaR values\n",
    "ax2.plot(var_10d['Date'], var10d_reg_95, 'b-', label='10-day VaR 95% (Regular)')\n",
    "ax2.plot(var_10d['Date'], var10d_reg_99, 'r-', label='10-day VaR 99% (Regular)')\n",
    "ax2.plot(var_10d['Date'], var10d_sqrt_95, 'b--', label='10-day VaR 95% (Sqrt Rule)')\n",
    "ax2.plot(var_10d['Date'], var10d_sqrt_99, 'r--', label='10-day VaR 99% (Sqrt Rule)')\n",
    "\n",
    "# Customize 10-day plot\n",
    "ax2.set_title('10-Day Value at Risk Over Time', fontsize=14)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Value (EUR)', fontsize=12)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout to prevent clipping of labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a plot to compare the violations between 5-day and 10-day VaR\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=False)\n",
    "\n",
    "# ----- 5-day VaR Violations -----\n",
    "# Calculate violations (when actual loss exceeds VaR)\n",
    "violations_5d_reg_95 = var_5d['Portfolio_loss'] > np.array(var5d_reg_95)\n",
    "violations_5d_reg_99 = var_5d['Portfolio_loss'] > np.array(var5d_reg_99)\n",
    "violations_5d_sqrt_95 = var_5d['Portfolio_loss'] > np.array(var5d_sqrt_95)\n",
    "violations_5d_sqrt_99 = var_5d['Portfolio_loss'] > np.array(var5d_sqrt_99)\n",
    "\n",
    "# Plot violations\n",
    "ax1.scatter(var_5d['Date'][violations_5d_reg_95], [1]*sum(violations_5d_reg_95), color='blue', marker='o', label='95% Regular')\n",
    "ax1.scatter(var_5d['Date'][violations_5d_reg_99], [2]*sum(violations_5d_reg_99), color='red', marker='o', label='99% Regular')\n",
    "ax1.scatter(var_5d['Date'][violations_5d_sqrt_95], [3]*sum(violations_5d_sqrt_95), color='blue', marker='x', label='95% Sqrt Rule')\n",
    "ax1.scatter(var_5d['Date'][violations_5d_sqrt_99], [4]*sum(violations_5d_sqrt_99), color='red', marker='x', label='99% Sqrt Rule')\n",
    "\n",
    "# Add violation counts to the legend\n",
    "ax1.set_title(f'5-Day VaR Violations\\n' + \n",
    "             f'95% Regular: {sum(violations_5d_reg_95)} ({sum(violations_5d_reg_95)/len(var_5d)*100:.2f}%), ' + \n",
    "             f'99% Regular: {sum(violations_5d_reg_99)} ({sum(violations_5d_reg_99)/len(var_5d)*100:.2f}%)\\n' + \n",
    "             f'95% Sqrt: {sum(violations_5d_sqrt_95)} ({sum(violations_5d_sqrt_95)/len(var_5d)*100:.2f}%), ' + \n",
    "             f'99% Sqrt: {sum(violations_5d_sqrt_99)} ({sum(violations_5d_sqrt_99)/len(var_5d)*100:.2f}%)', \n",
    "             fontsize=12)\n",
    "             \n",
    "ax1.set_yticks([1, 2, 3, 4])\n",
    "ax1.set_yticklabels(['95% Reg', '99% Reg', '95% Sqrt', '99% Sqrt'])\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# ----- 10-day VaR Violations -----\n",
    "# Calculate violations (when actual loss exceeds VaR)\n",
    "violations_10d_reg_95 = var_10d['Portfolio_loss'] > np.array(var10d_reg_95)\n",
    "violations_10d_reg_99 = var_10d['Portfolio_loss'] > np.array(var10d_reg_99)\n",
    "violations_10d_sqrt_95 = var_10d['Portfolio_loss'] > np.array(var10d_sqrt_95)\n",
    "violations_10d_sqrt_99 = var_10d['Portfolio_loss'] > np.array(var10d_sqrt_99)\n",
    "\n",
    "# Plot violations\n",
    "ax2.scatter(var_10d['Date'][violations_10d_reg_95], [1]*sum(violations_10d_reg_95), color='blue', marker='o', label='95% Regular')\n",
    "ax2.scatter(var_10d['Date'][violations_10d_reg_99], [2]*sum(violations_10d_reg_99), color='red', marker='o', label='99% Regular')\n",
    "ax2.scatter(var_10d['Date'][violations_10d_sqrt_95], [3]*sum(violations_10d_sqrt_95), color='blue', marker='x', label='95% Sqrt Rule')\n",
    "ax2.scatter(var_10d['Date'][violations_10d_sqrt_99], [4]*sum(violations_10d_sqrt_99), color='red', marker='x', label='99% Sqrt Rule')\n",
    "\n",
    "# Add violation counts to the legend\n",
    "ax2.set_title(f'10-Day VaR Violations\\n' + \n",
    "             f'95% Regular: {sum(violations_10d_reg_95)} ({sum(violations_10d_reg_95)/len(var_10d)*100:.2f}%), ' + \n",
    "             f'99% Regular: {sum(violations_10d_reg_99)} ({sum(violations_10d_reg_99)/len(var_10d)*100:.2f}%)\\n' + \n",
    "             f'95% Sqrt: {sum(violations_10d_sqrt_95)} ({sum(violations_10d_sqrt_95)/len(var_10d)*100:.2f}%), ' + \n",
    "             f'99% Sqrt: {sum(violations_10d_sqrt_99)} ({sum(violations_10d_sqrt_99)/len(var_10d)*100:.2f}%)', \n",
    "             fontsize=12)\n",
    "             \n",
    "ax2.set_yticks([1, 2, 3, 4])\n",
    "ax2.set_yticklabels(['95% Reg', '99% Reg', '95% Sqrt', '99% Sqrt'])\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Historical VaR and ES over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract the values for different confidence levels from the arrays\n",
    "var_95 = [row[0] for row in var_results_df['VaR Historical']]\n",
    "var_99 = [row[1] for row in var_results_df['VaR Historical']]\n",
    "es_95 = [row[0] for row in es_results_df['ES Historical']]\n",
    "es_99 = [row[1] for row in es_results_df['ES Historical']]\n",
    "\n",
    "# Plot VaR Historical 95% and 99%\n",
    "plt.plot(var_results_df['Date'], var_95, label='VaR Historical 95%', color='blue')\n",
    "plt.plot(var_results_df['Date'], var_99, label='VaR Historical 99%', color='red')\n",
    "\n",
    "# Plot ES Historical 95% and 99%\n",
    "plt.plot(es_results_df['Date'], es_95, label='ES Historical 95%', color='green', linestyle='--')\n",
    "plt.plot(es_results_df['Date'], es_99, label='ES Historical 99%', color='orange', linestyle='--')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Historical VaR and ES Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b028c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for all indices\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# S&P 500\n",
    "sp500_returns = main_df['C_S&P500_Returns'].dropna()\n",
    "mu_sp500 = sp500_returns.mean()\n",
    "sigma_sp500 = sp500_returns.std()\n",
    "x_sp500 = np.linspace(mu_sp500 - 4*sigma_sp500, mu_sp500 + 4*sigma_sp500, 100)\n",
    "ax1.hist(sp500_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax1.plot(x_sp500, st.norm.pdf(x_sp500, mu_sp500, sigma_sp500), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_sp500 / np.sqrt(df/(df-2))\n",
    "    ax1.plot(x_sp500, st.t.pdf((x_sp500-mu_sp500)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax1.set_title('S&P500 Returns Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# DAX40\n",
    "dax_returns = main_df['C_Dax40_Returns'].dropna()\n",
    "mu_dax = dax_returns.mean()\n",
    "sigma_dax = dax_returns.std()\n",
    "x_dax = np.linspace(mu_dax - 4*sigma_dax, mu_dax + 4*sigma_dax, 100)\n",
    "ax2.hist(dax_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax2.plot(x_dax, st.norm.pdf(x_dax, mu_dax, sigma_dax), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_dax / np.sqrt(df/(df-2))\n",
    "    ax2.plot(x_dax, st.t.pdf((x_dax-mu_dax)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax2.set_title('DAX40 Returns Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# NIKKEI\n",
    "nikkei_returns = main_df['C_Nikkei_Returns'].dropna()\n",
    "mu_nikkei = nikkei_returns.mean()\n",
    "sigma_nikkei = nikkei_returns.std()\n",
    "x_nikkei = np.linspace(mu_nikkei - 4*sigma_nikkei, mu_nikkei + 4*sigma_nikkei, 100)\n",
    "ax3.hist(nikkei_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax3.plot(x_nikkei, st.norm.pdf(x_nikkei, mu_nikkei, sigma_nikkei), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_nikkei / np.sqrt(df/(df-2))\n",
    "    ax3.plot(x_nikkei, st.t.pdf((x_nikkei-mu_nikkei)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax3.set_title('NIKKEI Returns Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# EU Bond\n",
    "bond_returns = main_df['Interest_Bond_daily_rate'].dropna()\n",
    "mu_bond = bond_returns.mean()\n",
    "sigma_bond = bond_returns.std()\n",
    "x_bond = np.linspace(mu_bond - 4*sigma_bond, mu_bond + 4*sigma_bond, 100)\n",
    "ax4.hist(bond_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax4.plot(x_bond, st.norm.pdf(x_bond, mu_bond, sigma_bond), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_bond / np.sqrt(df/(df-2))\n",
    "    ax4.plot(x_bond, st.t.pdf((x_bond-mu_bond)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax4.set_title('EU Bond Returns Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ce29e",
   "metadata": {},
   "source": [
    "## EWMA & Filtered Historical Simulation (FHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "def compute_ewma_volatility(\n",
    "    returns: pd.DataFrame, \n",
    "    lambdas: List[float] = [0.94, 0.97]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute EWMA volatility for each risk factor using different lambda values.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    returns : pd.DataFrame\n",
    "        DataFrame of returns (T x N), excluding Date column\n",
    "    lambdas : List[float], optional\n",
    "        List of smoothing factors, by default [0.94, 0.97]\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, pd.DataFrame]\n",
    "        Dictionary of DataFrames containing EWMA volatilities for each lambda\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(returns, pd.DataFrame):\n",
    "        raise TypeError(\"returns must be a pandas DataFrame\")\n",
    "    \n",
    "    # Remove Date column if present\n",
    "    if 'Date' in returns.columns:\n",
    "        returns = returns.drop('Date', axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        n_obs, n_assets = returns.shape\n",
    "        ewma_vol = np.zeros((n_obs, n_assets))\n",
    "        \n",
    "        # Initialize first value with sample standard deviation\n",
    "        ewma_vol[0] = returns.iloc[0].values.std()\n",
    "        \n",
    "        # Loop through time to apply EWMA formula\n",
    "        for t in range(1, n_obs):\n",
    "            ewma_vol[t] = np.sqrt(\n",
    "                lambda_ * ewma_vol[t-1]**2 + \n",
    "                (1 - lambda_) * returns.iloc[t-1].values**2\n",
    "            )\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        results[f'lambda_{lambda_}'] = pd.DataFrame(\n",
    "            ewma_vol,\n",
    "            index=returns.index,\n",
    "            columns=returns.columns\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "returns_df = main_df[['Date', 'C_S&P500_Returns', 'C_Dax40_Returns', \n",
    "                      'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "#set Date as index\n",
    "returns_df.set_index('Date', inplace=True)\n",
    "ewma_results = compute_ewma_volatility(returns_df)\n",
    "print(ewma_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_returns(returns: pd.DataFrame, ewma_vol: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Compute standardized (filtered) returns for each lambda value.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of raw returns\n",
    "    - ewma_vol: Dictionary containing DataFrames of EWMA volatilities for each lambda\n",
    "\n",
    "    Returns:\n",
    "    - filtered_returns: Dictionary of DataFrames with standardized returns for each lambda\n",
    "    \"\"\"\n",
    "    filtered_returns = {}\n",
    "    \n",
    "    # Filter returns for each lambda value\n",
    "    for lambda_key, vol_df in ewma_vol.items():\n",
    "        filtered_returns[lambda_key] = returns / vol_df\n",
    "    \n",
    "    return filtered_returns\n",
    "\n",
    "# Assuming returns_df and ewma_vol_df (from compute_ewma_volatility) are already defined\n",
    "filtered_returns_dict = filter_returns(returns_df, ewma_results)\n",
    "\n",
    "# Preview results for both lambda values\n",
    "print(\"\\nFiltered Returns (lambda = 0.94):\")\n",
    "print(filtered_returns_dict['lambda_0.94'].tail())\n",
    "print(\"\\nFiltered Returns (lambda = 0.97):\")\n",
    "print(filtered_returns_dict['lambda_0.97'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_historical_simulation_multivariate(filtered_returns_dict: dict, ewma_vol_dict: dict,\n",
    "                                              n_simulations: int = 10000, random_seed: int = None, \n",
    "                                              weights: np.ndarray = None) -> dict:\n",
    "    \"\"\"\n",
    "    Perform Filtered Historical Simulation for a multi-asset portfolio for different lambda values.\n",
    "\n",
    "    Parameters:\n",
    "    - filtered_returns_dict: Dictionary of DataFrames of standardized residuals for each lambda\n",
    "    - ewma_vol_dict: Dictionary of DataFrames of EWMA volatility for each lambda\n",
    "    - n_simulations: number of simulated return vectors\n",
    "    - random_seed: for reproducibility\n",
    "    - weights: portfolio weights (numpy array of shape [n_assets])\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing simulated portfolio returns for each lambda value\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process each lambda value\n",
    "    for lambda_key in filtered_returns_dict.keys():\n",
    "        filtered_returns = filtered_returns_dict[lambda_key]\n",
    "        # print(filtered_returns)\n",
    "        ewma_vol = ewma_vol_dict[lambda_key]\n",
    "        \n",
    "        assets = filtered_returns.columns\n",
    "        print(f\"Assets: {assets}\")\n",
    "        n_assets = len(assets)\n",
    "        \n",
    "        # Initialize simulated return matrix (n_simulations x n_assets)\n",
    "        sim_returns = np.zeros((n_simulations, n_assets))\n",
    "\n",
    "        for i, asset in enumerate(assets):\n",
    "            if asset == 'Date':\n",
    "                continue\n",
    "            z_asset = filtered_returns[asset].dropna().values\n",
    "            print(f\"z_asset: {z_asset}\")\n",
    "            z_star = np.random.choice(z_asset, size=n_simulations, replace=True)\n",
    "            print(f\"z_star: {z_star}\")\n",
    "            sigma_t = ewma_vol[asset].iloc[-1]  # latest volatility for asset\n",
    "            sim_returns[:, i] = sigma_t * z_star  # re-scale\n",
    "\n",
    "        if weights is not None:\n",
    "            portfolio_simulated_returns = sim_returns @ weights\n",
    "            results[lambda_key] = pd.Series(portfolio_simulated_returns, \n",
    "                                          name=f\"Simulated_Portfolio_Returns_{lambda_key}\")\n",
    "        else:\n",
    "            results[lambda_key] = pd.DataFrame(sim_returns, columns=assets)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define weights in the same order as your DataFrame columns\n",
    "weights = np.array([0.4, 0.3, 0.15, 0.15])  # Example: S&P500, DAX, Nikkei, Bond rate\n",
    "\n",
    "# Run multivariate FHS simulation for both lambda values\n",
    "simulated_returns = filtered_historical_simulation_multivariate(\n",
    "    filtered_returns_dict,\n",
    "    ewma_results,\n",
    "    n_simulations=10000,\n",
    "    random_seed=42,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "# Compute VaR and ES for 95% and 99% for each lambda value\n",
    "confidence_levels = [0.95, 0.99]\n",
    "results = {}\n",
    "\n",
    "for lambda_key, sim_returns in simulated_returns.items():\n",
    "    for cl in confidence_levels:\n",
    "        alpha = 1 - cl\n",
    "        percentile = alpha * 100\n",
    "        var = -np.percentile(sim_returns, percentile)\n",
    "        es = -sim_returns[sim_returns <= -var].mean()\n",
    "        results[f\"{lambda_key}_VaR_{int(cl * 100)}\"] = var\n",
    "        results[f\"{lambda_key}_ES_{int(cl * 100)}\"] = es\n",
    "\n",
    "# Print results\n",
    "for lambda_key in simulated_returns.keys():\n",
    "    print(f\"\\nResults for {lambda_key}:\")\n",
    "    for cl in confidence_levels:\n",
    "        print(f\"Portfolio 1-day VaR ({int(cl * 100)}%): {results[f'{lambda_key}_VaR_{int(cl * 100)}']:.5f}\")\n",
    "        print(f\"Portfolio 1-day ES  ({int(cl * 100)}%): {results[f'{lambda_key}_ES_{int(cl * 100)}']:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_historical_simulation_multiday(\n",
    "    filtered_returns_dict: dict,\n",
    "    ewma_vol_dict: dict,\n",
    "    lambda_key: str,\n",
    "    n_days: int = 1,\n",
    "    n_simulations: int = 10000,\n",
    "    random_seed: int = None,\n",
    "    weights: np.ndarray = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Simulate N-day portfolio returns using Filtered Historical Simulation.\n",
    "\n",
    "    Returns:\n",
    "    - Simulated N-day portfolio return series (n_simulations,)\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    filtered_returns = filtered_returns_dict[lambda_key]\n",
    "    ewma_vol = ewma_vol_dict[lambda_key]\n",
    "\n",
    "    assets = filtered_returns.columns\n",
    "    n_assets = len(assets)\n",
    "    sim_returns = np.zeros((n_simulations, n_days, n_assets))\n",
    "\n",
    "    for i, asset in enumerate(assets):\n",
    "        z_asset = filtered_returns[asset].dropna().values\n",
    "        sigma_t = ewma_vol[asset].iloc[-1]\n",
    "\n",
    "        if len(z_asset) == 0 or np.isnan(sigma_t):\n",
    "            raise ValueError(f\"Cannot simulate for asset '{asset}': empty or invalid data.\")\n",
    "\n",
    "        for day in range(n_days):\n",
    "            z_star = np.random.choice(z_asset, size=n_simulations, replace=True)\n",
    "            sim_returns[:, day, i] = sigma_t * z_star\n",
    "\n",
    "    # Combine all simulated daily returns into N-day portfolio PnL\n",
    "    total_pnl = (sim_returns @ weights).sum(axis=1)\n",
    "\n",
    "    return pd.Series(total_pnl, name=f\"Simulated_{n_days}Day_Returns_{lambda_key}\")\n",
    "\n",
    "\n",
    "# Example usage with both lambda values\n",
    "confidence_levels = [0.95, 0.99]\n",
    "horizons = [1, 5, 10]\n",
    "lambda_keys = ['lambda_0.94', 'lambda_0.97']\n",
    "\n",
    "for lambda_key in lambda_keys:\n",
    "    print(f\"\\n=== Results for {lambda_key} ===\")\n",
    "    for days in horizons:\n",
    "        print(f\"\\n--- {days}-Day VaR & ES ---\")\n",
    "        sim_returns = filtered_historical_simulation_multiday(\n",
    "            filtered_returns_dict,\n",
    "            ewma_results,\n",
    "            lambda_key,\n",
    "            n_days=days,\n",
    "            n_simulations=10000,\n",
    "            random_seed=42,\n",
    "            weights=weights\n",
    "        )\n",
    "\n",
    "        for cl in confidence_levels:\n",
    "            alpha = 1 - cl\n",
    "            percentile = alpha * 100\n",
    "            var = -np.percentile(sim_returns, percentile)\n",
    "            es = -sim_returns[sim_returns <= -var].mean()\n",
    "            print(f\"VaR ({int(cl*100)}%): {var:.5f} | ES ({int(cl*100)}%): {es:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_fhs_multiday_var_es(\n",
    "    returns_df: pd.DataFrame,\n",
    "    weights: np.ndarray,\n",
    "    window_size: int = 500,\n",
    "    horizons: list = [1, 5, 10],\n",
    "    confidence_levels: list = [0.95, 0.99],\n",
    "    n_simulations: int = 1000,\n",
    "    lambdas: list = [0.94, 0.97],\n",
    "    random_seed: int = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Rolling Filtered Historical Simulation for multi-day VaR & ES.\n",
    "    \"\"\"\n",
    "    time_window = returns_df.loc['2012-01-05':'2021-12-31']\n",
    "\n",
    "    var_results = []\n",
    "    es_results = []\n",
    "\n",
    "    for horizon in horizons:\n",
    "        adjusted_window_size = int(window_size / horizon)\n",
    "\n",
    "        for t in range(adjusted_window_size, len(time_window)):\n",
    "            current_date = time_window.index[t]\n",
    "            var_row = {'index': current_date}\n",
    "            es_row = {'index': current_date}\n",
    "\n",
    "            window = time_window.iloc[t - adjusted_window_size:t]\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Compute EWMA vol\n",
    "                ewma_results = compute_ewma_volatility(\n",
    "                    window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']],\n",
    "                    lambdas\n",
    "                )\n",
    "                # Filtered returns\n",
    "                filtered_returns_dict = {}\n",
    "                for lambda_key, vol_df in ewma_results.items():\n",
    "                    safe_vol_df = vol_df.replace(0, np.nan).fillna(method='ffill')\n",
    "                    filtered_returns = window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']] / safe_vol_df\n",
    "                    filtered_returns_dict[lambda_key] = filtered_returns\n",
    "\n",
    "                # Simulate for each lambda and confidence level\n",
    "                for lambda_key in ewma_results.keys():\n",
    "                    sim_returns = filtered_historical_simulation_multiday(\n",
    "                        filtered_returns_dict,\n",
    "                        ewma_results,\n",
    "                        lambda_key,\n",
    "                        n_days=horizon,\n",
    "                        n_simulations=n_simulations,\n",
    "                        weights=weights,\n",
    "                        random_seed=random_seed\n",
    "                    )\n",
    "\n",
    "                    for cl in confidence_levels:\n",
    "                        alpha = 1 - cl\n",
    "                        var = np.percentile(sim_returns, 100 * alpha)\n",
    "                        es = sim_returns[sim_returns <= var].mean()\n",
    "\n",
    "                        var_key = f\"VaR_{int(cl * 100)}_{lambda_key}_h{horizon}\"\n",
    "                        es_key = f\"ES_{int(cl * 100)}_{lambda_key}_h{horizon}\"\n",
    "\n",
    "                        var_row[var_key] = var\n",
    "                        es_row[es_key] = es\n",
    "\n",
    "                    var_results.append(var_row)\n",
    "                    es_results.append(es_row)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    var_df = pd.DataFrame(var_results).set_index('index')\n",
    "    es_df = pd.DataFrame(es_results).set_index('index')\n",
    "\n",
    "    var_df.index.name = 'Date'\n",
    "    es_df.index.name = 'Date'\n",
    "\n",
    "    return var_df, es_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc14ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_var_es_vs_actual_given_actuals(var_df, es_df, actual_returns_dict, lambdas, horizons, confidence_levels):\n",
    "    \"\"\"\n",
    "    Plot VaR and ES against already computed actual portfolio returns.\n",
    "\n",
    "    Parameters:\n",
    "    - var_df: DataFrame of rolling VaR values\n",
    "    - es_df: DataFrame of rolling ES values\n",
    "    - actual_returns_dict: Dict of actual portfolio return Series keyed by horizon\n",
    "    - lambdas: list of lambda values used (e.g. [0.94, 0.97])\n",
    "    - horizons: list of horizon days (e.g. [1, 5, 10])\n",
    "    - confidence_levels: list of confidence levels (e.g. [0.95, 0.99])\n",
    "    \"\"\"\n",
    "    for h in horizons:\n",
    "        actual_returns = actual_returns_dict[h].reindex(var_df.index)\n",
    "\n",
    "        for lambda_ in lambdas:\n",
    "            lambda_key = f\"lambda_{lambda_}\"\n",
    "\n",
    "            for cl in confidence_levels:\n",
    "                var_col = f\"VaR_{int(cl * 100)}_{lambda_key}_h{h}\"\n",
    "                es_col = f\"ES_{int(cl * 100)}_{lambda_key}_h{h}\"\n",
    "\n",
    "                plt.figure(figsize=(14, 5))\n",
    "                plt.plot(actual_returns, label=\"Actual Portfolio Return\", alpha=0.6)\n",
    "                plt.plot(-var_df[var_col], label=f\"VaR {int(cl * 100)}%\", color='red', linestyle='--')\n",
    "                plt.plot(-es_df[es_col], label=f\"ES {int(cl * 100)}%\", color='orange', linestyle=':')\n",
    "                plt.title(f\"{h}-Day VaR and ES vs Actual Returns | ={lambda_} | CL={int(cl*100)}%\")\n",
    "                plt.axhline(0, color='gray', linestyle='-')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89509d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actual_portfolio_returns(returns_df, weights, horizons=[1, 5, 10]):\n",
    "    actual_returns = {}\n",
    "    weighted_returns = returns_df @ weights\n",
    "\n",
    "    for h in horizons:\n",
    "        actual_returns[h] = weighted_returns.rolling(window=h).sum().shift(-h + 1)\n",
    "        actual_returns[h].name = f\"Actual_{h}d\"\n",
    "\n",
    "    return actual_returns\n",
    "\n",
    "actual_returns_dict = compute_actual_portfolio_returns(returns_df, weights)\n",
    "returns_dfe = pd.DataFrame(actual_returns_dict)\n",
    "display(returns_dfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VaR and ES using rolling FHS\n",
    "var_df, es_df = rolling_fhs_multiday_var_es(\n",
    "    returns_df=returns_df,  # DataFrame containing returns\n",
    "    weights=weights,        # Portfolio weights\n",
    "    window_size=500,       # Base window size\n",
    "    horizons=[1, 5, 10],   # Horizons for VaR calculation\n",
    "    confidence_levels=[0.95, 0.99],  # Confidence levels\n",
    "    n_simulations=1000,    # Number of simulations\n",
    "    lambdas=[0.94, 0.97]   # EWMA lambda values\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "plot_var_es_vs_actual_given_actuals(\n",
    "    var_df=var_df,\n",
    "    es_df=es_df,\n",
    "    actual_returns_dict=actual_returns_dict,\n",
    "    lambdas=[0.94, 0.97],\n",
    "    horizons=[1, 5, 10],\n",
    "    confidence_levels=[0.95, 0.99]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b28be",
   "metadata": {},
   "source": [
    "## GARCH(1,1) with constant conditional correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#maximum likelihood estimation of GARCH(1,1) parameters\n",
    "def garch_likelihood(params, returns):\n",
    "    omega, alpha, beta = params\n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "    var[0] = omega / (1 - alpha - beta)\n",
    "    ll = 0\n",
    "    for t in range(2, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "        ll += 0.5 * (np.log(2 * np.pi) + np.log(var[t]) + returns[t]**2 / var[t])\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) parameter estimation using MLE\n",
    "# This function estimates the parameters of a GARCH(1,1) model using maximum likelihood estimation (MLE).\n",
    "def parameter_estimation_GARCH(returns):\n",
    "    # Run the optimization\n",
    "    result = minimize(\n",
    "        garch_likelihood,\n",
    "        x0=[0.02, 0.13, 0.86],\n",
    "        args=(returns,),\n",
    "        method='SLSQP',\n",
    "        bounds=[(1e-6, None), (0, 0.99), (0, 0.99)],\n",
    "        constraints=[\n",
    "            {'type': 'ineq', 'fun': lambda x: 0.999 - x[1] - x[2]}\n",
    "        ],\n",
    "        options={'disp': True}\n",
    "    )\n",
    "\n",
    "    # Return the optimization result\n",
    "    return result\n",
    "# result = parameter_estimation_GARCH(main_df['Portfolio_Daily_Returns'].dropna())\n",
    "# Check the optimization result\n",
    "# if result.success:\n",
    "#     print(f\"Optimized parameters: omega={result.x[0]}, alpha={result.x[1]}, beta={result.x[2]}\")\n",
    "# else:\n",
    "#     print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5934ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the GARCH(1,1) volatility for returns t\n",
    "def garch_volatility(returns):\n",
    "    param = [0.000002, 0.13, 0.86]\n",
    "    param = type('obj', (object,), {'x': param})\n",
    "    omega, alpha, beta = param.x\n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "\n",
    "    var[0]= param.x[0] / (1 - param.x[1] - param.x[2])\n",
    "    for t in range(2, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "    return np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation matrix of the 4 time series\n",
    "# the method must take the returns as matrix of 4 vectors and return the correlation matrix\n",
    "def correlation_matrix(returns):\n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.cov(returns.T)\n",
    "    \n",
    "    # Calculate the standard deviations of each asset\n",
    "    std_devs = np.sqrt(np.diag(cov_matrix))\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = cov_matrix / np.outer(std_devs, std_devs)\n",
    "    \n",
    "    return corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the diagonal matrix with the garch volatility of the 4 time series\n",
    "def diagonal_matrix(returns):\n",
    "    # Initialize an empty dictionary to store volatilities for each column\n",
    "    volatilities_dict = {}\n",
    "\n",
    "    # Iterate through each column in the returns DataFrame\n",
    "    for column in returns.columns:\n",
    "        # Reset the index of the column to ensure numeric indexing\n",
    "        column_returns = returns[column].dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the GARCH(1,1) volatility for the column\n",
    "        volatilities_dict[column] = garch_volatility(column_returns)\n",
    "        \n",
    "    # Combine the volatilities into a single array\n",
    "    volatilities = np.array([vol[-1] for vol in volatilities_dict.values()])\n",
    "\n",
    "    # Create a diagonal matrix with the volatilities\n",
    "    #diag_matrix = np.diag(volatilities)\n",
    "    \n",
    "    return volatilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the covariance matrix of the 4 time series\n",
    "def covariance_matrix(returns, corr_matrix):\n",
    "    # Calculate the GARCH(1,1) volatilities\n",
    "    volatilities = diagonal_matrix(returns)\n",
    "    \n",
    "    # # Calculate the correlation matrix\n",
    "    # corr_matrix = correlation_matrix(returns)\n",
    "    \n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.outer(volatilities, volatilities) * corr_matrix\n",
    "    \n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac630cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the portfolio variance and volatility\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    # Calculate the portfolio variance\n",
    "    port_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    \n",
    "    # Calculate the portfolio volatility\n",
    "    port_volatility = np.sqrt(port_variance)\n",
    "    \n",
    "    return port_variance, port_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the VaR and ES of the portfolio using the normal distribution\n",
    "def portfolio_VaR_ES(weights, cov_matrix, alpha=0.99):\n",
    "    # Calculate the portfolio variance and volatility\n",
    "    _, port_volatility = portfolio_variance(weights, cov_matrix)\n",
    "    \n",
    "    # Calculate the VaR using the normal distribution\n",
    "    VaR = -port_volatility * st.norm.ppf(alpha)\n",
    "    \n",
    "    # Calculate the ES using the normal distribution\n",
    "    ES = -port_volatility * (st.norm.pdf(st.norm.ppf(1 - alpha)) / (1 - alpha))\n",
    "    \n",
    "    return VaR, ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828763a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_violations(returns, VaR, ES):\n",
    "    # Ensure VaR and ES are scalar values (e.g., take the first element if they are lists)\n",
    "    if isinstance(VaR, list):\n",
    "        VaR = VaR[0]  # Use the first value in the list\n",
    "    \n",
    "    # Count the number of violations for VaR\n",
    "    VaR_violations = np.sum(returns < VaR)\n",
    "    \n",
    "    # Calculate the relative violations\n",
    "    VaR_violations = VaR_violations / len(returns)\n",
    "\n",
    "    # count the average shortfall for the VaR violations\n",
    "    ES_violations = returns[returns < VaR]\n",
    "    ES_violation_av = np.mean(ES_violations) if len(ES_violations) > 0 else 0\n",
    "\n",
    "    return VaR_violations, ES_violation_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to run the analysis\n",
    "def main_analysis(time_window_size):\n",
    "    # Initialize lists to store results\n",
    "    portfolio_VaR_list = []\n",
    "    portfolio_ES_list = []\n",
    "    dates = []\n",
    "    corr_matrix = correlation_matrix(main_df[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna())\n",
    "    # estimate the parameters per investment on the whole dataset\n",
    "    \n",
    "    # param = {}\n",
    "    # for column in ['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']:\n",
    "    #     param[column] = parameter_estimation_GARCH(main_df[column].dropna())\n",
    "    # estimate the parameters for the portfolio on the whole dataset\n",
    "    # returns = main_df['Portfolio_Daily_Returns'].dropna()\n",
    "    # param = parameter_estimation_GARCH(returns)\n",
    "\n",
    "    # Iterate through the dataset with a rolling time window\n",
    "    for i in range(time_window_size, len(main_df)):\n",
    "        # Extract the rolling time window\n",
    "        window = main_df.iloc[i - time_window_size:i]\n",
    "        current_date = main_df.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate the returns for each asset in the window\n",
    "        returns = window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "\n",
    "        # Define the weights for the portfolio\n",
    "        weights = np.array([0.4, 0.3, 0.15, 0.15])\n",
    "\n",
    "        # Calculate the covariance matrix\n",
    "        cov_matrix = covariance_matrix(returns, corr_matrix)\n",
    "\n",
    "        # Calculate the portfolio VaR and ES\n",
    "        VaR, ES = portfolio_VaR_ES(weights, cov_matrix)\n",
    "\n",
    "        # Append results\n",
    "        portfolio_VaR_list.append(VaR)\n",
    "        portfolio_ES_list.append(ES)\n",
    "        dates.append(current_date)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    # Calculate daily portfolio returns\n",
    "    daily_portfolio_returns = main_df['Portfolio_Daily_Returns'][time_window_size:].reset_index(drop=True)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Portfolio VaR': portfolio_VaR_list,\n",
    "        'Portfolio ES': portfolio_ES_list,\n",
    "        'Portfolio Daily Returns': daily_portfolio_returns\n",
    "    })\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results_df['Date'], results_df['Portfolio VaR'], label='Portfolio VaR', color='red')\n",
    "    plt.plot(results_df['Date'], results_df['Portfolio ES'], label='Portfolio ES', color='blue')\n",
    "    plt.plot(main_df['Date'], main_df['Portfolio_Daily_Returns'], label='Portfolio Daily Returns', color='green', alpha=0.5)\n",
    "    plt.title('Portfolio VaR, ES, and Daily Returns Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    portfolio_returns = main_df['Portfolio_Daily_Returns'][time_window_size:].reset_index(drop=True)\n",
    "    # count the amount of violations via the method count_violations\n",
    "    VaR_violations, ES_violations = count_violations(portfolio_returns, results_df['Portfolio VaR'], results_df['Portfolio ES'])\n",
    "    print(f\"VaR Violations: {VaR_violations:.2%}\")\n",
    "    print(f\"Mean return at violation: {ES_violations}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "time_window_size = 500\n",
    "\n",
    "main_analysis(time_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only the column names and the first 3 rows of the main dataframe\n",
    "print(main_df.columns)\n",
    "print(main_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006647f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking assumption on taking mean = 0 \n",
    "# Calculate portfolio daily returns\n",
    "main_df['Portfolio_Daily_Returns'] = (\n",
    "    weights['S&P500'] * main_df['C_S&P500_Returns'] +\n",
    "    weights['DAX40'] * main_df['C_Dax40_Returns'] +\n",
    "    weights['NIKKEI'] * main_df['C_Nikkei_Returns'] +\n",
    "    weights['EU-BOND'] * main_df['Interest_Bond_daily_rate']\n",
    ")\n",
    "\n",
    "# make a graph of the portfolio daily returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(main_df['Date'], main_df['Portfolio_Daily_Returns'], label='Portfolio Daily Returns', color='blue')\n",
    "plt.title('Portfolio Daily Returns Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.show()\n",
    "\n",
    "# Calculate the mean of the returns for each stock over a subset of the data defined on the time range\n",
    "# Define the sample size\n",
    "sample_size = 500\n",
    "\n",
    "# Initialize variables to store the mean returns for each stock for each time window\n",
    "mean_sp500_returns_list = []\n",
    "mean_dax40_returns_list = []\n",
    "mean_nikkei_returns_list = []\n",
    "mean_eu_bond_returns_list = []\n",
    "\n",
    "# Loop through the data with a fixed sample size\n",
    "for i in range(sample_size, len(main_df)):\n",
    "    time_range_start = main_df['Date'].iloc[i - sample_size]\n",
    "    time_range_end = main_df['Date'].iloc[i]\n",
    "    time_range_df = main_df[(main_df['Date'] >= time_range_start) & (main_df['Date'] <= time_range_end)]\n",
    "    \n",
    "    # Calculate mean returns for each stock\n",
    "    mean_sp500_returns = time_range_df['C_S&P500_Returns'].mean()\n",
    "    mean_dax40_returns = time_range_df['C_Dax40_Returns'].mean()\n",
    "    mean_nikkei_returns = time_range_df['C_Nikkei_Returns'].mean()\n",
    "    mean_eu_bond_returns = time_range_df['Interest_Bond_daily_rate'].mean()\n",
    "    \n",
    "    # Append the results to the respective lists\n",
    "    mean_sp500_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean S&P500 Returns': mean_sp500_returns\n",
    "    })\n",
    "    mean_dax40_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean DAX40 Returns': mean_dax40_returns\n",
    "    })\n",
    "    mean_nikkei_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean Nikkei Returns': mean_nikkei_returns\n",
    "    })\n",
    "    mean_eu_bond_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean EU Bond Returns': mean_eu_bond_returns\n",
    "    })\n",
    "\n",
    "# Convert the results to DataFrames for easier analysis\n",
    "mean_sp500_returns_df = pd.DataFrame(mean_sp500_returns_list)\n",
    "mean_dax40_returns_df = pd.DataFrame(mean_dax40_returns_list)\n",
    "mean_nikkei_returns_df = pd.DataFrame(mean_nikkei_returns_list)\n",
    "mean_eu_bond_returns_df = pd.DataFrame(mean_eu_bond_returns_list)\n",
    "\n",
    "# Display the results\n",
    "display(mean_sp500_returns_df.head())\n",
    "display(mean_dax40_returns_df.head())\n",
    "display(mean_nikkei_returns_df.head())\n",
    "display(mean_eu_bond_returns_df.head())\n",
    "\n",
    "# Plot the mean returns for each stock over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_sp500_returns_df['End Date'], mean_sp500_returns_df['Mean S&P500 Returns'], label='Mean S&P500 Returns', color='blue')\n",
    "plt.plot(mean_dax40_returns_df['End Date'], mean_dax40_returns_df['Mean DAX40 Returns'], label='Mean DAX40 Returns', color='orange')\n",
    "plt.plot(mean_nikkei_returns_df['End Date'], mean_nikkei_returns_df['Mean Nikkei Returns'], label='Mean Nikkei Returns', color='green')\n",
    "plt.plot(mean_eu_bond_returns_df['End Date'], mean_eu_bond_returns_df['Mean EU Bond Returns'], label='Mean EU Bond Returns', color='red')\n",
    "plt.title('Mean Returns Over Time for Each Stock')\n",
    "plt.xlabel('End Date')\n",
    "plt.ylabel('Mean Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb867408",
   "metadata": {},
   "source": [
    "# Backtesting VaR and ES\n",
    "\n",
    "In this section, we perform backtesting on the calculated Value at Risk (VaR) and Expected Shortfall (ES) measures. Backtesting helps assess the accuracy and reliability of the risk models.\n",
    "\n",
    "We will:\n",
    "1.  **Calculate Violations:** Identify the days where the actual portfolio loss exceeded the predicted VaR.\n",
    "2.  **Compare Actual vs. Expected Violations (VaR):** Group violations by year and compare the observed number of violations against the number expected based on the confidence level (alpha).\n",
    "3.  **Compare Actual Shortfall vs. Predicted ES (ES):** For the days a violation occurred, compare the average actual loss (shortfall) against the predicted ES, grouped by year.\n",
    "4.  **Visualize Violations:** Plot the occurrences of violations over time to visually inspect for clustering or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_violations(actual_losses, var_predictions):\n",
    "    \"\"\"Checks for VaR violations.\"\"\"\n",
    "    return actual_losses > var_predictions\n",
    "\n",
    "def backtest_var(violations, alpha, dates):\n",
    "    \"\"\"Compares actual vs. expected VaR violations yearly.\"\"\"\n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date': # Ensure index is Date for grouping\n",
    "         violations = violations.set_index(dates)\n",
    "            \n",
    "    violations_df = pd.DataFrame({'Violations': violations, 'Year': violations.index.year})\n",
    "    yearly_violations = violations_df.groupby('Year')['Violations'].sum()\n",
    "    yearly_counts = violations_df.groupby('Year')['Violations'].count()\n",
    "    \n",
    "    expected_violations = yearly_counts * (1 - alpha)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Actual Violations': yearly_violations,\n",
    "        'Expected Violations': expected_violations,\n",
    "        'Total Observations': yearly_counts\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def backtest_es(actual_losses, violations, es_predictions, dates):\n",
    "    \"\"\"Compares actual average shortfall vs. predicted ES yearly.\"\"\"\n",
    "    # Ensure inputs are pandas Series with Date index\n",
    "    if not isinstance(actual_losses, pd.Series):\n",
    "        actual_losses = pd.Series(actual_losses, index=dates)\n",
    "    elif actual_losses.index.name != 'Date':\n",
    "        actual_losses = actual_losses.set_index(dates)\n",
    "        \n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date':\n",
    "        violations = violations.set_index(dates)\n",
    "        \n",
    "    if not isinstance(es_predictions, pd.Series):\n",
    "        es_predictions = pd.Series(es_predictions, index=dates)\n",
    "    elif es_predictions.index.name != 'Date':\n",
    "        es_predictions = es_predictions.set_index(dates)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_Loss': actual_losses,\n",
    "        'Violation': violations,\n",
    "        'Predicted_ES': es_predictions,\n",
    "        'Year': actual_losses.index.year\n",
    "    })\n",
    "    \n",
    "    # Filter for violations\n",
    "    violation_data = results_df[results_df['Violation']]\n",
    "    \n",
    "    # Calculate yearly averages\n",
    "    yearly_avg_actual_shortfall = violation_data.groupby('Year')['Actual_Loss'].mean()\n",
    "    yearly_avg_predicted_es = violation_data.groupby('Year')['Predicted_ES'].mean()\n",
    "    yearly_violation_count = violation_data.groupby('Year').size()\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Avg Actual Shortfall': yearly_avg_actual_shortfall,\n",
    "        'Avg Predicted ES': yearly_avg_predicted_es,\n",
    "        'Violation Count': yearly_violation_count\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def plot_violations(violations, dates, title):\n",
    "    \"\"\"Plots VaR violations over time.\"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(dates, violations, 'ro', markersize=4, alpha=0.7, label='Violation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Violation (1=Yes, 0=No)')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "def run_backtesting(main_df, var_results_df, es_results_df):\n",
    "    \"\"\"Runs the backtesting process for VaR and ES models.\"\"\"\n",
    "    # Align main_df with var/es results (which start after the initial window)\n",
    "    backtest_dates = var_results_df['Date']\n",
    "    backtest_data = main_df[main_df['Date'].isin(backtest_dates)].set_index('Date')\n",
    "    actual_losses = backtest_data['Portfolio_loss']\n",
    "\n",
    "    # Confidence levels used\n",
    "    alphas = [0.95, 0.99]\n",
    "    alpha_indices = {0.95: 0, 0.99: 1} # Index mapping for results arrays\n",
    "\n",
    "    # Iterate through models (columns in var_results_df/es_results_df)\n",
    "    var_model_cols = [col for col in var_results_df.columns if col != 'Date']\n",
    "    es_model_cols = [col for col in es_results_df.columns if col != 'Date']\n",
    "\n",
    "    for i, model_name in enumerate(var_model_cols):\n",
    "        print(f\"\\n--- Backtesting for Model: {model_name} ---\")\n",
    "        \n",
    "        # Extract predictions for this model\n",
    "        # Need to handle the fact that predictions are stored as arrays [pred_95, pred_99]\n",
    "        var_preds_list = var_results_df[model_name].tolist()\n",
    "        # Ensure alignment between var and es model columns\n",
    "        if i < len(es_model_cols):\n",
    "            es_preds_list = es_results_df[es_model_cols[i]].tolist()\n",
    "        else:\n",
    "            print(f\"  Warning: No matching ES column found for {model_name}. Skipping ES backtest.\")\n",
    "            es_preds_list = None\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            alpha_idx = alpha_indices[alpha]\n",
    "            print(f\"\\nConfidence Level: {alpha*100}%\")\n",
    "            \n",
    "            # Extract predictions for the specific alpha\n",
    "            # Handle potential errors if data isn't as expected (e.g., not a list/array)\n",
    "            try:\n",
    "                var_predictions = pd.Series([p[alpha_idx] for p in var_preds_list], index=backtest_dates)\n",
    "                if es_preds_list:\n",
    "                    es_predictions = pd.Series([p[alpha_idx] for p in es_preds_list], index=backtest_dates)\n",
    "                else:\n",
    "                    es_predictions = None\n",
    "            except (TypeError, IndexError) as e:\n",
    "                print(f\"  Error extracting predictions for alpha={alpha}: {e}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # 1. Calculate Violations\n",
    "            violations = calculate_violations(actual_losses, var_predictions)\n",
    "            \n",
    "            # 2. Backtest VaR\n",
    "            var_summary = backtest_var(violations, alpha, backtest_dates)\n",
    "            print(\"\\nVaR Backtest Summary (Yearly):\")\n",
    "            display(var_summary)\n",
    "            \n",
    "            # 3. Backtest ES\n",
    "            if es_predictions is not None:\n",
    "                es_summary = backtest_es(actual_losses, violations, es_predictions, backtest_dates)\n",
    "                print(\"\\nES Backtest Summary (Yearly):\")\n",
    "                display(es_summary)\n",
    "            \n",
    "            # 4. Plot Violations\n",
    "            plot_violations(violations, backtest_dates, f'VaR Violations for {model_name} (alpha={alpha})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the backtesting function with the required dataframes\n",
    "run_backtesting(main_df, var_results_df, es_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f38a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cf1e78",
   "metadata": {},
   "source": [
    "### Backtesting Interpretation\n",
    "\n",
    "Review the tables and plots above:\n",
    "\n",
    "*   **VaR Backtest:** Compare 'Actual Violations' to 'Expected Violations' each year. Significant deviations might indicate issues with the VaR model's calibration. If actual violations consistently exceed expected, the model underestimates risk. If they are consistently lower, it might be too conservative.\n",
    "*   **ES Backtest:** Compare 'Avg Actual Shortfall' to 'Avg Predicted ES'. If the actual average shortfall during violations is consistently higher than the predicted ES, the model underestimates the severity of tail losses.\n",
    "*   **Violation Plots:** Look for patterns. Ideally, violations should be randomly distributed. Clustering of violations suggests the model fails to adapt quickly to changing market volatility (violation dependence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe882a8c",
   "metadata": {},
   "source": [
    "# Stress Testing\n",
    "\n",
    "For stress testing, different extreme changes are tested to measure their effect on the VaR and ES in the portfolio\n",
    "\n",
    " Equity index values or stock prices changing by +/- 20% and +/- 40% of the current values\n",
    "\n",
    " Currencies moving by +/- 10% for major currencies and +/- 20% for other currencies\n",
    "\n",
    " Commodity prices changing by +/- 20% and +/- 40% of the current values\n",
    "\n",
    " Interest rates shifting by +/- 2% and +/- 3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea196d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 1: Equity index values or stock prices changing by +/- 20% and +/- 40% of the current values\n",
    "# These changes will have to be at random dates in the future and will occur at a maximum of 4 consecutive days and will be repeated 5 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 2: Currencies moving by +/- 10% for major currencies and +/- 20% for other currencies\n",
    "# These changes will have to be at random dates in the future and will occur at a maximum of 4 consecutive days and will be repeated 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 3: Commodity prices changing by +/- 20% and +/- 40% of the current values\n",
    "# These changes will have to be at random dates in the future and will occur at a maximum of 4 consecutive days and will be repeated 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81783367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 4: Interest rates shifting by +/- 2% and +/- 3%\n",
    "# These changes will have to be at random dates in the future and will occur at a maximum of 4 consecutive days and will be repeated 5 times"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
