{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc2ba2c",
   "metadata": {},
   "source": [
    "### Initial package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f45638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c440bd4",
   "metadata": {},
   "source": [
    "## Initial files being read in and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9ce118",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "# Read the data\n",
    "main_df = pd.read_csv('Data/Cleaned_Indices_Assignment1.csv', sep=';')\n",
    "\n",
    "# Read the interest rate data\n",
    "#interest_rate_df = pd.read_csv('Data/ECB_Rates_2012_to_2022.csv', sep=';')\n",
    "interest_rate_bond_df = pd.read_csv('Data/ECB_Data_10yr_Treasury_bond.csv', sep=',')\n",
    "\n",
    "# Convert date columns to datetime format for proper merging\n",
    "main_df['Date'] = pd.to_datetime(main_df['Date'], format='%d-%m-%Y')\n",
    "#interest_rate_df['Date'] = pd.to_datetime(interest_rate_df['Date'], format='%d-%m-%Y')\n",
    "#gov_bond_investment_df['Date'] = pd.to_datetime(gov_bond_investment_df['Date'], format='%Y-%m-%d')\n",
    "interest_rate_bond_df['Date'] = pd.to_datetime(interest_rate_bond_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Merge the dataframes on the Date column\n",
    "main_df = pd.merge(main_df, interest_rate_bond_df, on='Date', how='left')\n",
    "#main_df = pd.merge(main_df, gov_bond_investment_df, on='Date', how='left')\n",
    "\n",
    "# Remove rows where the bond does not have a yield curve spot rate (Market closed?)\n",
    "main_df = main_df.dropna(axis=0, subset=['Yield curve spot rate, 10-year maturity - Government bond'])\n",
    "\n",
    "# Filter the dataframe to start from 2012-01-04\n",
    "main_df = main_df[main_df['Date'] >= '2012-01-04']\n",
    "main_df = main_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c10b0c",
   "metadata": {},
   "source": [
    "### government bond column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e954a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "# Add a column for the interest bond value per day\n",
    "days_per_annum = 365\n",
    "interest_bond = 1500000\n",
    "\n",
    "# Initialize the arrays with appropriate lengths matching the DataFrame\n",
    "interest_bond_vector = np.zeros(len(main_df))\n",
    "interest_bond_profit_vector = np.zeros(len(main_df))\n",
    "interest_bond_loss_vector = np.zeros(len(main_df))\n",
    "daily_rates = np.zeros(len(main_df))\n",
    "\n",
    "# Set initial value\n",
    "interest_bond_vector[0] = interest_bond\n",
    "\n",
    "\n",
    "# Calculate bond values day by day based on the daily yield rate\n",
    "for i in range(len(main_df)):\n",
    "    # Adding 1.5% to account for the credit risk spread\n",
    "    daily_rate = (((main_df['Yield curve spot rate, 10-year maturity - Government bond'].iloc[i] + 1.5) / (days_per_annum)) * (7/5)) / 100\n",
    "    daily_rates[i] = daily_rate\n",
    "    \n",
    "    if i > 0:\n",
    "        previous_value = interest_bond_vector[i-1]\n",
    "        current_value = previous_value * (1 + daily_rate)\n",
    "        interest_bond_vector[i] = current_value\n",
    "        \n",
    "        # Calculate change, profit/loss and return\n",
    "        change = current_value - previous_value\n",
    "        interest_bond_profit_vector[i] = change\n",
    "        interest_bond_loss_vector[i] = -change\n",
    "\n",
    "# Add vectors to the dataframe\n",
    "main_df['Interest_Bond'] = interest_bond_vector\n",
    "main_df['Interest_Bond_Profit'] = interest_bond_profit_vector\n",
    "main_df['Interest_Bond_Loss'] = interest_bond_loss_vector\n",
    "main_df['Interest_Bond_daily_rate'] = daily_rates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de917d",
   "metadata": {},
   "source": [
    "## Portfolio details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3557320",
   "metadata": {},
   "source": [
    "### details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d95594",
   "metadata": {},
   "source": [
    "#### Instruments:\n",
    "- **S&P500**\n",
    "- **DAX40**\n",
    "- **NIKKEI**\n",
    "- **EU Government Bond (10-year maturity, AAA-rated)**\n",
    "\n",
    "#### Invested amount:\n",
    "- **10,000,000 EURO**\n",
    "\n",
    "#### Period:\n",
    "- **01/01/2012 - 31/12/2022**\n",
    "\n",
    "#### Weights:\n",
    "- **S&P500**: 0.4  \n",
    "- **DAX40**: 0.3  \n",
    "- **NIKKEI**: 0.15  \n",
    "- **EU Government Bond**: 0.15  \n",
    "\n",
    "#### Measures:\n",
    "- **Value at Risk (VaR)**: 1, 5, 10 days  \n",
    "- **Expected Shortfall (ES)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d60941",
   "metadata": {},
   "source": [
    "### weights and currency correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "688bd314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5177993.527508091, 3000000.0, 148957298.90764648, 1500000.0]\n"
     ]
    }
   ],
   "source": [
    "# initial investment \n",
    "weights = {\n",
    "    'S&P500': 0.4,\n",
    "    'DAX40': 0.3,\n",
    "    'NIKKEI': 0.15,\n",
    "    'EU-BOND': 0.15,\n",
    "}\n",
    "\n",
    "starting_investment = 10000000  # 10 million euros\n",
    "starting_date = '2012-01-04'\n",
    "\n",
    "# Filter the main_df for the starting date\n",
    "starting_row = main_df[main_df['Date'] == starting_date]\n",
    "\n",
    "# Extract the exchange rates for the starting date\n",
    "usd_to_eur = float(starting_row['USD/EUR'].iloc[0])\n",
    "jpy_to_eur = float(starting_row['JPY/EUR'].iloc[0])\n",
    "\n",
    "# Calculate the invested amounts\n",
    "invested_amount_SP500 = starting_investment * weights['S&P500'] / usd_to_eur\n",
    "invested_amount_DAX40 = starting_investment * weights['DAX40']\n",
    "invested_amount_NIKKEI = starting_investment * weights['NIKKEI'] / jpy_to_eur\n",
    "invested_amount_EU_BOND = starting_investment * weights['EU-BOND']\n",
    "\n",
    "invested_amounts = [\n",
    "    invested_amount_SP500, #in USD\n",
    "    invested_amount_DAX40, #in EUR\n",
    "    invested_amount_NIKKEI, #in JPY\n",
    "    invested_amount_EU_BOND #in EUR\n",
    "]\n",
    "\n",
    "print(invested_amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a6bef",
   "metadata": {},
   "source": [
    "### Returns Portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ef3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to track investments for each asset\n",
    "# Initialize the first day with the initial invested amounts\n",
    "main_df.loc[0, 'SP500_Investment'] = invested_amount_SP500\n",
    "main_df.loc[0, 'DAX40_Investment'] = invested_amount_DAX40\n",
    "main_df.loc[0, 'NIKKEI_Investment'] = invested_amount_NIKKEI\n",
    "main_df.loc[0, 'EU_BOND_Investment'] = invested_amount_EU_BOND\n",
    "\n",
    "# Calculate daily investment values for subsequent days\n",
    "# This uses cumulative returns to track the value growth\n",
    "for i in range(1, len(main_df)):\n",
    "    # S&P 500 in USD\n",
    "    main_df.loc[i, 'SP500_Investment'] = main_df.loc[i-1, 'SP500_Investment'] * (1 + main_df.loc[i, 'C_S&P500_Returns'])\n",
    "    \n",
    "    # DAX 40 in EUR\n",
    "    main_df.loc[i, 'DAX40_Investment'] = main_df.loc[i-1, 'DAX40_Investment'] * (1 + main_df.loc[i, 'C_Dax40_Returns'])\n",
    "    \n",
    "    # NIKKEI in JPY\n",
    "    main_df.loc[i, 'NIKKEI_Investment'] = main_df.loc[i-1, 'NIKKEI_Investment'] * (1 + main_df.loc[i, 'C_Nikkei_Returns'])\n",
    "    \n",
    "# EU Government Bond value is already calculated in the Interest_Bond column\n",
    "main_df['EU_BOND_Investment'] = main_df['Interest_Bond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff1f7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>S&amp;P500_Closing</th>\n",
       "      <th>Dax40_Closing</th>\n",
       "      <th>Nikkei_Closing</th>\n",
       "      <th>U_S&amp;P500_Returns</th>\n",
       "      <th>U_Dax40_Returns</th>\n",
       "      <th>U_Nikkei_Returns</th>\n",
       "      <th>U_S&amp;P500_Loss</th>\n",
       "      <th>U_Dax40_Loss</th>\n",
       "      <th>U_Nikkei_Loss</th>\n",
       "      <th>...</th>\n",
       "      <th>TIME PERIOD</th>\n",
       "      <th>Yield curve spot rate, 10-year maturity - Government bond</th>\n",
       "      <th>Interest_Bond</th>\n",
       "      <th>Interest_Bond_Profit</th>\n",
       "      <th>Interest_Bond_Loss</th>\n",
       "      <th>Interest_Bond_daily_rate</th>\n",
       "      <th>SP500_Investment</th>\n",
       "      <th>DAX40_Investment</th>\n",
       "      <th>NIKKEI_Investment</th>\n",
       "      <th>EU_BOND_Investment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>1277.300000</td>\n",
       "      <td>6111.550000</td>\n",
       "      <td>8560.110000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>04 Jan 2012</td>\n",
       "      <td>2.776691</td>\n",
       "      <td>1500000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>5177993.527508</td>\n",
       "      <td>3000000.000000</td>\n",
       "      <td>148957298.907646</td>\n",
       "      <td>1500000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1281.060000</td>\n",
       "      <td>6095.990000</td>\n",
       "      <td>8488.710000</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>-0.008376</td>\n",
       "      <td>-3.760000</td>\n",
       "      <td>15.560000</td>\n",
       "      <td>71.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>05 Jan 2012</td>\n",
       "      <td>2.784807</td>\n",
       "      <td>1500246.523142</td>\n",
       "      <td>246.523142</td>\n",
       "      <td>-246.523142</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>5189897.180583</td>\n",
       "      <td>2992352.262000</td>\n",
       "      <td>148944667.328699</td>\n",
       "      <td>1500246.523142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1277.810000</td>\n",
       "      <td>6057.920000</td>\n",
       "      <td>8390.350000</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.011655</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>38.070000</td>\n",
       "      <td>98.360000</td>\n",
       "      <td>...</td>\n",
       "      <td>06 Jan 2012</td>\n",
       "      <td>2.788371</td>\n",
       "      <td>1500493.291886</td>\n",
       "      <td>246.768744</td>\n",
       "      <td>-246.768744</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>5179533.786296</td>\n",
       "      <td>2973606.159562</td>\n",
       "      <td>148926943.509066</td>\n",
       "      <td>1500493.291886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>1280.700000</td>\n",
       "      <td>6017.230000</td>\n",
       "      <td>8390.350000</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>-0.006739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.890000</td>\n",
       "      <td>40.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>09 Jan 2012</td>\n",
       "      <td>2.757489</td>\n",
       "      <td>1500738.323863</td>\n",
       "      <td>245.031977</td>\n",
       "      <td>-245.031977</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>5188699.359796</td>\n",
       "      <td>2953565.582481</td>\n",
       "      <td>148926943.509066</td>\n",
       "      <td>1500738.323863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>1292.080000</td>\n",
       "      <td>6162.980000</td>\n",
       "      <td>8422.260000</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>-11.380000</td>\n",
       "      <td>-145.750000</td>\n",
       "      <td>-31.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>10 Jan 2012</td>\n",
       "      <td>2.746027</td>\n",
       "      <td>1500982.736072</td>\n",
       "      <td>244.412209</td>\n",
       "      <td>-244.412209</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>5224622.209432</td>\n",
       "      <td>3024254.466714</td>\n",
       "      <td>148932706.981779</td>\n",
       "      <td>1500982.736072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>3844.820000</td>\n",
       "      <td>13940.930000</td>\n",
       "      <td>26235.250000</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>-0.010338</td>\n",
       "      <td>-22.430000</td>\n",
       "      <td>-26.860000</td>\n",
       "      <td>272.620000</td>\n",
       "      <td>...</td>\n",
       "      <td>23 Dec 2022</td>\n",
       "      <td>2.464312</td>\n",
       "      <td>1898481.681703</td>\n",
       "      <td>288.631268</td>\n",
       "      <td>-288.631268</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>11203387.897754</td>\n",
       "      <td>5587239.551284</td>\n",
       "      <td>150353000.311487</td>\n",
       "      <td>1898481.681703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>3829.250000</td>\n",
       "      <td>13995.100000</td>\n",
       "      <td>26447.870000</td>\n",
       "      <td>-0.004058</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>15.570000</td>\n",
       "      <td>-54.170000</td>\n",
       "      <td>-42.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27 Dec 2022</td>\n",
       "      <td>2.501054</td>\n",
       "      <td>1898773.032356</td>\n",
       "      <td>291.350653</td>\n",
       "      <td>-291.350653</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>11160667.822429</td>\n",
       "      <td>5608907.709937</td>\n",
       "      <td>150354684.265090</td>\n",
       "      <td>1898773.032356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>3783.220000</td>\n",
       "      <td>13925.600000</td>\n",
       "      <td>26340.500000</td>\n",
       "      <td>-0.012093</td>\n",
       "      <td>-0.004978</td>\n",
       "      <td>-0.004068</td>\n",
       "      <td>46.030000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>107.370000</td>\n",
       "      <td>...</td>\n",
       "      <td>28 Dec 2022</td>\n",
       "      <td>2.522043</td>\n",
       "      <td>1899065.956342</td>\n",
       "      <td>292.923986</td>\n",
       "      <td>-292.923986</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>11033498.028193</td>\n",
       "      <td>5580984.351838</td>\n",
       "      <td>150350399.156589</td>\n",
       "      <td>1899065.956342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>3849.280000</td>\n",
       "      <td>14071.720000</td>\n",
       "      <td>26093.670000</td>\n",
       "      <td>0.017311</td>\n",
       "      <td>0.010438</td>\n",
       "      <td>-0.009415</td>\n",
       "      <td>-66.060000</td>\n",
       "      <td>-146.120000</td>\n",
       "      <td>246.830000</td>\n",
       "      <td>...</td>\n",
       "      <td>29 Dec 2022</td>\n",
       "      <td>2.526705</td>\n",
       "      <td>1899359.265102</td>\n",
       "      <td>293.308760</td>\n",
       "      <td>-293.308760</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>11212633.819784</td>\n",
       "      <td>5639239.989196</td>\n",
       "      <td>150340415.890085</td>\n",
       "      <td>1899359.265102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>3839.500000</td>\n",
       "      <td>13923.590000</td>\n",
       "      <td>26094.500000</td>\n",
       "      <td>-0.002544</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>9.780000</td>\n",
       "      <td>148.130000</td>\n",
       "      <td>-0.830000</td>\n",
       "      <td>...</td>\n",
       "      <td>30 Dec 2022</td>\n",
       "      <td>2.555314</td>\n",
       "      <td>1899654.703390</td>\n",
       "      <td>295.438288</td>\n",
       "      <td>-295.438288</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>11185989.002573</td>\n",
       "      <td>5579562.247036</td>\n",
       "      <td>150340450.017359</td>\n",
       "      <td>1899654.703390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2805 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  S&P500_Closing  Dax40_Closing  Nikkei_Closing  \\\n",
       "0    2012-01-04     1277.300000    6111.550000     8560.110000   \n",
       "1    2012-01-05     1281.060000    6095.990000     8488.710000   \n",
       "2    2012-01-06     1277.810000    6057.920000     8390.350000   \n",
       "3    2012-01-09     1280.700000    6017.230000     8390.350000   \n",
       "4    2012-01-10     1292.080000    6162.980000     8422.260000   \n",
       "...         ...             ...            ...             ...   \n",
       "2800 2022-12-23     3844.820000   13940.930000    26235.250000   \n",
       "2801 2022-12-27     3829.250000   13995.100000    26447.870000   \n",
       "2802 2022-12-28     3783.220000   13925.600000    26340.500000   \n",
       "2803 2022-12-29     3849.280000   14071.720000    26093.670000   \n",
       "2804 2022-12-30     3839.500000   13923.590000    26094.500000   \n",
       "\n",
       "      U_S&P500_Returns  U_Dax40_Returns  U_Nikkei_Returns  U_S&P500_Loss  \\\n",
       "0                  NaN              NaN               NaN            NaN   \n",
       "1             0.002939        -0.002549         -0.008376      -3.760000   \n",
       "2            -0.002540        -0.006265         -0.011655       3.250000   \n",
       "3             0.002259        -0.006739          0.000000      -2.890000   \n",
       "4             0.008847         0.023933          0.003796     -11.380000   \n",
       "...                ...              ...               ...            ...   \n",
       "2800          0.005851         0.001929         -0.010338     -22.430000   \n",
       "2801         -0.004058         0.003878          0.001589      15.570000   \n",
       "2802         -0.012093        -0.004978         -0.004068      46.030000   \n",
       "2803          0.017311         0.010438         -0.009415     -66.060000   \n",
       "2804         -0.002544        -0.010583          0.000032       9.780000   \n",
       "\n",
       "      U_Dax40_Loss  U_Nikkei_Loss  ...  TIME PERIOD  \\\n",
       "0              NaN            NaN  ...  04 Jan 2012   \n",
       "1        15.560000      71.400000  ...  05 Jan 2012   \n",
       "2        38.070000      98.360000  ...  06 Jan 2012   \n",
       "3        40.690000       0.000000  ...  09 Jan 2012   \n",
       "4      -145.750000     -31.910000  ...  10 Jan 2012   \n",
       "...            ...            ...  ...          ...   \n",
       "2800    -26.860000     272.620000  ...  23 Dec 2022   \n",
       "2801    -54.170000     -42.000000  ...  27 Dec 2022   \n",
       "2802     69.500000     107.370000  ...  28 Dec 2022   \n",
       "2803   -146.120000     246.830000  ...  29 Dec 2022   \n",
       "2804    148.130000      -0.830000  ...  30 Dec 2022   \n",
       "\n",
       "      Yield curve spot rate, 10-year maturity - Government bond  \\\n",
       "0                                              2.776691           \n",
       "1                                              2.784807           \n",
       "2                                              2.788371           \n",
       "3                                              2.757489           \n",
       "4                                              2.746027           \n",
       "...                                                 ...           \n",
       "2800                                           2.464312           \n",
       "2801                                           2.501054           \n",
       "2802                                           2.522043           \n",
       "2803                                           2.526705           \n",
       "2804                                           2.555314           \n",
       "\n",
       "      Interest_Bond  Interest_Bond_Profit  Interest_Bond_Loss  \\\n",
       "0    1500000.000000              0.000000            0.000000   \n",
       "1    1500246.523142            246.523142         -246.523142   \n",
       "2    1500493.291886            246.768744         -246.768744   \n",
       "3    1500738.323863            245.031977         -245.031977   \n",
       "4    1500982.736072            244.412209         -244.412209   \n",
       "...             ...                   ...                 ...   \n",
       "2800 1898481.681703            288.631268         -288.631268   \n",
       "2801 1898773.032356            291.350653         -291.350653   \n",
       "2802 1899065.956342            292.923986         -292.923986   \n",
       "2803 1899359.265102            293.308760         -293.308760   \n",
       "2804 1899654.703390            295.438288         -295.438288   \n",
       "\n",
       "      Interest_Bond_daily_rate  SP500_Investment  DAX40_Investment  \\\n",
       "0                     0.000164    5177993.527508    3000000.000000   \n",
       "1                     0.000164    5189897.180583    2992352.262000   \n",
       "2                     0.000164    5179533.786296    2973606.159562   \n",
       "3                     0.000163    5188699.359796    2953565.582481   \n",
       "4                     0.000163    5224622.209432    3024254.466714   \n",
       "...                        ...               ...               ...   \n",
       "2800                  0.000152   11203387.897754    5587239.551284   \n",
       "2801                  0.000153   11160667.822429    5608907.709937   \n",
       "2802                  0.000154   11033498.028193    5580984.351838   \n",
       "2803                  0.000154   11212633.819784    5639239.989196   \n",
       "2804                  0.000156   11185989.002573    5579562.247036   \n",
       "\n",
       "     NIKKEI_Investment  EU_BOND_Investment  \n",
       "0     148957298.907646      1500000.000000  \n",
       "1     148944667.328699      1500246.523142  \n",
       "2     148926943.509066      1500493.291886  \n",
       "3     148926943.509066      1500738.323863  \n",
       "4     148932706.981779      1500982.736072  \n",
       "...                ...                 ...  \n",
       "2800  150353000.311487      1898481.681703  \n",
       "2801  150354684.265090      1898773.032356  \n",
       "2802  150350399.156589      1899065.956342  \n",
       "2803  150340415.890085      1899359.265102  \n",
       "2804  150340450.017359      1899654.703390  \n",
       "\n",
       "[2805 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fc2cb",
   "metadata": {},
   "source": [
    "## Methods input values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3a087",
   "metadata": {},
   "source": [
    "### time window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd3e01",
   "metadata": {},
   "source": [
    "### Expected returns (daily) --> Action make into method with time window as input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b4360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_daily_returns(time_window, current_date):\n",
    "    expected_returns = {\n",
    "        'Date': current_date,\n",
    "        'SP500_Mean_Return': time_window['C_S&P500_Returns'].mean(),\n",
    "        'DAX40_Mean_Return': time_window['C_Dax40_Returns'].mean(),\n",
    "        'NIKKEI_Mean_Return': time_window['C_Nikkei_Returns'].mean(),\n",
    "        'EU_Bond_Mean_Return': time_window['Interest_Bond_Profit'].mean(),\n",
    "        'Portfolio_Mean_Return': (\n",
    "            weights['S&P500'] * time_window['C_S&P500_Returns'].mean() +\n",
    "            weights['DAX40'] * time_window['C_Dax40_Returns'].mean() +\n",
    "            weights['NIKKEI'] * time_window['C_Nikkei_Returns'].mean() +\n",
    "            weights['EU-BOND'] * time_window['Interest_Bond_Profit'].mean()\n",
    "        )\n",
    "    }\n",
    "    return expected_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d45a427",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500_Investment</th>\n",
       "      <th>DAX40_Investment</th>\n",
       "      <th>NIKKEI_Investment</th>\n",
       "      <th>EU_BOND_Investment</th>\n",
       "      <th>USD/EUR_filled</th>\n",
       "      <th>JPY/EUR_filled</th>\n",
       "      <th>Portfolio_Value_EUR</th>\n",
       "      <th>Portfolio_Change_EUR</th>\n",
       "      <th>Portfolio_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>5177993.527508</td>\n",
       "      <td>3000000.000000</td>\n",
       "      <td>148957298.907646</td>\n",
       "      <td>1500000.000000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>10000000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>5189897.180583</td>\n",
       "      <td>2992352.262000</td>\n",
       "      <td>148944667.328699</td>\n",
       "      <td>1500246.523142</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>10058937.403442</td>\n",
       "      <td>58937.403442</td>\n",
       "      <td>-58937.403442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>5179533.786296</td>\n",
       "      <td>2973606.159562</td>\n",
       "      <td>148926943.509066</td>\n",
       "      <td>1500493.291886</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.010210</td>\n",
       "      <td>10066275.054084</td>\n",
       "      <td>7337.650642</td>\n",
       "      <td>-7337.650642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>5188699.359796</td>\n",
       "      <td>2953565.582481</td>\n",
       "      <td>148926943.509066</td>\n",
       "      <td>1500738.323863</td>\n",
       "      <td>0.783300</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>10036177.669230</td>\n",
       "      <td>-30097.384854</td>\n",
       "      <td>30097.384854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>5224622.209432</td>\n",
       "      <td>3024254.466714</td>\n",
       "      <td>148932706.981779</td>\n",
       "      <td>1500982.736072</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>10131650.828032</td>\n",
       "      <td>95473.158802</td>\n",
       "      <td>-95473.158802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  SP500_Investment  DAX40_Investment  NIKKEI_Investment  \\\n",
       "0 2012-01-04    5177993.527508    3000000.000000   148957298.907646   \n",
       "1 2012-01-05    5189897.180583    2992352.262000   148944667.328699   \n",
       "2 2012-01-06    5179533.786296    2973606.159562   148926943.509066   \n",
       "3 2012-01-09    5188699.359796    2953565.582481   148926943.509066   \n",
       "4 2012-01-10    5224622.209432    3024254.466714   148932706.981779   \n",
       "\n",
       "   EU_BOND_Investment  USD/EUR_filled  JPY/EUR_filled  Portfolio_Value_EUR  \\\n",
       "0      1500000.000000        0.772500        0.010070      10000000.000000   \n",
       "1      1500246.523142        0.782100        0.010120      10058937.403442   \n",
       "2      1500493.291886        0.786100        0.010210      10066275.054084   \n",
       "3      1500738.323863        0.783300        0.010190      10036177.669230   \n",
       "4      1500982.736072        0.782600        0.010190      10131650.828032   \n",
       "\n",
       "   Portfolio_Change_EUR  Portfolio_loss  \n",
       "0              0.000000        0.000000  \n",
       "1          58937.403442   -58937.403442  \n",
       "2           7337.650642    -7337.650642  \n",
       "3         -30097.384854    30097.384854  \n",
       "4          95473.158802   -95473.158802  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate total portfolio value in EUR for each day\n",
    "# Forward fill exchange rates first to avoid deprecation warning\n",
    "main_df['USD/EUR_filled'] = main_df['USD/EUR'].ffill()\n",
    "main_df['JPY/EUR_filled'] = main_df['JPY/EUR'].ffill()\n",
    "\n",
    "# Calculate portfolio value components\n",
    "main_df['Portfolio_Value_EUR'] = (\n",
    "    main_df['SP500_Investment'].fillna(0) * main_df['USD/EUR_filled'] +\n",
    "    main_df['DAX40_Investment'].fillna(0) +\n",
    "    main_df['NIKKEI_Investment'].fillna(0) * main_df['JPY/EUR_filled'] +\n",
    "    main_df['EU_BOND_Investment'].fillna(0)\n",
    ")\n",
    "\n",
    "# First day should be the initial investment amount\n",
    "main_df.loc[0, 'Portfolio_Value_EUR'] = starting_investment\n",
    "\n",
    "# Calculate the daily change in portfolio value (profit/loss)\n",
    "main_df['Portfolio_Change_EUR'] = main_df['Portfolio_Value_EUR'].diff()\n",
    "main_df.loc[0, 'Portfolio_Change_EUR'] = 0.0  # Set the first day's change to 0\n",
    "\n",
    "# Portfolio loss is the negative of the daily change\n",
    "main_df['Portfolio_loss'] = -main_df['Portfolio_Change_EUR']\n",
    "\n",
    "# Set the first day's loss to 0 (there's no previous day to compare with)\n",
    "main_df.loc[0, 'Portfolio_loss'] = 0.0\n",
    "\n",
    "# Display the relevant columns to verify\n",
    "display(main_df[['Date', 'SP500_Investment', 'DAX40_Investment', 'NIKKEI_Investment', \n",
    "                'EU_BOND_Investment', 'USD/EUR_filled', 'JPY/EUR_filled', 'Portfolio_Value_EUR', \n",
    "                'Portfolio_Change_EUR', 'Portfolio_loss']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26176760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Loss Statistics:\n",
      "Minimum Loss: -850226.1277\n",
      "Maximum Loss: 1342732.3566\n",
      "Mean Loss: -3208.1727\n",
      "Number of valid loss values: 2805 out of 2805\n"
     ]
    }
   ],
   "source": [
    "loss_values = main_df['Portfolio_loss'].values\n",
    "\n",
    "# Calculate and print the minimum, maximum, and mean of portfolio loss values\n",
    "min_loss = np.nanmin(loss_values)\n",
    "max_loss = np.nanmax(loss_values)\n",
    "mean_loss = np.nanmean(loss_values)\n",
    "\n",
    "print(f\"Portfolio Loss Statistics:\")\n",
    "print(f\"Minimum Loss: {min_loss:.4f}\")\n",
    "print(f\"Maximum Loss: {max_loss:.4f}\")\n",
    "print(f\"Mean Loss: {mean_loss:.4f}\")\n",
    "\n",
    "# Also print the number of valid loss values (non-NaN)\n",
    "valid_count = np.sum(~np.isnan(loss_values))\n",
    "print(f\"Number of valid loss values: {valid_count} out of {len(loss_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62edd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_loss_variables(time_window, current_date):\n",
    "    # Calculate the mean and standard deviation of portfolio loss from the time windows\n",
    "    loss_dict = {\n",
    "        \"Date\": current_date,\n",
    "        \"Portfolio_mean_loss\": np.nanmean(time_window['Portfolio_loss']),\n",
    "        \"Portfolio_std_loss\": np.nanstd(time_window['Portfolio_loss'])\n",
    "    }\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81c989",
   "metadata": {},
   "source": [
    "### Variances --> Action make into method with time window as input parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346e2cc",
   "metadata": {},
   "source": [
    "### Covariance matrix --> action make into method with variable time window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72986b3d",
   "metadata": {},
   "source": [
    "### Portfolio variance & standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fbc13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_return(window, current_date, weights):\n",
    "    \"\"\"\n",
    "    Calculate the variance returns for each index and the portfolio variance and volatility.\n",
    "    \"\"\"\n",
    "    variance_return = {\n",
    "        'Date': current_date,\n",
    "        'SP500_Var_Return': window['C_S&P500_Returns'].var(),\n",
    "        'DAX40_Var_Return': window['C_Dax40_Returns'].var(),\n",
    "        'NIKKEI_Var_Return': window['C_Nikkei_Returns'].var(),\n",
    "        'EU_Bond_Var_Return': window['Interest_Bond_daily_rate'].var()\n",
    "    }\n",
    "\n",
    "    weights = np.array([weights['S&P500'], weights['DAX40'], weights['NIKKEI'], weights['EU-BOND']])\n",
    "\n",
    "    # Calculate the covariance matrix for the returns in the window\n",
    "    covariance_matrix = window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].cov()\n",
    "\n",
    "    # Calculate the portfolio variance using the covariance matrix and weights\n",
    "    portfolio_variance = np.dot(weights.T, np.dot(covariance_matrix.values, weights))\n",
    "    portfolio_volatility = np.sqrt(portfolio_variance)\n",
    "\n",
    "    # Add portfolio variance and volatility to the variance_return dictionary\n",
    "    variance_return['Portfolio_Variance'] = portfolio_variance\n",
    "    variance_return['Portfolio_Volatility'] = portfolio_volatility\n",
    "\n",
    "    return variance_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2842c0",
   "metadata": {},
   "source": [
    "# Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab29edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Get the VaR of the normal model\n",
    "\n",
    "    Inputs:\n",
    "        alpha   double, level\n",
    "        r       double, expected return\n",
    "        s       double, volatility\n",
    "        df      (optional) double, degrees of freedom for student-t\n",
    "\n",
    "    Return value:\n",
    "        dVaR    double, VaR\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        dVaR0= st.norm.ppf(alpha)\n",
    "        dVaR = r + s*dVaR0\n",
    "    else:\n",
    "        dVaR0= st.t.ppf(alpha, df= df)\n",
    "\n",
    "        dS2t= df/(df-2)\n",
    "\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dVaR= r + c*dVaR0\n",
    "    return dVaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12755bb3",
   "metadata": {},
   "source": [
    "# Expected Shortfall (ES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb43cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Get the ES of the normal/student model\n",
    "\n",
    "    Inputs:\n",
    "        alpha   double, level\n",
    "        r       double, expected return\n",
    "        s       double, volatility\n",
    "        df      (optional, default= 0/normal) double, df\n",
    "\n",
    "    Return value:\n",
    "        dES     double, ES\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        dVaR0= st.norm.ppf(alpha)\n",
    "        dES0= st.norm.pdf(dVaR0) / (1-alpha)\n",
    "        dES= r + s*dES0\n",
    "    else:\n",
    "        dVaR0= st.t.ppf(alpha, df= df)\n",
    "        dES0= st.t.pdf(dVaR0, df= df)*((df + dVaR0**2)/(df-1)) / (1-alpha)\n",
    "\n",
    "        dS2t= df/(df-2)\n",
    "        c= s / np.sqrt(dS2t)\n",
    "        dES= r + c*dES0\n",
    "    return dES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0d4d6",
   "metadata": {},
   "source": [
    "## performing different methods\n",
    "\n",
    "write method for variance covariance where the sample period is an input parameter alongside other parameters that are needed for the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d7a40",
   "metadata": {},
   "source": [
    "## 1. var/cov multivar normal dist & T-distribution\n",
    "\n",
    "4 code blocks with functions to calculate components of Var/cov method, 1 code block with for loop to iterate through set window for daily VaR and ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8ad66",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e631c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize lists to store results\n",
    "    mean_losses = []\n",
    "    portfolio_std_losses = [] # Store standard dev of losses\n",
    "    VaR_results = []\n",
    "    ES_results = []  # List to store ES results\n",
    "    time_window = main_df[(main_df['Date'] >= '2012-01-05') & (main_df['Date'] <= '2021-12-31')]\n",
    "\n",
    "    vAlpha= [.95, .99]\n",
    "    vAlpha= np.array(vAlpha)\n",
    "\n",
    "    # Iterate through the DataFrame with a rolling window of 500 days\n",
    "    sample_size = 500\n",
    "    degrees_of_freedom = [0, 3, 4, 5, 6] # Normal (0) and T-distributions\n",
    "\n",
    "    for i in range(sample_size, len(time_window)):\n",
    "        # Extract the window of 500 days\n",
    "        window = time_window.iloc[i - sample_size:i]\n",
    "        current_date = time_window.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate mean loss and portfolio std dev of LOSS\n",
    "        loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "        mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "        portfolio_std_loss = loss_stats[\"Portfolio_std_loss\"] # Use std dev of loss\n",
    "        \n",
    "        # Append the results to the lists\n",
    "        mean_losses.append(mean_loss)\n",
    "        portfolio_std_losses.append(portfolio_std_loss)\n",
    "        \n",
    "        # Dictionaries to hold results for the current date\n",
    "        var_row = {'Date': current_date}\n",
    "        es_row = {'Date': current_date}\n",
    "        \n",
    "        # Calculate VaR and ES for each degree of freedom using mean and std dev of LOSS\n",
    "        for df in degrees_of_freedom:\n",
    "            # Use portfolio_std_loss (std dev of loss) as 's' parameter\n",
    "            current_var = VaR(vAlpha, mean_loss, portfolio_std_loss, df)\n",
    "            current_es = ES(vAlpha, mean_loss, portfolio_std_loss, df)\n",
    "            \n",
    "            # Determine the key name (e.g., 'VaR Normal', 'VaR T3')\n",
    "            var_key = f\"VaR {'Normal' if df == 0 else f'T{df}'}\"\n",
    "            es_key = f\"ES {'Normal' if df == 0 else f'T{df}'}\"\n",
    "            \n",
    "            # Add to the row dictionaries\n",
    "            var_row[var_key] = current_var\n",
    "            es_row[es_key] = current_es\n",
    "\n",
    "        # Historical simulation\n",
    "        # Extract the portfolio loss values from the window\n",
    "        historical_losses = window['Portfolio_loss'].dropna()  # Remove NaN values directly\n",
    "        \n",
    "        # Sort the losses in ascending order\n",
    "        sorted_losses = np.sort(historical_losses)\n",
    "\n",
    "        # Calculate VaR for alpha levels\n",
    "        var_95 = np.percentile(sorted_losses, 95) \n",
    "        var_99 = np.percentile(sorted_losses, 99)  \n",
    "\n",
    "        # Calculate ES\n",
    "        es_95 = sorted_losses[sorted_losses >= var_95].mean()  # Mean of losses below VaR 95\n",
    "        es_99 = sorted_losses[sorted_losses >= var_99].mean()  # Mean of losses below VaR 99\n",
    "\n",
    "        # Add to the row dictionaries\n",
    "        var_row['VaR Historical 95%'] = var_95\n",
    "        var_row['VaR Historical 99%'] = var_99\n",
    "        es_row['ES Historical 95%'] = es_95\n",
    "        es_row['ES Historical 99%'] = es_99\n",
    "        \n",
    "        # Append the dictionaries to the results lists\n",
    "        VaR_results.append(var_row)\n",
    "        ES_results.append(es_row)\n",
    "\n",
    "    # Convert the results to DataFrames for easier analysis\n",
    "    mean_losses_df = pd.DataFrame(mean_losses, columns=['Mean_Loss'])\n",
    "    portfolio_std_losses_df = pd.DataFrame(portfolio_std_losses, columns=['Portfolio_Std_Loss']) # New DataFrame for std dev of loss\n",
    "    var_results_df = pd.DataFrame(VaR_results)\n",
    "    es_results_df = pd.DataFrame(ES_results)\n",
    "\n",
    "    # Return all relevant DataFrames\n",
    "    return mean_losses_df, portfolio_std_losses_df, var_results_df, es_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losss_df, portfolio_std_losses_df, var_results_df, es_results_df = main()\n",
    "\n",
    "print(\"mean_loss\")\n",
    "display(mean_losss_df.head())\n",
    "print(\"portfolio_std_losses\")\n",
    "display(portfolio_std_losses_df.head())\n",
    "print(\"VaR results\")\n",
    "display(var_results_df.head())\n",
    "print(\"ES results\")\n",
    "display(es_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Historical VaR and ES over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot VaR Historical 95% and 99%\n",
    "plt.plot(var_results_df['Date'], var_results_df['VaR Historical 95%'], label='VaR Historical 95%', color='blue')\n",
    "plt.plot(var_results_df['Date'], var_results_df['VaR Historical 99%'], label='VaR Historical 99%', color='red')\n",
    "\n",
    "# Plot ES Historical 95% and 99%\n",
    "plt.plot(es_results_df['Date'], es_results_df['ES Historical 95%'], label='ES Historical 95%', color='green', linestyle='--')\n",
    "plt.plot(es_results_df['Date'], es_results_df['ES Historical 99%'], label='ES Historical 99%', color='orange', linestyle='--')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Historical VaR and ES Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for all indices\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# S&P 500\n",
    "sp500_returns = main_df['C_S&P500_Returns'].dropna()\n",
    "mu_sp500 = sp500_returns.mean()\n",
    "sigma_sp500 = sp500_returns.std()\n",
    "x_sp500 = np.linspace(mu_sp500 - 4*sigma_sp500, mu_sp500 + 4*sigma_sp500, 100)\n",
    "ax1.hist(sp500_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax1.plot(x_sp500, st.norm.pdf(x_sp500, mu_sp500, sigma_sp500), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_sp500 / np.sqrt(df/(df-2))\n",
    "    ax1.plot(x_sp500, st.t.pdf((x_sp500-mu_sp500)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax1.set_title('S&P500 Returns Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# DAX40\n",
    "dax_returns = main_df['C_Dax40_Returns'].dropna()\n",
    "mu_dax = dax_returns.mean()\n",
    "sigma_dax = dax_returns.std()\n",
    "x_dax = np.linspace(mu_dax - 4*sigma_dax, mu_dax + 4*sigma_dax, 100)\n",
    "ax2.hist(dax_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax2.plot(x_dax, st.norm.pdf(x_dax, mu_dax, sigma_dax), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_dax / np.sqrt(df/(df-2))\n",
    "    ax2.plot(x_dax, st.t.pdf((x_dax-mu_dax)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax2.set_title('DAX40 Returns Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# NIKKEI\n",
    "nikkei_returns = main_df['C_Nikkei_Returns'].dropna()\n",
    "mu_nikkei = nikkei_returns.mean()\n",
    "sigma_nikkei = nikkei_returns.std()\n",
    "x_nikkei = np.linspace(mu_nikkei - 4*sigma_nikkei, mu_nikkei + 4*sigma_nikkei, 100)\n",
    "ax3.hist(nikkei_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax3.plot(x_nikkei, st.norm.pdf(x_nikkei, mu_nikkei, sigma_nikkei), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_nikkei / np.sqrt(df/(df-2))\n",
    "    ax3.plot(x_nikkei, st.t.pdf((x_nikkei-mu_nikkei)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax3.set_title('NIKKEI Returns Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# EU Bond\n",
    "bond_returns = main_df['Interest_Bond_daily_rate'].dropna()\n",
    "mu_bond = bond_returns.mean()\n",
    "sigma_bond = bond_returns.std()\n",
    "x_bond = np.linspace(mu_bond - 4*sigma_bond, mu_bond + 4*sigma_bond, 100)\n",
    "ax4.hist(bond_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax4.plot(x_bond, st.norm.pdf(x_bond, mu_bond, sigma_bond), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_bond / np.sqrt(df/(df-2))\n",
    "    ax4.plot(x_bond, st.t.pdf((x_bond-mu_bond)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax4.set_title('EU Bond Returns Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3798712",
   "metadata": {},
   "source": [
    "## Historical simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b72584",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Define the confidence level\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Extract historical losses for S&P500 for the specified period\n",
    "sp500_losses = main_df[(main_df['Date'] >= '2012-01-04') & (main_df['Date'] <= '2021-12-31')]['C_S&P500_Loss']\n",
    "\n",
    "# Sort the losses in ascending order (smallest loss first, largest loss last)\n",
    "sorted_losses = sp500_losses.sort_values()\n",
    "\n",
    "# Calculate VaR at the specified confidence level\n",
    "# This finds the value at the threshold separating the worst (1-confidence_level)% losses\n",
    "VaR_sp500 = sorted_losses.quantile(confidence_level)\n",
    "\n",
    "# Calculate ES (Expected Shortfall)\n",
    "# This is the average of the losses that are greater than or equal to the VaR value\n",
    "tail_sp_losses = sorted_losses[sorted_losses >= VaR_sp500]\n",
    "ES_sp500 = tail_sp_losses.mean()\n",
    "\n",
    "print(f\"VaR (S&P500) at {confidence_level * 100}% confidence level: {VaR_sp500}\")\n",
    "print(f\"ES (S&P500) at {confidence_level * 100}% confidence level: {ES_sp500}\")\n",
    "\n",
    "# Plot the sorted losses (Empirical Cumulative Distribution Function - CDF)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plotting the empirical CDF: x-axis is loss value, y-axis is cumulative probability\n",
    "plt.plot(sorted_losses.values, np.linspace(0, 1, len(sorted_losses), endpoint=False), label='Sorted Losses CDF', color='blue')\n",
    "\n",
    "# Highlight VaR and ES on the graph\n",
    "# VaR is the loss value at the confidence level percentile\n",
    "plt.axvline(x=VaR_sp500, color='red', linestyle='--', label=f'VaR ({confidence_level * 100}%) = {VaR_sp500:.4f}')\n",
    "# ES is the average loss in the tail beyond VaR\n",
    "plt.axvline(x=ES_sp500, color='green', linestyle='--', label=f'ES ({confidence_level * 100}%) = {ES_sp500:.4f}')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Losses')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Sorted S&P 500 Losses CDF with VaR and ES (Historical Simulation)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf5bf1",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "confidence_level = 0.95\n",
    "# Extract historical losses for DAX40\n",
    "dax40_losses = main_df[(main_df['Date'] >= '2012-01-04') & (main_df['Date'] <= '2021-12-31')]['C_Dax40_Loss']\n",
    "\n",
    "# Sort the losses in ascending order\n",
    "sorted_dax40_losses = dax40_losses.sort_values()\n",
    "\n",
    "# Calculate VaR at the specified confidence level\n",
    "# This finds the value at the threshold separating the worst (1-confidence_level)% losses\n",
    "VaR_dax40 = sorted_dax40_losses.quantile(confidence_level)\n",
    "\n",
    "# Calculate ES (Expected Shortfall)\n",
    "# This is the average of the losses that are greater than or equal to the VaR value\n",
    "tail_dax_losses = sorted_dax40_losses[sorted_dax40_losses >= VaR_dax40]\n",
    "ES_dax40 = tail_dax_losses.mean()\n",
    "\n",
    "print(f\"VaR (DAX40) at {confidence_level * 100}% confidence level: {VaR_dax40}\")\n",
    "print(f\"ES (DAX40) at {confidence_level * 100}% confidence level: {ES_dax40}\")\n",
    "\n",
    "# Plot the sorted losses as a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_dax40_losses.values, np.linspace(0, 1, len(sorted_dax40_losses), endpoint=False), label='Sorted Losses', color='blue')\n",
    "\n",
    "# Highlight VaR and ES on the graph\n",
    "plt.axvline(x=VaR_dax40, color='red', linestyle='--', label=f'VaR ({confidence_level * 100}%)')\n",
    "plt.axvline(x=ES_dax40, color='green', linestyle='--', label=f'ES ({confidence_level * 100}%)')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Losses')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Sorted Losses with VaR and ES for DAX40')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772db75",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "confidence_level = 0.95\n",
    "\n",
    "# Extract historical losses for Nikkei\n",
    "nikkei_losses = main_df[(main_df['Date'] >= '2012-01-04') & (main_df['Date'] <= '2021-12-31')]['C_Nikkei_Loss']\n",
    "\n",
    "# Sort the losses in ascending order\n",
    "sorted_nikkei_losses = nikkei_losses.sort_values()\n",
    "\n",
    "# Calculate VaR at the specified confidence level\n",
    "# This finds the value at the threshold separating the worst (1-confidence_level)% losses\n",
    "VaR_nikkei = sorted_nikkei_losses.quantile(confidence_level)\n",
    "\n",
    "# Calculate ES (Expected Shortfall)\n",
    "# This is the average of the losses that are greater than or equal to the VaR value\n",
    "tail_nik_losses = sorted_nikkei_losses[sorted_nikkei_losses >= VaR_nikkei]\n",
    "ES_nikkei = tail_nik_losses.mean()\n",
    "\n",
    "print(f\"VaR (Nikkei) at {confidence_level * 100}% confidence level: {VaR_nikkei}\")\n",
    "print(f\"ES (Nikkei) at {confidence_level * 100}% confidence level: {ES_nikkei}\")\n",
    "\n",
    "# Plot the sorted losses as a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_nikkei_losses.values, np.linspace(0, 1, len(sorted_nikkei_losses), endpoint=False), label='Sorted Losses', color='blue')\n",
    "\n",
    "# Highlight VaR and ES on the graph\n",
    "plt.axvline(x=VaR_nikkei, color='red', linestyle='--', label=f'VaR ({confidence_level * 100}%)')\n",
    "plt.axvline(x=ES_nikkei, color='green', linestyle='--', label=f'ES ({confidence_level * 100}%)')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Losses')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Sorted Losses with VaR and ES for Nikkei')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b52406",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "## Backtesting VaR and ES\n",
    "\n",
    "In this section, we perform backtesting on the calculated Value at Risk (VaR) and Expected Shortfall (ES) measures. Backtesting helps assess the accuracy and reliability of the risk models.\n",
    "\n",
    "We will:\n",
    "1.  **Calculate Violations:** Identify the days where the actual portfolio loss exceeded the predicted VaR.\n",
    "2.  **Compare Actual vs. Expected Violations (VaR):** Group violations by year and compare the observed number of violations against the number expected based on the confidence level (alpha).\n",
    "3.  **Compare Actual Shortfall vs. Predicted ES (ES):** For the days a violation occurred, compare the average actual loss (shortfall) against the predicted ES, grouped by year.\n",
    "4.  **Visualize Violations:** Plot the occurrences of violations over time to visually inspect for clustering or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e41335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_violations(actual_losses, var_predictions):\n",
    "    \"\"\"Checks for VaR violations.\"\"\"\n",
    "    return actual_losses > var_predictions\n",
    "\n",
    "def backtest_var(violations, alpha, dates):\n",
    "    \"\"\"Compares actual vs. expected VaR violations yearly.\"\"\"\n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date': # Ensure index is Date for grouping\n",
    "         violations = violations.set_index(dates)\n",
    "            \n",
    "    violations_df = pd.DataFrame({'Violations': violations, 'Year': violations.index.year})\n",
    "    yearly_violations = violations_df.groupby('Year')['Violations'].sum()\n",
    "    yearly_counts = violations_df.groupby('Year')['Violations'].count()\n",
    "    \n",
    "    expected_violations = yearly_counts * (1 - alpha)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Actual Violations': yearly_violations,\n",
    "        'Expected Violations': expected_violations,\n",
    "        'Total Observations': yearly_counts\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def backtest_es(actual_losses, violations, es_predictions, dates):\n",
    "    \"\"\"Compares actual average shortfall vs. predicted ES yearly.\"\"\"\n",
    "    # Ensure inputs are pandas Series with Date index\n",
    "    if not isinstance(actual_losses, pd.Series):\n",
    "        actual_losses = pd.Series(actual_losses, index=dates)\n",
    "    elif actual_losses.index.name != 'Date':\n",
    "        actual_losses = actual_losses.set_index(dates)\n",
    "        \n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date':\n",
    "        violations = violations.set_index(dates)\n",
    "        \n",
    "    if not isinstance(es_predictions, pd.Series):\n",
    "        es_predictions = pd.Series(es_predictions, index=dates)\n",
    "    elif es_predictions.index.name != 'Date':\n",
    "        es_predictions = es_predictions.set_index(dates)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_Loss': actual_losses,\n",
    "        'Violation': violations,\n",
    "        'Predicted_ES': es_predictions,\n",
    "        'Year': actual_losses.index.year\n",
    "    })\n",
    "    \n",
    "    # Filter for violations\n",
    "    violation_data = results_df[results_df['Violation']]\n",
    "    \n",
    "    # Calculate yearly averages\n",
    "    yearly_avg_actual_shortfall = violation_data.groupby('Year')['Actual_Loss'].mean()\n",
    "    yearly_avg_predicted_es = violation_data.groupby('Year')['Predicted_ES'].mean()\n",
    "    yearly_violation_count = violation_data.groupby('Year').size()\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Avg Actual Shortfall': yearly_avg_actual_shortfall,\n",
    "        'Avg Predicted ES': yearly_avg_predicted_es,\n",
    "        'Violation Count': yearly_violation_count\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def plot_violations(violations, dates, title):\n",
    "    \"\"\"Plots VaR violations over time.\"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(dates, violations, 'ro', markersize=4, alpha=0.7, label='Violation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Violation (1=Yes, 0=No)')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "def run_backtesting(main_df, var_results_df, es_results_df):\n",
    "    \"\"\"Runs the backtesting process for VaR and ES models.\"\"\"\n",
    "    # Align main_df with var/es results (which start after the initial window)\n",
    "    backtest_dates = var_results_df['Date']\n",
    "    backtest_data = main_df[main_df['Date'].isin(backtest_dates)].set_index('Date')\n",
    "    actual_losses = backtest_data['Portfolio_loss']\n",
    "\n",
    "    # Confidence levels used\n",
    "    alphas = [0.95, 0.99]\n",
    "    alpha_indices = {0.95: 0, 0.99: 1} # Index mapping for results arrays\n",
    "\n",
    "    # Iterate through models (columns in var_results_df/es_results_df)\n",
    "    var_model_cols = [col for col in var_results_df.columns if col != 'Date']\n",
    "    es_model_cols = [col for col in es_results_df.columns if col != 'Date']\n",
    "\n",
    "    for i, model_name in enumerate(var_model_cols):\n",
    "        print(f\"\\n--- Backtesting for Model: {model_name} ---\")\n",
    "        \n",
    "        # Extract predictions for this model\n",
    "        # Need to handle the fact that predictions are stored as arrays [pred_95, pred_99]\n",
    "        var_preds_list = var_results_df[model_name].tolist()\n",
    "        # Ensure alignment between var and es model columns\n",
    "        if i < len(es_model_cols):\n",
    "            es_preds_list = es_results_df[es_model_cols[i]].tolist()\n",
    "        else:\n",
    "            print(f\"  Warning: No matching ES column found for {model_name}. Skipping ES backtest.\")\n",
    "            es_preds_list = None\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            alpha_idx = alpha_indices[alpha]\n",
    "            print(f\"\\nConfidence Level: {alpha*100}%\")\n",
    "            \n",
    "            # Extract predictions for the specific alpha\n",
    "            # Handle potential errors if data isn't as expected (e.g., not a list/array)\n",
    "            try:\n",
    "                var_predictions = pd.Series([p[alpha_idx] for p in var_preds_list], index=backtest_dates)\n",
    "                if es_preds_list:\n",
    "                    es_predictions = pd.Series([p[alpha_idx] for p in es_preds_list], index=backtest_dates)\n",
    "                else:\n",
    "                    es_predictions = None\n",
    "            except (TypeError, IndexError) as e:\n",
    "                print(f\"  Error extracting predictions for alpha={alpha}: {e}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # 1. Calculate Violations\n",
    "            violations = calculate_violations(actual_losses, var_predictions)\n",
    "            \n",
    "            # 2. Backtest VaR\n",
    "            var_summary = backtest_var(violations, alpha, backtest_dates)\n",
    "            print(\"\\nVaR Backtest Summary (Yearly):\")\n",
    "            display(var_summary)\n",
    "            \n",
    "            # 3. Backtest ES\n",
    "            if es_predictions is not None:\n",
    "                es_summary = backtest_es(actual_losses, violations, es_predictions, backtest_dates)\n",
    "                print(\"\\nES Backtest Summary (Yearly):\")\n",
    "                display(es_summary)\n",
    "            \n",
    "            # 4. Plot Violations\n",
    "            plot_violations(violations, backtest_dates, f'VaR Violations for {model_name} (alpha={alpha})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ad9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the backtesting function with the required dataframes\n",
    "run_backtesting(main_df, var_results_df, es_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced84f1",
   "metadata": {},
   "source": [
    "### Backtesting Interpretation\n",
    "\n",
    "Review the tables and plots above:\n",
    "\n",
    "*   **VaR Backtest:** Compare 'Actual Violations' to 'Expected Violations' each year. Significant deviations might indicate issues with the VaR model's calibration. If actual violations consistently exceed expected, the model underestimates risk. If they are consistently lower, it might be too conservative.\n",
    "*   **ES Backtest:** Compare 'Avg Actual Shortfall' to 'Avg Predicted ES'. If the actual average shortfall during violations is consistently higher than the predicted ES, the model underestimates the severity of tail losses.\n",
    "*   **Violation Plots:** Look for patterns. Ideally, violations should be randomly distributed. Clustering of violations suggests the model fails to adapt quickly to changing market volatility (violation dependence)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
