{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc2ba2c",
   "metadata": {},
   "source": [
    "### Initial package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f45638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from typing import Dict, List\n",
    "import random\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c440bd4",
   "metadata": {},
   "source": [
    "## Initial files being read in and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9ce118",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "# Read the data\n",
    "main_df = pd.read_csv('Data/Cleaned_Indices_Assignment1.csv', sep=';')\n",
    "\n",
    "# Read the interest rate data\n",
    "#interest_rate_df = pd.read_csv('Data/ECB_Rates_2012_to_2022.csv', sep=';')\n",
    "interest_rate_bond_df = pd.read_csv('Data/ECB_Data_10yr_Treasury_bond.csv', sep=',')\n",
    "\n",
    "# Convert date columns to datetime format for proper merging\n",
    "main_df['Date'] = pd.to_datetime(main_df['Date'], format='%d-%m-%Y')\n",
    "#interest_rate_df['Date'] = pd.to_datetime(interest_rate_df['Date'], format='%d-%m-%Y')\n",
    "#gov_bond_investment_df['Date'] = pd.to_datetime(gov_bond_investment_df['Date'], format='%Y-%m-%d')\n",
    "interest_rate_bond_df['Date'] = pd.to_datetime(interest_rate_bond_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Merge the dataframes on the Date column\n",
    "main_df = pd.merge(main_df, interest_rate_bond_df, on='Date', how='left')\n",
    "#main_df = pd.merge(main_df, gov_bond_investment_df, on='Date', how='left')\n",
    "\n",
    "# Remove rows where the bond does not have a yield curve spot rate (Market closed?)\n",
    "main_df = main_df.dropna(axis=0, subset=['Yield curve spot rate, 10-year maturity - Government bond'])\n",
    "\n",
    "# Filter the dataframe to start from 2012-01-04\n",
    "main_df = main_df[main_df['Date'] >= '2012-01-04']\n",
    "main_df = main_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c10b0c",
   "metadata": {},
   "source": [
    "### government bond column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e954a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Add a column for the interest bond value per day\n",
    "days_per_annum = 365\n",
    "interest_bond = 1500000\n",
    "\n",
    "# Initialize the arrays with appropriate lengths matching the DataFrame\n",
    "interest_bond_vector = np.zeros(len(main_df))\n",
    "interest_bond_profit_vector = np.zeros(len(main_df))\n",
    "interest_bond_loss_vector = np.zeros(len(main_df))\n",
    "daily_rates = np.zeros(len(main_df))\n",
    "\n",
    "# Set initial value\n",
    "interest_bond_vector[0] = interest_bond\n",
    "\n",
    "# Calculate bond values day by day based on the daily yield rate\n",
    "for i in range(len(main_df)):\n",
    "    # Adding 1.5% to account for the credit risk spread\n",
    "    daily_rate = (((main_df['Yield curve spot rate, 10-year maturity - Government bond'].iloc[i] + 1.5) / (days_per_annum)) * (7/5)) / 100\n",
    "    daily_rates[i] = daily_rate\n",
    "    \n",
    "    if i > 0:\n",
    "        previous_value = interest_bond_vector[i-1]\n",
    "        current_value = previous_value * (1 + daily_rate)\n",
    "        interest_bond_vector[i] = current_value\n",
    "        \n",
    "        # Calculate change, profit/loss and return\n",
    "        change = current_value - previous_value\n",
    "        interest_bond_profit_vector[i] = change\n",
    "        interest_bond_loss_vector[i] = -change\n",
    "\n",
    "# Add vectors to the dataframe\n",
    "main_df['Interest_Bond'] = interest_bond_vector\n",
    "main_df['Interest_Bond_Profit'] = interest_bond_profit_vector\n",
    "main_df['Interest_Bond_Loss'] = interest_bond_loss_vector\n",
    "main_df['Interest_Bond_daily_rate'] = daily_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de917d",
   "metadata": {},
   "source": [
    "## Portfolio details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3557320",
   "metadata": {},
   "source": [
    "### details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d95594",
   "metadata": {},
   "source": [
    "#### Instruments:\n",
    "- **S&P500**\n",
    "- **DAX40**\n",
    "- **NIKKEI**\n",
    "- **EU Government Bond (10-year maturity, AAA-rated)**\n",
    "\n",
    "#### Invested amount:\n",
    "- **10,000,000 EURO**\n",
    "\n",
    "#### Period:\n",
    "- **01/01/2012 - 31/12/2022**\n",
    "\n",
    "#### Weights:\n",
    "- **S&P500**: 0.4  \n",
    "- **DAX40**: 0.3  \n",
    "- **NIKKEI**: 0.15  \n",
    "- **EU Government Bond**: 0.15  \n",
    "\n",
    "#### Measures:\n",
    "- **Value at Risk (VaR)**: 1, 5, 10 days  \n",
    "- **Expected Shortfall (ES)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d60941",
   "metadata": {},
   "source": [
    "### weights and currency correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bd314",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(op, a, b, use_numexpr)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[39m, in \u001b[36m_evaluate_standard\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m     72\u001b[39m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: can't multiply sequence by non-int of type 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     13\u001b[39m starting_row = main_df[main_df[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m] == starting_date]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# main_df = main_df[main_df['Date'] >= starting_date]\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# main_df = main_df.reset_index(drop=True)\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# invested_amount_NIKKEI = starting_investment * weights['NIKKEI'] / jpy_to_eur\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# invested_amount_EU_BOND = starting_investment * weights['EU-BOND']\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33mcorrected_value_SP500\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mmain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mS&P500_Closing\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUSD/EUR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     28\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33mcorrected_value_DAX40\u001b[39m\u001b[33m'\u001b[39m] = main_df[\u001b[33m'\u001b[39m\u001b[33mDax40_Closing\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     29\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33mcorrected_value_NIKKEI\u001b[39m\u001b[33m'\u001b[39m] = main_df[\u001b[33m'\u001b[39m\u001b[33mNikkei_Closing\u001b[39m\u001b[33m'\u001b[39m] * main_df[\u001b[33m'\u001b[39m\u001b[33mJPY/EUR\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\arraylike.py:202\u001b[39m, in \u001b[36mOpsMixin.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__mul__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[39m, in \u001b[36mSeries._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m   6134\u001b[39m     \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._align_for_op(other)\n\u001b[32m-> \u001b[39m\u001b[32m6135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:1382\u001b[39m, in \u001b[36mIndexOpsMixin._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1379\u001b[39m     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(result, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    279\u001b[39m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    221\u001b[39m         left.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    222\u001b[39m     ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m         result = \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhruv\\Downloads\\Master Vakken\\Assignments\\Quantitative Financial Risk Management\\Assignment_1_FQRM\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[39m, in \u001b[36m_masked_arith_op\u001b[39m\u001b[34m(x, y, op)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         result[mask] = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[31mTypeError\u001b[39m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "# initial investment \n",
    "weights = {\n",
    "    'S&P500': 0.4,\n",
    "    'DAX40': 0.3,\n",
    "    'NIKKEI': 0.15,\n",
    "    'EU-BOND': 0.15,\n",
    "}\n",
    "\n",
    "starting_investment = 10000000  # 10 million euros\n",
    "starting_date = '2012-01-04'\n",
    "\n",
    "# Filter the main_df for the starting date\n",
    "starting_row = main_df[main_df['Date'] == starting_date]\n",
    "\n",
    "# Extract the exchange rates for the starting date\n",
    "usd_to_eur = float(starting_row['USD/EUR'].iloc[0])\n",
    "jpy_to_eur = float(starting_row['JPY/EUR'].iloc[0])\n",
    "\n",
    "# Calculate the invested amounts\n",
    "invested_amount_SP500 = starting_investment * weights['S&P500'] / usd_to_eur\n",
    "invested_amount_DAX40 = starting_investment * weights['DAX40']\n",
    "invested_amount_NIKKEI = starting_investment * weights['NIKKEI'] / jpy_to_eur\n",
    "invested_amount_EU_BOND = starting_investment * weights['EU-BOND']\n",
    "\n",
    "main_df['corrected_value_SP500'] = main_df['S&P500_Closing'] * main_df['USD/EUR']\n",
    "main_df['corrected_value_DAX40'] = main_df['Dax40_Closing']\n",
    "main_df['corrected_value_NIKKEI'] = main_df['Nikkei_Closing'] * main_df['JPY/EUR']\n",
    "\n",
    "main_df['C_S&P500_Returns'] = main_df['corrected_value_SP500'].pct_change(fill_method=None)\n",
    "main_df['C_Dax40_Returns'] = main_df['corrected_value_DAX40'].pct_change(fill_method=None)\n",
    "main_df['C_Nikkei_Returns'] = main_df['corrected_value_NIKKEI'].pct_change(fill_method=None)\n",
    "\n",
    "invested_amounts = [\n",
    "    invested_amount_SP500, #in USD\n",
    "    invested_amount_DAX40, #in EUR\n",
    "    invested_amount_NIKKEI, #in JPY\n",
    "    invested_amount_EU_BOND #in EUR\n",
    "]\n",
    "\n",
    "print(invested_amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a6bef",
   "metadata": {},
   "source": [
    "### Returns Portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to track investments for each asset\n",
    "# Initialize the first day with the initial invested amounts\n",
    "main_df.loc[0, 'SP500_Investment'] = invested_amount_SP500\n",
    "main_df.loc[0, 'DAX40_Investment'] = invested_amount_DAX40\n",
    "main_df.loc[0, 'NIKKEI_Investment'] = invested_amount_NIKKEI\n",
    "main_df.loc[0, 'EU_BOND_Investment'] = invested_amount_EU_BOND\n",
    "\n",
    "# Calculate daily investment values for subsequent days\n",
    "# This uses cumulative returns to track the value growth\n",
    "for i in range(1, len(main_df)):\n",
    "    # S&P 500 in USD\n",
    "    main_df.loc[i, 'SP500_Investment'] = main_df.loc[i-1, 'SP500_Investment'] * (1 + main_df.loc[i, 'C_S&P500_Returns'])\n",
    "    \n",
    "    # DAX 40 in EUR\n",
    "    main_df.loc[i, 'DAX40_Investment'] = main_df.loc[i-1, 'DAX40_Investment'] * (1 + main_df.loc[i, 'C_Dax40_Returns'])\n",
    "    \n",
    "    # NIKKEI in JPY\n",
    "    main_df.loc[i, 'NIKKEI_Investment'] = main_df.loc[i-1, 'NIKKEI_Investment'] * (1 + main_df.loc[i, 'C_Nikkei_Returns'])\n",
    "    \n",
    "# EU Government Bond value is already calculated in the Interest_Bond column\n",
    "main_df['EU_BOND_Investment'] = main_df['Interest_Bond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fc2cb",
   "metadata": {},
   "source": [
    "# Methods input values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd3e01",
   "metadata": {},
   "source": [
    "### Portfolio change Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total portfolio value in EUR for each day\n",
    "main_df['Portfolio_Value_EUR'] = (\n",
    "    main_df['SP500_Investment'].fillna(0) * main_df['USD/EUR'] +\n",
    "    main_df['DAX40_Investment'].fillna(0) +\n",
    "    main_df['NIKKEI_Investment'].fillna(0) * main_df['JPY/EUR'] +\n",
    "    main_df['EU_BOND_Investment'].fillna(0)\n",
    ")\n",
    "\n",
    "# First day should be the initial investment amount\n",
    "main_df.loc[0, 'Portfolio_Value_EUR'] = starting_investment\n",
    "\n",
    "# Calculate the daily change in portfolio value (profit/loss)\n",
    "main_df['Portfolio_Change_EUR'] = main_df['Portfolio_Value_EUR'].diff()\n",
    "main_df.loc[0, 'Portfolio_Change_EUR'] = 0.0  # Set the first day's change to 0\n",
    "\n",
    "# Portfolio loss is the negative of the daily change\n",
    "main_df['Portfolio_loss'] = -main_df['Portfolio_Change_EUR']\n",
    "\n",
    "# Set the first day's loss to 0 (there's no previous day to compare with)\n",
    "main_df.loc[0, 'Portfolio_loss'] = 0.0\n",
    "\n",
    "# Display the relevant columns to verify\n",
    "display(main_df[['Date', 'SP500_Investment', 'DAX40_Investment', 'NIKKEI_Investment', \n",
    "                'EU_BOND_Investment', 'USD/EUR', 'JPY/EUR', 'Portfolio_Value_EUR', \n",
    "                'Portfolio_Change_EUR', 'Portfolio_loss']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_loss_variables(time_window, current_date):\n",
    "    # Calculate the mean and standard deviation of portfolio loss from the time windows\n",
    "    loss_dict = {\n",
    "        \"Date\": current_date,\n",
    "        \"Portfolio_mean_loss\": np.nanmean(time_window['Portfolio_loss']),\n",
    "        \"Portfolio_std_loss\": np.nanstd(time_window['Portfolio_loss'])\n",
    "    }\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2842c0",
   "metadata": {},
   "source": [
    "### Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Get the VaR of the normal model\n",
    "\n",
    "    Inputs:\n",
    "        alpha   double, level\n",
    "        r       double, expected return\n",
    "        s       double, volatility\n",
    "        df      (optional) double, degrees of freedom for student-t\n",
    "\n",
    "    Return value:\n",
    "        dVaR    double, VaR\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        dVaR0= st.norm.ppf(alpha)\n",
    "        dVaR = r + s*dVaR0\n",
    "    else:\n",
    "        dVaR0= st.t.ppf(alpha, df= df)\n",
    "\n",
    "        dS2t= df/(df-2)\n",
    "\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dVaR= r + c*dVaR0\n",
    "    return dVaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12755bb3",
   "metadata": {},
   "source": [
    "### Expected Shortfall (ES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb43cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Get the ES of the normal/student model\n",
    "\n",
    "    Inputs:\n",
    "        alpha   double, level\n",
    "        r       double, expected return\n",
    "        s       double, volatility\n",
    "        df      (optional, default= 0/normal) double, df\n",
    "\n",
    "    Return value:\n",
    "        dES     double, ES\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        dVaR0= st.norm.ppf(alpha)\n",
    "        dES0= st.norm.pdf(dVaR0) / (1-alpha)\n",
    "        dES= r + s*dES0\n",
    "    else:\n",
    "        dVaR0= st.t.ppf(alpha, df= df)\n",
    "        dES0= st.t.pdf(dVaR0, df= df)*((df + dVaR0**2)/(df-1)) / (1-alpha)\n",
    "\n",
    "        dS2t= df/(df-2)\n",
    "        c= s / np.sqrt(dS2t)\n",
    "        dES= r + c*dES0\n",
    "    return dES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d7a40",
   "metadata": {},
   "source": [
    "## 1. var/cov multivar normal dist & T-distribution & Historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8ad66",
   "metadata": {},
   "source": [
    "### Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e631c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var_cov(current_date, vAlpha, mean_loss, portfolio_std_loss, df=0):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk using variance-covariance method.\n",
    "    \n",
    "    Parameters:\n",
    "    - window: DataFrame containing the time window\n",
    "    - current_date: Current date for reporting\n",
    "    - vAlpha: Confidence levels (array)\n",
    "    - mean_loss: Mean loss for the portfolio\n",
    "    - portfolio_std_loss: Standard deviation of losses\n",
    "    - df: Degrees of freedom for t-distribution (0 for normal distribution)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with date, VaR and ES values\n",
    "    \"\"\"\n",
    "    # Calculate VaR and ES\n",
    "    var_result = VaR(vAlpha, mean_loss, portfolio_std_loss, df=df)\n",
    "    es_result = ES(vAlpha, mean_loss, portfolio_std_loss, df=df)\n",
    "    \n",
    "    # Set label for distribution type\n",
    "    if df == 0:\n",
    "        dist_label = \"Normal\"\n",
    "    else:\n",
    "        dist_label = f\"T{df}\"\n",
    "        \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        f'VaR {dist_label}': var_result,\n",
    "        f'ES {dist_label}': es_result\n",
    "    }\n",
    "\n",
    "def calculate_historical_var_es(window, current_date, vAlpha):\n",
    "    \"\"\"\n",
    "    Calculate VaR and ES using historical simulation method.\n",
    "    \"\"\"\n",
    "    # Extract portfolio loss values from the window\n",
    "    historical_losses = window['Portfolio_loss'].dropna()\n",
    "    \n",
    "    # Sort losses in ascending order\n",
    "    sorted_losses = np.sort(historical_losses)\n",
    "    \n",
    "    # Calculate VaR for alpha levels\n",
    "    var_hist = np.percentile(sorted_losses, 100*vAlpha)\n",
    "    \n",
    "    # Calculate ES for each alpha level\n",
    "    es_hist = []\n",
    "    for i, alpha in enumerate(vAlpha):\n",
    "        es_hist.append(sorted_losses[sorted_losses >= var_hist[i]].mean())\n",
    "    \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        'VaR Historical': var_hist,\n",
    "        'ES Historical': es_hist\n",
    "    }\n",
    "\n",
    "def calculate_multiday_var(vAlpha, interval, sample_size):\n",
    "    \"\"\"\n",
    "    Calculate multi-day VaR using the historical simulation method.\n",
    "\n",
    "    Parameters:\n",
    "    - vAlpha: Confidence levels (array)\n",
    "    - interval: Number of days for the multi-day calculation (e.g., 5 or 10)\n",
    "    - sample_size: Number of initial rows to skip (burn-in)\n",
    "\n",
    "    Returns:\n",
    "    - multi_day_losses_df: DataFrame with multi-day losses and VaR estimates\n",
    "    \"\"\"\n",
    "    # Filter data for the period we want to analyze\n",
    "    time_window_multi = main_df[(main_df['Date'] >= '2012-01-05') & (main_df['Date'] <= '2021-12-31')].reset_index(drop=True)\n",
    "    multi_day_losses = []\n",
    "\n",
    "    # Calculate rolling sum of losses over the interval, stepping by 'interval'\n",
    "    for i in range(sample_size, len(time_window_multi) - interval + 1, interval):\n",
    "        window = time_window_multi.iloc[i:i+interval]\n",
    "        if len(window) == interval:\n",
    "            total_loss = window['Portfolio_loss'].sum()\n",
    "            date = window['Date'].iloc[-1]\n",
    "            daily_loss = window['Portfolio_loss'].iloc[-1]\n",
    "            multi_day_losses.append({'Date': date, 'Portfolio_loss': total_loss, 'Daily_loss': daily_loss})\n",
    "\n",
    "    multi_day_losses_df = pd.DataFrame(multi_day_losses)\n",
    "\n",
    "    # Calculate VaR as percentiles of the historical multi-day losses up to each row\n",
    "    var_list = []\n",
    "    for idx in range(len(multi_day_losses_df)):\n",
    "        hist_losses = multi_day_losses_df['Portfolio_loss'][:idx+1]\n",
    "        # For losses, VaR should be negative at high confidence, so use lower percentiles (100*(1-alpha))\n",
    "        var_row = [np.percentile(hist_losses, 100 * (1 - alpha)) for alpha in vAlpha]\n",
    "        var_list.append(var_row)\n",
    "    multi_day_losses_df[f'VaR_{interval}d_reg'] = var_list\n",
    "\n",
    "    # Calculate VaR per row using the square root of time rule for each alpha level\n",
    "    sqrt_var_list = []\n",
    "    for idx in range(len(multi_day_losses_df)):\n",
    "        hist_daily_losses = multi_day_losses_df['Daily_loss'][:idx+1]\n",
    "        sqrt_var_row = [np.percentile(hist_daily_losses, 100 * (1 - alpha)) * np.sqrt(interval) for alpha in vAlpha]\n",
    "        sqrt_var_list.append(sqrt_var_row)\n",
    "    multi_day_losses_df[f'VaR_{interval}d_sqrt'] = sqrt_var_list\n",
    "\n",
    "    # drop the first row as it will not have a correct prediction\n",
    "    multi_day_losses_df = multi_day_losses_df.iloc[1:]\n",
    "    return multi_day_losses_df[['Date', 'Portfolio_loss', f'VaR_{interval}d_reg', 'Daily_loss', f'VaR_{interval}d_sqrt']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d31b7",
   "metadata": {},
   "source": [
    "## GARCH(1,1) with constant conditional correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69656712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum likelihood estimation of GARCH(1,1) parameters\n",
    "def garch_likelihood(params, returns):\n",
    "    omega, alpha, beta = params\n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "    var[0] = omega / (1 - alpha - beta)\n",
    "    ll = 0\n",
    "    for t in range(2, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "        ll += 0.5 * (np.log(2 * np.pi) + np.log(var[t]) + returns[t]**2 / var[t])\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) parameter estimation using MLE\n",
    "# This function estimates the parameters of a GARCH(1,1) model using maximum likelihood estimation (MLE).\n",
    "def parameter_estimation_GARCH(returns):\n",
    "    # Run the optimization\n",
    "    result = minimize(\n",
    "        garch_likelihood,\n",
    "        x0=[0.02, 0.13, 0.86],\n",
    "        args=(returns,),\n",
    "        method='SLSQP',\n",
    "        bounds=[(1e-6, None), (0, 0.99), (0, 0.99)],\n",
    "        constraints=[\n",
    "            {'type': 'ineq', 'fun': lambda x: 0.999 - x[1] - x[2]}\n",
    "        ],\n",
    "        options={'disp': True}\n",
    "    )\n",
    "\n",
    "    # Return the optimization result\n",
    "    return result\n",
    "    \n",
    "# result = parameter_estimation_GARCH(main_df['Portfolio_Daily_Returns'].dropna())\n",
    "# Check the optimization result\n",
    "# if result.success:\n",
    "#     print(f\"Optimized parameters: omega={result.x[0]}, alpha={result.x[1]}, beta={result.x[2]}\")\n",
    "# else:\n",
    "#     print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the GARCH(1,1) volatility for returns t\n",
    "def garch_volatility(returns):\n",
    "    param = [0.000002, 0.13, 0.86]\n",
    "    param = type('obj', (object,), {'x': param})\n",
    "    omega, alpha, beta = param.x\n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "\n",
    "    var[0]= param.x[0] / (1 - param.x[1] - param.x[2])\n",
    "    for t in range(2, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "    return np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation matrix of the 4 time series\n",
    "# the method must take the returns as matrix of 4 vectors and return the correlation matrix\n",
    "def correlation_matrix(returns):\n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.cov(returns.T)\n",
    "    \n",
    "    # Calculate the standard deviations of each asset\n",
    "    std_devs = np.sqrt(np.diag(cov_matrix))\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = cov_matrix / np.outer(std_devs, std_devs)\n",
    "    \n",
    "    return corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf018fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_matrix(returns, corr_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the covariance matrix using GARCH(1,1) volatilities and a given correlation matrix.\n",
    "    \"\"\"\n",
    "    # Calculate GARCH(1,1) volatilities for each column\n",
    "    volatilities_dict = {}\n",
    "    for column in returns.columns:\n",
    "        column_returns = returns[column].dropna().reset_index(drop=True)\n",
    "        volatilities_dict[column] = garch_volatility(column_returns)\n",
    "    volatilities = np.array([vol[-1] for vol in volatilities_dict.values()])\n",
    "\n",
    "    # Construct the covariance matrix\n",
    "    cov_matrix = np.outer(volatilities, volatilities) * corr_matrix\n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffe398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_var_es_and_variance(weights, cov_matrix, alpha=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the portfolio variance, volatility, VaR, and ES using the normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: array-like, portfolio weights\n",
    "    - cov_matrix: np.ndarray, covariance matrix\n",
    "    - alpha: float, confidence level for VaR/ES (default 0.99)\n",
    "\n",
    "    Returns:\n",
    "    - dict with keys: 'variance', 'volatility', 'VaR', 'ES'\n",
    "    \"\"\"\n",
    "    # Portfolio variance and volatility\n",
    "    port_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    port_volatility = np.sqrt(port_variance)\n",
    "\n",
    "    # VaR and ES (normal distribution)\n",
    "    VaR = -port_volatility * st.norm.ppf(alpha)\n",
    "    ES = -port_volatility * (st.norm.pdf(st.norm.ppf(1 - alpha)) / (1 - alpha))\n",
    "\n",
    "    return VaR, ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to run the analysis\n",
    "def main_analysis(time_window_size, vAlpha):\n",
    "    # Initialize lists to store results\n",
    "    portfolio_VaR_list = []\n",
    "    portfolio_ES_list = []\n",
    "    dates = []\n",
    "    corr_matrix = correlation_matrix(main_df[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna())\n",
    "    \n",
    "    main_df['Portfolio_Daily_Returns'] = (\n",
    "    weights[0] * main_df['C_S&P500_Returns'] +\n",
    "    weights[1] * main_df['C_Dax40_Returns'] +\n",
    "    weights[2] * main_df['C_Nikkei_Returns'] +\n",
    "    weights[3] * main_df['Interest_Bond_daily_rate']\n",
    ")\n",
    "\n",
    "    # Iterate through the dataset with a rolling time window\n",
    "    for i in range(time_window_size, len(main_df)):\n",
    "        # Extract the rolling time window\n",
    "        window = main_df.iloc[i - time_window_size:i]\n",
    "        current_date = main_df.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate the returns for each asset in the window\n",
    "        returns = window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "\n",
    "        # Define the weights for the portfolio\n",
    "        weights_arr = np.array([0.4, 0.3, 0.15, 0.15])\n",
    "\n",
    "        # Calculate the covariance matrix\n",
    "        cov_matrix = covariance_matrix(returns, corr_matrix)\n",
    "\n",
    "        # Calculate the portfolio VaR and ES for each alpha in vAlpha\n",
    "        VaR_list = []\n",
    "        ES_list = []\n",
    "        for alpha in vAlpha:\n",
    "            VaR, ES = portfolio_var_es_and_variance(weights_arr, cov_matrix, alpha=alpha)\n",
    "            VaR_list.append(VaR)\n",
    "            ES_list.append(ES)\n",
    "\n",
    "        # Append results\n",
    "        portfolio_VaR_list.append(VaR_list)\n",
    "        portfolio_ES_list.append(ES_list)\n",
    "        dates.append(current_date)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    # Calculate daily portfolio returns\n",
    "    daily_portfolio_returns = main_df['Portfolio_Daily_Returns'][time_window_size:].reset_index(drop=True)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Portfolio VaR': portfolio_VaR_list,\n",
    "        'Portfolio ES': portfolio_ES_list,\n",
    "        'Portfolio Daily Returns': daily_portfolio_returns\n",
    "    })\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3153dd58",
   "metadata": {},
   "source": [
    "## EWMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7439c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monetary_values(var_df, es_df, main_df):\n",
    "    \"\"\"\n",
    "    Returns two DataFrames containing only monetary VaR and ES columns,\n",
    "    calculated by multiplying return-based values with portfolio values.\n",
    "    \"\"\"\n",
    "    # Ensure datetime format and alignment\n",
    "    var_df.index = pd.to_datetime(var_df.index)\n",
    "    es_df.index = pd.to_datetime(es_df.index)\n",
    "    main_df['Date'] = pd.to_datetime(main_df['Date'])\n",
    "\n",
    "    # Align portfolio values to var_df index\n",
    "    portfolio_values = main_df.set_index('Date').loc[var_df.index, 'Portfolio_Value_EUR']\n",
    "\n",
    "    # Create new DataFrames to hold only the monetary results\n",
    "    monetary_var_df = pd.DataFrame(index=var_df.index)\n",
    "    monetary_es_df = pd.DataFrame(index=es_df.index)\n",
    "\n",
    "    # Add monetary VaR columns\n",
    "    for col in var_df.columns:\n",
    "        monetary_var_df[f'{col}'] = var_df[col] * portfolio_values *-1\n",
    "\n",
    "    # Add monetary ES columns\n",
    "    for col in es_df.columns:\n",
    "        monetary_es_df[f'{col}'] = es_df[col] * portfolio_values *-1\n",
    "\n",
    "    return monetary_var_df, monetary_es_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewma_volatility(\n",
    "    returns: pd.DataFrame, \n",
    "    lambdas: List[float] = [0.94, 0.97]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute EWMA volatility for each risk factor using different lambda values.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    returns : pd.DataFrame\n",
    "        DataFrame of returns (T x N), excluding Date column\n",
    "    lambdas : List[float], optional\n",
    "        List of smoothing factors, by default [0.94, 0.97]\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, pd.DataFrame]\n",
    "        Dictionary of DataFrames containing EWMA volatilities for each lambda\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(returns, pd.DataFrame):\n",
    "        raise TypeError(\"returns must be a pandas DataFrame\")\n",
    "    \n",
    "    # Remove Date column if present\n",
    "    if 'Date' in returns.columns:\n",
    "        returns = returns.drop('Date', axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        n_obs, n_assets = returns.shape\n",
    "        ewma_vol = np.zeros((n_obs, n_assets))\n",
    "        \n",
    "        # Initialize first value with sample standard deviation\n",
    "        ewma_vol[0] = returns.std().values  # per-asset std dev as initial values\n",
    "\n",
    "        \n",
    "        # Loop through time to apply EWMA formula\n",
    "        for t in range(1, n_obs):\n",
    "            ewma_vol[t] = np.sqrt(\n",
    "                lambda_ * ewma_vol[t-1]**2 + \n",
    "                (1 - lambda_) * returns.iloc[t-1].values**2\n",
    "            )\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        results[f'lambda_{lambda_}'] = pd.DataFrame(\n",
    "            ewma_vol,\n",
    "            index=returns.index,\n",
    "            columns=returns.columns\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "returns_df = main_df[['Date', 'C_S&P500_Returns', 'C_Dax40_Returns', \n",
    "                      'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "#set Date as index\n",
    "returns_df.set_index('Date', inplace=True)\n",
    "ewma_results = compute_ewma_volatility(returns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a graphs where you plot the actual returns and the simulated returns to compare them\n",
    "def plot_simulated_vs_actual_returns(\n",
    "    actual_returns: pd.Series, \n",
    "    simulated_returns: pd.Series, \n",
    "    title: str = \"Simulated vs Actual Returns\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot simulated returns against actual returns.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    actual_returns : pd.Series\n",
    "        Series of actual returns\n",
    "    simulated_returns : pd.Series\n",
    "        Series of simulated returns\n",
    "    title : str, optional\n",
    "        Title of the plot, by default \"Simulated vs Actual Returns\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual_returns.index, actual_returns, label='Actual Returns', color='blue')\n",
    "    plt.plot(simulated_returns.index, simulated_returns, label='Simulated Returns', color='red', alpha=0.7)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "# Example usage\n",
    "# Simulated returns for S&P500\n",
    "simulated_returns_sp500 = pd.Series(\n",
    "    np.random.normal(loc=0, scale=ewma_results['lambda_0.94']['C_S&P500_Returns'].values, size=len(ewma_results['lambda_0.94'])),\n",
    "    index=ewma_results['lambda_0.94'].index\n",
    ")\n",
    "# Simulated returns for DAX40\n",
    "simulated_returns_dax = pd.Series(\n",
    "    np.random.normal(loc=0, scale=ewma_results['lambda_0.94']['C_Dax40_Returns'].values, size=len(ewma_results['lambda_0.94'])),\n",
    "    index=ewma_results['lambda_0.94'].index\n",
    ")\n",
    "# Simulated returns for NIKKEI\n",
    "simulated_returns_nikkei = pd.Series(\n",
    "    np.random.normal(loc=0, scale=ewma_results['lambda_0.94']['C_Nikkei_Returns'].values, size=len(ewma_results['lambda_0.94'])),\n",
    "    index=ewma_results['lambda_0.94'].index\n",
    ")\n",
    "# Simulated returns for EU Bond\n",
    "simulated_returns_bond = pd.Series(\n",
    "    np.random.normal(loc=0, scale=ewma_results['lambda_0.94']['Interest_Bond_daily_rate'].values, size=len(ewma_results['lambda_0.94'])),\n",
    "    index=ewma_results['lambda_0.94'].index\n",
    ")\n",
    "# Plot actual vs simulated returns for S&P500\n",
    "plot_simulated_vs_actual_returns(\n",
    "    actual_returns=returns_df['C_S&P500_Returns'], \n",
    "    simulated_returns=simulated_returns_sp500, \n",
    "    title=\"S&P500: Simulated vs Actual Returns\"\n",
    ")\n",
    "# Plot actual vs simulated returns for DAX40\n",
    "plot_simulated_vs_actual_returns(\n",
    "    actual_returns=returns_df['C_Dax40_Returns'], \n",
    "    simulated_returns=simulated_returns_dax, \n",
    "    title=\"DAX40: Simulated vs Actual Returns\"\n",
    ")\n",
    "# Plot actual vs simulated returns for NIKKEI\n",
    "plot_simulated_vs_actual_returns(\n",
    "    actual_returns=returns_df['C_Nikkei_Returns'], \n",
    "    simulated_returns=simulated_returns_nikkei, \n",
    "    title=\"NIKKEI: Simulated vs Actual Returns\"\n",
    ")\n",
    "# Plot actual vs simulated returns for EU Bond\n",
    "plot_simulated_vs_actual_returns(\n",
    "    actual_returns=returns_df['Interest_Bond_daily_rate'], \n",
    "    simulated_returns=simulated_returns_bond, \n",
    "    title=\"EU Bond: Simulated vs Actual Returns\"\n",
    ")\n",
    "#plot the actual returns and the simulated returns for all indices in one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_returns(returns: pd.DataFrame, ewma_vol: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Compute standardized (filtered) returns for each lambda value.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of raw returns\n",
    "    - ewma_vol: Dictionary containing DataFrames of EWMA volatilities for each lambda\n",
    "\n",
    "    Returns:\n",
    "    - filtered_returns: Dictionary of DataFrames with standardized returns for each lambda\n",
    "    \"\"\"\n",
    "    filtered_returns = {}\n",
    "    \n",
    "    # Filter returns for each lambda value\n",
    "    for lambda_key, vol_df in ewma_vol.items():\n",
    "        filtered_returns[lambda_key] = returns / vol_df\n",
    "    \n",
    "    return filtered_returns\n",
    "\n",
    "# Assuming returns_df and ewma_vol_df (from compute_ewma_volatility) are already defined\n",
    "filtered_returns_dict = filter_returns(returns_df, ewma_results)\n",
    "\n",
    "# Preview results for both lambda values\n",
    "print(\"\\nFiltered Returns (lambda = 0.94):\")\n",
    "print(filtered_returns_dict['lambda_0.94'].tail())\n",
    "print(\"\\nFiltered Returns (lambda = 0.97):\")\n",
    "print(filtered_returns_dict['lambda_0.97'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a963681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_historical_simulation_multivariate(filtered_returns_dict: dict, ewma_vol_dict: dict,\n",
    "                                              n_simulations: int = 10000, random_seed: int = None, \n",
    "                                              weights: np.ndarray = None) -> dict:\n",
    "    \"\"\"\n",
    "    Perform Filtered Historical Simulation for a multi-asset portfolio for different lambda values.\n",
    "\n",
    "    Parameters:\n",
    "    - filtered_returns_dict: Dictionary of DataFrames of standardized residuals for each lambda\n",
    "    - ewma_vol_dict: Dictionary of DataFrames of EWMA volatility for each lambda\n",
    "    - n_simulations: number of simulated return vectors\n",
    "    - random_seed: for reproducibility\n",
    "    - weights: portfolio weights (numpy array of shape [n_assets])\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing simulated portfolio returns for each lambda value\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process each lambda value\n",
    "    for lambda_key in filtered_returns_dict.keys():\n",
    "        filtered_returns = filtered_returns_dict[lambda_key]\n",
    "        # print(filtered_returns)\n",
    "        ewma_vol = ewma_vol_dict[lambda_key]\n",
    "        \n",
    "        assets = filtered_returns.columns\n",
    "        n_assets = len(assets)\n",
    "        \n",
    "        # Initialize simulated return matrix (n_simulations x n_assets)\n",
    "        sim_returns = np.zeros((n_simulations, n_assets))\n",
    "\n",
    "        for i, asset in enumerate(assets):\n",
    "            if asset == 'Date':\n",
    "                continue\n",
    "            z_asset = filtered_returns[asset].dropna().values\n",
    "            z_star = np.random.choice(z_asset, size=n_simulations, replace=True)\n",
    "            sigma_t = ewma_vol[asset].iloc[-1]  # latest volatility for asset\n",
    "            sim_returns[:, i] = sigma_t * z_star  # re-scale\n",
    "\n",
    "        if weights is not None:\n",
    "            portfolio_simulated_returns = sim_returns @ weights\n",
    "            results[lambda_key] = pd.Series(portfolio_simulated_returns, \n",
    "                                          name=f\"Simulated_Portfolio_Returns_{lambda_key}\")\n",
    "        else:\n",
    "            results[lambda_key] = pd.DataFrame(sim_returns, columns=assets)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define weights in the same order as your DataFrame columns\n",
    "weights = np.array([0.4, 0.3, 0.15, 0.15])  # Example: S&P500, DAX, Nikkei, Bond rate\n",
    "\n",
    "# Run multivariate FHS simulation for both lambda values\n",
    "simulated_returns = filtered_historical_simulation_multivariate(\n",
    "    filtered_returns_dict,\n",
    "    ewma_results,\n",
    "    n_simulations=10000,\n",
    "    random_seed=42,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "# Compute VaR and ES for 95% and 99% for each lambda value\n",
    "confidence_levels = [0.95, 0.99]\n",
    "results = {}\n",
    "\n",
    "for lambda_key, sim_returns in simulated_returns.items():\n",
    "    for cl in confidence_levels:\n",
    "        alpha = 1 - cl\n",
    "        percentile = alpha * 100\n",
    "        var = -np.percentile(sim_returns, percentile)\n",
    "        es = -sim_returns[sim_returns <= -var].mean()\n",
    "        results[f\"{lambda_key}_VaR_{int(cl * 100)}\"] = var\n",
    "        results[f\"{lambda_key}_ES_{int(cl * 100)}\"] = es\n",
    "\n",
    "# Print results\n",
    "for lambda_key in simulated_returns.keys():\n",
    "    print(f\"\\nResults for {lambda_key}:\")\n",
    "    for cl in confidence_levels:\n",
    "        print(f\"Portfolio 1-day VaR ({int(cl * 100)}%): {results[f'{lambda_key}_VaR_{int(cl * 100)}']:.5f}\")\n",
    "        print(f\"Portfolio 1-day ES  ({int(cl * 100)}%): {results[f'{lambda_key}_ES_{int(cl * 100)}']:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a39b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_historical_simulation_multiday(\n",
    "    filtered_returns_dict: dict,\n",
    "    ewma_vol_dict: dict,\n",
    "    lambda_key: str,\n",
    "    n_days: int = 1,\n",
    "    n_simulations: int = 10000,\n",
    "    random_seed: int = None,\n",
    "    weights: np.ndarray = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Simulate N-day portfolio returns using Filtered Historical Simulation.\n",
    "\n",
    "    Returns:\n",
    "    - Simulated N-day portfolio return series (n_simulations,)\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    filtered_returns = filtered_returns_dict[lambda_key]\n",
    "    ewma_vol = ewma_vol_dict[lambda_key]\n",
    "\n",
    "    assets = filtered_returns.columns\n",
    "    n_assets = len(assets)\n",
    "    sim_returns = np.zeros((n_simulations, n_days, n_assets))\n",
    "\n",
    "    for i, asset in enumerate(assets):\n",
    "        z_asset = filtered_returns[asset].dropna().values\n",
    "        sigma_t = ewma_vol[asset].iloc[-1]\n",
    "\n",
    "        if len(z_asset) == 0 or np.isnan(sigma_t):\n",
    "            raise ValueError(f\"Cannot simulate for asset '{asset}': empty or invalid data.\")\n",
    "\n",
    "        for day in range(n_days):\n",
    "            z_star = np.random.choice(z_asset, size=n_simulations, replace=True)\n",
    "            sim_returns[:, day, i] = sigma_t * z_star\n",
    "\n",
    "    # Combine all simulated daily returns into N-day portfolio PnL\n",
    "    total_pnl = (sim_returns @ weights).sum(axis=1)\n",
    "\n",
    "    return pd.Series(total_pnl, name=f\"Simulated_{n_days}Day_Returns_{lambda_key}\")\n",
    "\n",
    "\n",
    "# Example usage with both lambda values\n",
    "confidence_levels = [0.95, 0.99]\n",
    "horizons = [1, 5, 10]\n",
    "lambda_keys = ['lambda_0.94', 'lambda_0.97']\n",
    "\n",
    "for lambda_key in lambda_keys:\n",
    "    print(f\"\\n=== Results for {lambda_key} ===\")\n",
    "    for days in horizons:\n",
    "        print(f\"\\n--- {days}-Day VaR & ES ---\")\n",
    "        sim_returns = filtered_historical_simulation_multiday(\n",
    "            filtered_returns_dict,\n",
    "            ewma_results,\n",
    "            lambda_key,\n",
    "            n_days=days,\n",
    "            n_simulations=10000,\n",
    "            random_seed=42,\n",
    "            weights=weights\n",
    "        )\n",
    "\n",
    "        for cl in confidence_levels:\n",
    "            alpha = 1 - cl\n",
    "            percentile = alpha * 100\n",
    "            var = -np.percentile(sim_returns, percentile)\n",
    "            es = -sim_returns[sim_returns <= -var].mean()\n",
    "            print(f\"VaR ({int(cl*100)}%): {var:.5f} | ES ({int(cl*100)}%): {es:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_fhs_multiday_var_es(\n",
    "    returns_df: pd.DataFrame,\n",
    "    weights: np.ndarray,\n",
    "    window_size: int = 500,\n",
    "    horizons: list = [1],\n",
    "    confidence_levels: list = [0.95, 0.99],\n",
    "    n_simulations: int = 1000,\n",
    "    lambdas: list = [0.94, 0.97],\n",
    "    random_seed: int = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Rolling Filtered Historical Simulation for multi-day VaR & ES.\n",
    "    \"\"\"\n",
    "    time_window = returns_df.loc['2012-01-05':'2021-12-31']\n",
    "\n",
    "    var_results = []\n",
    "    es_results = []\n",
    "\n",
    "    for horizon in horizons:\n",
    "        #adjusted_window_size = int(window_size / horizon)\n",
    "\n",
    "        for t in range(window_size, len(time_window)):\n",
    "            current_date = time_window.index[t]\n",
    "            var_row = {'index': current_date}\n",
    "            es_row = {'index': current_date}\n",
    "\n",
    "            window = time_window.iloc[t - window_size:t]\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Compute EWMA vol\n",
    "                ewma_results = compute_ewma_volatility(\n",
    "                    window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']],\n",
    "                    lambdas\n",
    "                )\n",
    "                # Filtered returns\n",
    "                filtered_returns_dict = {}\n",
    "                for lambda_key, vol_df in ewma_results.items():\n",
    "                    safe_vol_df = vol_df.replace(0, np.nan).ffill()\n",
    "                    filtered_returns = window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']] / safe_vol_df\n",
    "                    filtered_returns_dict[lambda_key] = filtered_returns\n",
    "\n",
    "                # Simulate for each lambda and confidence level\n",
    "                for lambda_key in ewma_results.keys():\n",
    "                    sim_returns = filtered_historical_simulation_multiday(\n",
    "                        filtered_returns_dict,\n",
    "                        ewma_results,\n",
    "                        lambda_key,\n",
    "                        n_days=horizon,\n",
    "                        n_simulations=n_simulations,\n",
    "                        weights=weights,\n",
    "                        random_seed=random_seed\n",
    "                    )\n",
    "\n",
    "                    for cl in confidence_levels:\n",
    "                        alpha = 1 - cl\n",
    "                        var = np.percentile(sim_returns, 100 * alpha)\n",
    "                        es = sim_returns[sim_returns <= var].mean()\n",
    "\n",
    "                        var_key = f\"VaR_{int(cl * 100)}_{lambda_key}_h{horizon}\"\n",
    "                        es_key = f\"ES_{int(cl * 100)}_{lambda_key}_h{horizon}\"\n",
    "\n",
    "                        var_row[var_key] = var\n",
    "                        es_row[es_key] = es\n",
    "\n",
    "                var_results.append(var_row)\n",
    "                es_results.append(es_row)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    var_df = pd.DataFrame(var_results).set_index('index')\n",
    "    es_df = pd.DataFrame(es_results).set_index('index')\n",
    "\n",
    "    var_df.index.name = 'Date'\n",
    "    es_df.index.name = 'Date'\n",
    "\n",
    "    return var_df, es_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10885e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var_es_vs_actual_given_actuals(var_df, es_df, actual_returns_dict, lambdas, horizons, confidence_levels):\n",
    "    \"\"\"\n",
    "    Plot VaR and ES against already computed actual portfolio returns and mark violations.\n",
    "\n",
    "    Parameters:\n",
    "    - var_df: DataFrame of rolling VaR values\n",
    "    - es_df: DataFrame of rolling ES values\n",
    "    - actual_returns_dict: Dict of actual portfolio return Series keyed by horizon\n",
    "    - lambdas: list of lambda values used (e.g. [0.94, 0.97])\n",
    "    - horizons: list of horizon days (e.g. [1, 5, 10])\n",
    "    - confidence_levels: list of confidence levels (e.g. [0.95, 0.99])\n",
    "    \"\"\"\n",
    "\n",
    "    for h in horizons:\n",
    "        actual_returns = actual_returns_dict[h].reindex(var_df.index).sort_index()\n",
    "        actual_returns = actual_returns[~actual_returns.index.duplicated(keep='first')]  \n",
    "\n",
    "        for lambda_ in lambdas:\n",
    "            lambda_key = f\"lambda_{lambda_}\"\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            plt.plot(actual_returns, label=\"Actual Portfolio Return\", alpha=0.6)\n",
    "\n",
    "            title_addon = \"\"\n",
    "\n",
    "            for cl in confidence_levels:\n",
    "                var_col = f\"VaR_{int(cl * 100)}_{lambda_key}_h{h}\"\n",
    "                var_series = var_df[var_col]\n",
    "                plt.plot(var_series, label=f\"VaR {int(cl * 100)}%\", linestyle='--')\n",
    "\n",
    "                # Find violations\n",
    "                actual_aligned, var_aligned = actual_returns.align(var_series, join='inner')\n",
    "                violations = actual_aligned < var_aligned\n",
    "                violation_points = actual_aligned[violations]\n",
    "\n",
    "\n",
    "                # Mark violations with different styles\n",
    "                if cl == 0.95:\n",
    "                    plt.scatter(violation_points.index, violation_points, \n",
    "                                color='red', label=\"VaR 95% Violation\", marker='x', s=50)\n",
    "                elif cl == 0.99:\n",
    "                    plt.scatter(violation_points.index, violation_points, \n",
    "                                color='purple', label=\"VaR 99% Violation\", marker='v', s=60)\n",
    "\n",
    "                # Count violations and append to title\n",
    "                total_obs = actual_returns.dropna().shape[0]\n",
    "                violation_count = len(violation_points)\n",
    "                percentage = violation_count / total_obs * 100\n",
    "                title_addon += f\" | {int(cl*100)}% Viol: {violation_count} ({percentage:.2f}%)\"\n",
    "\n",
    "            plt.title(f\"{h}-Day VaR and ES vs Actual Returns | λ={lambda_}{title_addon}\")\n",
    "            plt.axhline(0, color='gray', linestyle='-')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actual_portfolio_returns(returns_df, weights, horizons=[1, 5, 10]):\n",
    "    actual_returns = {}\n",
    "    weighted_returns = returns_df @ weights\n",
    "\n",
    "    for h in horizons:\n",
    "        actual_returns[h] = weighted_returns.rolling(window=h).sum().shift(-h + 1)\n",
    "        actual_returns[h].name = f\"Actual_{h}d\"\n",
    "\n",
    "    return actual_returns\n",
    "\n",
    "actual_returns_dict = compute_actual_portfolio_returns(returns_df, weights)\n",
    "returns_dfe = pd.DataFrame(actual_returns_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b52406",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Backtesting VaR and ES\n",
    "\n",
    "In this section, we perform backtesting on the calculated Value at Risk (VaR) and Expected Shortfall (ES) measures. Backtesting helps assess the accuracy and reliability of the risk models.\n",
    "\n",
    "We will:\n",
    "1.  **Calculate Violations:** Identify the days where the actual portfolio loss exceeded the predicted VaR.\n",
    "2.  **Compare Actual vs. Expected Violations (VaR):** Group violations by year and compare the observed number of violations against the number expected based on the confidence level (alpha).\n",
    "3.  **Compare Actual Shortfall vs. Predicted ES (ES):** For the days a violation occurred, compare the average actual loss (shortfall) against the predicted ES, grouped by year.\n",
    "4.  **Visualize Violations:** Plot the occurrences of violations over time to visually inspect for clustering or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e41335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_violations(negative_actual_returns, var_predictions):\n",
    "    \"\"\"Checks for VaR violations. Violation occurs if negative return is less than negative VaR.\"\"\"\n",
    "    # var_predictions are typically positive loss values, so compare negative return to -VaR\n",
    "    return negative_actual_returns < -var_predictions\n",
    "\n",
    "def backtest_var(violations, alpha, dates):\n",
    "    \"\"\"Compares actual vs. expected VaR violations yearly.\"\"\"\n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date': # Ensure index is Date for grouping\n",
    "         violations = violations.set_index(dates)\n",
    "            \n",
    "    violations_df = pd.DataFrame({'Violations': violations, 'Year': violations.index.year})\n",
    "    yearly_violations = violations_df.groupby('Year')['Violations'].sum()\n",
    "    yearly_counts = violations_df.groupby('Year')['Violations'].count()\n",
    "    \n",
    "    # Expected violations based on the confidence level (1 - alpha)\n",
    "    expected_violations = yearly_counts * (1 - alpha)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Actual Violations': yearly_violations,\n",
    "        'Expected Violations': expected_violations,\n",
    "        'Total Observations': yearly_counts\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def backtest_es(negative_actual_returns, violations, es_predictions, dates):\n",
    "    \"\"\"Compares actual average shortfall vs. predicted ES yearly.\"\"\"\n",
    "    # Ensure inputs are pandas Series with Date index\n",
    "    if not isinstance(negative_actual_returns, pd.Series):\n",
    "        negative_actual_returns = pd.Series(negative_actual_returns, index=dates)\n",
    "    elif negative_actual_returns.index.name != 'Date':\n",
    "        negative_actual_returns = negative_actual_returns.set_index(dates)\n",
    "        \n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date':\n",
    "        violations = violations.set_index(dates)\n",
    "        \n",
    "    if not isinstance(es_predictions, pd.Series):\n",
    "        es_predictions = pd.Series(es_predictions, index=dates)\n",
    "    elif es_predictions.index.name != 'Date':\n",
    "        es_predictions = es_predictions.set_index(dates)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Negative_Return': negative_actual_returns,\n",
    "        'Violation': violations,\n",
    "        'Predicted_ES': es_predictions, # Assumed positive loss value\n",
    "        'Year': negative_actual_returns.index.year\n",
    "    })\n",
    "    \n",
    "    # Filter for violations\n",
    "    violation_data = results_df[results_df['Violation']]\n",
    "    \n",
    "    # Calculate yearly averages\n",
    "    # Actual shortfall is the loss on violation days, which is -Negative_Return\n",
    "    yearly_avg_actual_shortfall = violation_data.groupby('Year')['Negative_Return'].apply(lambda x: -x.mean())\n",
    "    yearly_avg_predicted_es = violation_data.groupby('Year')['Predicted_ES'].mean()\n",
    "    yearly_violation_count = violation_data.groupby('Year').size()\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Avg Actual Shortfall': yearly_avg_actual_shortfall,\n",
    "        'Avg Predicted ES': yearly_avg_predicted_es,\n",
    "        'Violation Count': yearly_violation_count\n",
    "    })\n",
    "    # Handle years with no violations (results in NaN means)\n",
    "    summary = summary.fillna({'Avg Actual Shortfall': 0, 'Avg Predicted ES': 0})\n",
    "    return summary\n",
    "\n",
    "def calculate_violation_spacing(violations, dates):\n",
    "    \"\"\"Calculate the spacing between consecutive violations.\"\"\"\n",
    "    # Ensure violations and dates are aligned\n",
    "    violation_dates = pd.Series(dates[violations])\n",
    "    \n",
    "    if len(violation_dates) <= 1:\n",
    "        return []  # Not enough violations to calculate spacing\n",
    "        \n",
    "    # Calculate days between consecutive violations\n",
    "    spacing = [(violation_dates.iloc[i] - violation_dates.iloc[i-1]).days \n",
    "               for i in range(1, len(violation_dates))]\n",
    "    \n",
    "    return spacing\n",
    "\n",
    "def plot_violation_spacing_qq(violations_dict, model_name, alpha):\n",
    "    \"\"\"Create QQ plot for the spacing between violations.\"\"\"\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for method, (violations, dates) in violations_dict.items():\n",
    "        # Get spacing between violations\n",
    "        spacing = calculate_violation_spacing(violations, dates)\n",
    "        \n",
    "        if not spacing:\n",
    "            continue  # Skip if not enough violations\n",
    "            \n",
    "        # Calculate theoretical quantiles for exponential distribution\n",
    "        # For independent violations at rate (1-alpha), spacing should follow Exp(1-alpha)\n",
    "        n = len(spacing)\n",
    "        p = np.arange(1, n + 1) / (n + 1)  # Empirical probabilities\n",
    "        theoretical_quantiles = -np.log(1 - p) / (1 - alpha)  # Exponential quantiles\n",
    "        \n",
    "        # Sort the observed spacings\n",
    "        observed_quantiles = sorted(spacing)\n",
    "        \n",
    "        # Plot the QQ plot\n",
    "        plt.scatter(theoretical_quantiles, observed_quantiles, \n",
    "                   label=f\"{method}\", alpha=0.7)\n",
    "    \n",
    "    # Add reference line (y=x)\n",
    "    max_val = max([max(theoretical_quantiles) for theoretical_quantiles, _ \n",
    "                  in [(theoretical_quantiles, spacing) \n",
    "                     for _, (violations, dates) in violations_dict.items() \n",
    "                     if calculate_violation_spacing(violations, dates)]], \n",
    "                 default=10)\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--', label='Expected')\n",
    "    \n",
    "    plt.title(f'QQ Plot of VaR Violation Spacing - {model_name} (alpha={alpha})')\n",
    "    plt.xlabel('Theoretical Quantiles (Exponential Distribution)')\n",
    "    plt.ylabel('Observed Spacing (Days)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_backtesting(main_df, var_results_df, es_results_df):\n",
    "    \"\"\"Runs the backtesting process for VaR and ES models and creates consolidated tables.\"\"\"\n",
    "    # Align main_df with var/es results (which start after the initial window)\n",
    "    if 'Date' not in var_results_df.columns or 'Date' not in es_results_df.columns or 'Date' not in main_df.columns:\n",
    "        raise ValueError(\"All input DataFrames must contain a 'Date' column.\")\n",
    "        \n",
    "    # Use merge to ensure alignment and handle potential missing dates\n",
    "    backtest_base = pd.merge(var_results_df[['Date']], main_df[['Date', 'Portfolio_loss']], on='Date', how='left')\n",
    "    backtest_base = backtest_base.set_index('Date')\n",
    "    \n",
    "    # Calculate negative actual returns\n",
    "    negative_actual_returns = -backtest_base['Portfolio_loss']\n",
    "    backtest_dates = negative_actual_returns.index # Use index from aligned data\n",
    "\n",
    "    # Confidence levels used (assuming 95% and 99%)\n",
    "    alphas = [0.95, 0.99]\n",
    "    alpha_indices = {0.95: 0, 0.99: 1} # Index mapping for results arrays\n",
    "\n",
    "    # Ensure Date columns are handled correctly\n",
    "    var_model_cols = [col for col in var_results_df.columns if col != 'Date']\n",
    "    \n",
    "    # Align var_results_df and es_results_df to the backtest_dates\n",
    "    var_results_aligned = var_results_df.set_index('Date').reindex(backtest_dates)\n",
    "    es_results_aligned = es_results_df.set_index('Date').reindex(backtest_dates)\n",
    "\n",
    "    # Create consolidated tables for each alpha level\n",
    "    for alpha in alphas:\n",
    "        alpha_idx = alpha_indices[alpha]\n",
    "        \n",
    "        # Create dictionaries to store violations by year for each method\n",
    "        yearly_violations = {}\n",
    "        violations_for_qq = {}\n",
    "        \n",
    "        # Get all unique years in the dataset\n",
    "        all_years = pd.Series(backtest_dates).dt.year.unique()\n",
    "        \n",
    "        print(f\"\\n--- Backtesting Results for Confidence Level: {alpha*100}% ---\\n\")\n",
    "        \n",
    "        # Process each model\n",
    "        for model_name in var_model_cols:\n",
    "            try:\n",
    "                # Extract predictions for this model at this alpha level\n",
    "                var_preds_list = var_results_aligned[model_name].tolist()\n",
    "                var_predictions = pd.Series([p[alpha_idx] if isinstance(p, (list, np.ndarray)) and len(p) > alpha_idx \n",
    "                                           else np.nan for p in var_preds_list], index=backtest_dates)\n",
    "                \n",
    "                # Drop NaN predictions and align actual returns\n",
    "                valid_idx = var_predictions.notna()\n",
    "                var_predictions_clean = var_predictions[valid_idx]\n",
    "                negative_actual_returns_clean = negative_actual_returns[valid_idx]\n",
    "                backtest_dates_clean = backtest_dates[valid_idx]\n",
    "                \n",
    "                if negative_actual_returns_clean.empty:\n",
    "                    print(f\"  Warning: No valid data points for {model_name}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate violations\n",
    "                violations = calculate_violations(negative_actual_returns_clean, var_predictions_clean)\n",
    "                \n",
    "                # Get VaR backtest summary by year\n",
    "                var_summary = backtest_var(violations, alpha, backtest_dates_clean)\n",
    "                \n",
    "                # Store violations by year for this method\n",
    "                yearly_violations[model_name] = var_summary['Actual Violations']\n",
    "                \n",
    "                # Store violations and dates for QQ plot\n",
    "                violations_for_qq[model_name] = (violations.values, backtest_dates_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {model_name} for alpha={alpha}: {e}\")\n",
    "        \n",
    "        # Create a consolidated dataframe for all models\n",
    "        if yearly_violations:\n",
    "            # Create a DataFrame with all years as index\n",
    "            violations_df = pd.DataFrame(index=all_years)\n",
    "            \n",
    "            # Add violations for each method\n",
    "            for model_name, violations in yearly_violations.items():\n",
    "                violations_df[model_name] = violations\n",
    "                \n",
    "            # Add expected violations column (same for all methods at given alpha)\n",
    "            yearly_counts = backtest_base.groupby(backtest_base.index.year)['Portfolio_loss'].count()\n",
    "            violations_df['Expected Violations'] = yearly_counts * (1 - alpha)\n",
    "            \n",
    "            # Fill NaN with 0 (years with no violations)\n",
    "            violations_df = violations_df.fillna(0)\n",
    "            \n",
    "            # Display the consolidated table\n",
    "            print(f\"\\nAnnual VaR Violations Table ({alpha*100}% Confidence Level):\")\n",
    "            display(violations_df)\n",
    "            \n",
    "            # Create QQ plot for violation spacing\n",
    "            plot_violation_spacing_qq(violations_for_qq, \"All Models\", alpha)\n",
    "        else:\n",
    "            print(f\"No valid violation data available for alpha={alpha}\")\n",
    "\n",
    "    return None  # No return value needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9487ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio daily returns if not already present\n",
    "if 'Portfolio_Daily_Returns' not in main_df.columns:\n",
    "    # Calculate the daily returns as percentage change in portfolio value\n",
    "    main_df['Portfolio_Daily_Returns'] = main_df['Portfolio_Value_EUR'].pct_change()\n",
    "\n",
    "# Extract portfolio returns\n",
    "portfolio_returns = main_df['Portfolio_Daily_Returns'].dropna()\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "gs = GridSpec(2, 3, figure=fig)\n",
    "\n",
    "# Create QQ plot for normal distribution\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "res1 = st.probplot(portfolio_returns, dist='norm', plot=ax1)\n",
    "ax1.set_title('QQ Plot: Portfolio Returns vs. Normal Distribution')\n",
    "r_squared_normal = np.corrcoef(res1[0][0], res1[0][1])[0, 1]**2\n",
    "ax1.text(0.05, 0.95, f'R² = {r_squared_normal:.4f}', transform=ax1.transAxes,\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Create QQ plots for Student's t with 3, 4, 5, 6 degrees of freedom\n",
    "dfs = [3, 4, 5, 6]\n",
    "r_squared_values = {}\n",
    "\n",
    "# Function to create a t distribution with a specified df\n",
    "def t_dist_generator(df):\n",
    "    # Create a frozen t distribution with the given df\n",
    "    frozen_t = st.t(df=df)\n",
    "    return frozen_t\n",
    "\n",
    "# Create QQ plot for t distribution with df=3\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "t3 = t_dist_generator(dfs[0])\n",
    "res2 = st.probplot(portfolio_returns, dist=t3, plot=ax2)\n",
    "ax2.set_title(f'QQ Plot: Portfolio Returns vs. Student t (df={dfs[0]})')\n",
    "r_squared_t3 = np.corrcoef(res2[0][0], res2[0][1])[0, 1]**2\n",
    "r_squared_values[dfs[0]] = r_squared_t3\n",
    "ax2.text(0.05, 0.95, f'R² = {r_squared_t3:.4f}', transform=ax2.transAxes,\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Create QQ plot for t distribution with df=4\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "t4 = t_dist_generator(dfs[1])\n",
    "res3 = st.probplot(portfolio_returns, dist=t4, plot=ax3)\n",
    "ax3.set_title(f'QQ Plot: Portfolio Returns vs. Student t (df={dfs[1]})')\n",
    "r_squared_t4 = np.corrcoef(res3[0][0], res3[0][1])[0, 1]**2\n",
    "r_squared_values[dfs[1]] = r_squared_t4\n",
    "ax3.text(0.05, 0.95, f'R² = {r_squared_t4:.4f}', transform=ax3.transAxes,\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Create QQ plot for t distribution with df=5\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "t5 = t_dist_generator(dfs[2])\n",
    "res4 = st.probplot(portfolio_returns, dist=t5, plot=ax4)\n",
    "ax4.set_title(f'QQ Plot: Portfolio Returns vs. Student t (df={dfs[2]})')\n",
    "r_squared_t5 = np.corrcoef(res4[0][0], res4[0][1])[0, 1]**2\n",
    "r_squared_values[dfs[2]] = r_squared_t5\n",
    "ax4.text(0.05, 0.95, f'R² = {r_squared_t5:.4f}', transform=ax4.transAxes,\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Create QQ plot for t distribution with df=6\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "t6 = t_dist_generator(dfs[3])\n",
    "res5 = st.probplot(portfolio_returns, dist=t6, plot=ax5)\n",
    "ax5.set_title(f'QQ Plot: Portfolio Returns vs. Student t (df={dfs[3]})')\n",
    "r_squared_t6 = np.corrcoef(res5[0][0], res5[0][1])[0, 1]**2\n",
    "r_squared_values[dfs[3]] = r_squared_t6\n",
    "ax5.text(0.05, 0.95, f'R² = {r_squared_t6:.4f}', transform=ax5.transAxes,\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Create a summary subplot\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.axis('off')\n",
    "ax6.text(0.1, 0.9, 'R² Correlation Summary:', fontsize=14, fontweight='bold')\n",
    "ax6.text(0.1, 0.8, f'Normal Distribution: {r_squared_normal:.6f}')\n",
    "ax6.text(0.1, 0.7, f'Student t (df=3): {r_squared_t3:.6f}')\n",
    "ax6.text(0.1, 0.6, f'Student t (df=4): {r_squared_t4:.6f}')\n",
    "ax6.text(0.1, 0.5, f'Student t (df=5): {r_squared_t5:.6f}')\n",
    "ax6.text(0.1, 0.4, f'Student t (df=6): {r_squared_t6:.6f}')\n",
    "\n",
    "# Find the best fit\n",
    "best_fit = max(r_squared_values.items(), key=lambda x: x[1])\n",
    "best_fit_text = f'Best fit: Student t with df={best_fit[0]} (R²={best_fit[1]:.6f})'\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5067a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize lists to store results\n",
    "    VaR_results = []\n",
    "    ES_results = []\n",
    "    # Define time window\n",
    "    time_window = main_df[(main_df['Date'] >= '2012-01-05') & (main_df['Date'] <= '2021-12-31')]\n",
    "\n",
    "    # Define confidence levels\n",
    "    vAlpha = np.array([0.95, 0.99])\n",
    "    \n",
    "    # Define sample size and t-distribution degrees of freedom\n",
    "    sample_size = 500\n",
    "    degrees_of_freedom = [0, 3, 4, 5, 6]  # 0 represents normal distribution\n",
    "    \n",
    "    for i in range(sample_size, len(time_window)):\n",
    "        # Extract the rolling window\n",
    "        window = time_window.iloc[i - sample_size:i]\n",
    "        current_date = time_window.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate loss statistics\n",
    "        loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "        mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "        portfolio_std_loss = loss_stats[\"Portfolio_std_loss\"]\n",
    "        \n",
    "        # Initialize result dictionaries for this date\n",
    "        var_row = {'Date': current_date}\n",
    "        es_row = {'Date': current_date}\n",
    "\n",
    "        # Calculate VaR and ES using various distributions\n",
    "        for df in degrees_of_freedom:\n",
    "            results = calculate_var_cov(current_date, vAlpha, mean_loss, portfolio_std_loss, df)\n",
    "            # Get the distribution label\n",
    "            if df == 0:\n",
    "                dist_label = \"Normal\"\n",
    "            else:\n",
    "                dist_label = f\"T{df}\"\n",
    "            # Add results to the dictionaries\n",
    "            var_row[f'VaR {dist_label}'] = results[f'VaR {dist_label}']\n",
    "            es_row[f'ES {dist_label}'] = results[f'ES {dist_label}']\n",
    "        \n",
    "        # Calculate VaR and ES using historical simulation\n",
    "        hist_results = calculate_historical_var_es(window, current_date, vAlpha)\n",
    "        var_row['VaR Historical'] = hist_results['VaR Historical']\n",
    "        es_row['ES Historical'] = hist_results['ES Historical']\n",
    "        \n",
    "        # Add results for this date\n",
    "        VaR_results.append(var_row)\n",
    "        ES_results.append(es_row)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    var_results_df = pd.DataFrame(VaR_results)\n",
    "    es_results_df = pd.DataFrame(ES_results)\n",
    "\n",
    "    # Example usage\n",
    "    returns_df = main_df[['Date', 'C_S&P500_Returns', 'C_Dax40_Returns', \n",
    "                        'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "    #set Date as index\n",
    "    returns_df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Calculate VaR and ES using rolling FHS\n",
    "    var_df, es_df = rolling_fhs_multiday_var_es(\n",
    "        returns_df=returns_df,  # DataFrame containing returns\n",
    "        weights=weights,        # Portfolio weights\n",
    "        window_size=500,       # Base window size\n",
    "        horizons=[1],   # Horizons for VaR calculation\n",
    "        confidence_levels=[0.95, 0.99],  # Confidence levels\n",
    "        n_simulations=1000,    # Number of simulations\n",
    "        lambdas=[0.94, 0.97]   # EWMA lambda values\n",
    "    )\n",
    "\n",
    "    # Align indices for merging\n",
    "    var_results_df = var_results_df.copy()\n",
    "    es_results_df = es_results_df.copy()\n",
    "\n",
    "    # Ensure 'Date' is datetime and set as index for alignment\n",
    "    var_results_df['Date'] = pd.to_datetime(var_results_df['Date'])\n",
    "    es_results_df['Date'] = pd.to_datetime(es_results_df['Date'])\n",
    "    var_results_df.set_index('Date', inplace=True)\n",
    "    es_results_df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Add FHS VaR and ES columns as lists [95, 99] for each lambda\n",
    "    for lambda_ in [0.94, 0.97]:\n",
    "        var_col_95 = f'VaR_95_lambda_{lambda_}_h1'\n",
    "        var_col_99 = f'VaR_99_lambda_{lambda_}_h1'\n",
    "        es_col_95 = f'ES_95_lambda_{lambda_}_h1'\n",
    "        es_col_99 = f'ES_99_lambda_{lambda_}_h1'\n",
    "\n",
    "        # compute monetary values\n",
    "        var_df_corrected, es_df_corrected = calculate_monetary_values(var_df, es_df, main_df)\n",
    "\n",
    "        # Align var_df/es_df to var_results_df by Date\n",
    "        aligned_var = var_df_corrected.loc[var_results_df.index, [var_col_95, var_col_99]]\n",
    "        aligned_es = es_df_corrected.loc[es_results_df.index, [es_col_95, es_col_99]]\n",
    "\n",
    "        # Combine [95,99] into a list for each row\n",
    "        var_results_df[f'VaR FHS λ={lambda_}'] = aligned_var.values.tolist()\n",
    "        es_results_df[f'ES FHS λ={lambda_}'] = aligned_es.values.tolist()\n",
    "\n",
    "    # Reset index to restore 'Date' as a column\n",
    "    var_results_df.reset_index(inplace=True)\n",
    "    es_results_df.reset_index(inplace=True)\n",
    "\n",
    "    # Insert the GARCH values into var_results_df and es_results_df using the main_analysis function\n",
    "    garch_results_df = main_analysis(500, vAlpha=[0.95, 0.99])\n",
    "\n",
    "    # Ensure 'Date' is datetime for alignment\n",
    "    garch_results_df['Date'] = pd.to_datetime(garch_results_df['Date'])\n",
    "\n",
    "    # convert to monetary values by multiplying by the portfolio value\n",
    "    # portfolio value is Portfolio_Value_EUR in main_df, each row for Portfolio VaR and ES contains a list of [95, 99] which is the VaR and ES for each confidence level\n",
    "    # it is converted by multiplying the portfolio value by the VaR and ES values\n",
    "    portfolio_value = main_df['Portfolio_Value_EUR'].iloc[-len(garch_results_df):].values\n",
    "    # Multiply each VaR/ES value by the corresponding portfolio value for each row\n",
    "    garch_results_df['Portfolio VaR'] = [\n",
    "        [v * pv * -1 for v in var_list] for var_list, pv in zip(garch_results_df['Portfolio VaR'], portfolio_value)\n",
    "    ]\n",
    "    garch_results_df['Portfolio ES'] = [\n",
    "        [v * pv * -1 for v in es_list] for es_list, pv in zip(garch_results_df['Portfolio ES'], portfolio_value)\n",
    "    ]\n",
    "\n",
    "    # Align GARCH results to var_results_df by Date\n",
    "    aligned_garch = garch_results_df.set_index('Date').reindex(var_results_df['Date'])\n",
    "\n",
    "    # Add GARCH VaR and ES columns as lists [95, 99] for each row\n",
    "    var_results_df['VaR GARCH'] = aligned_garch['Portfolio VaR'].tolist()\n",
    "    es_results_df['ES GARCH'] = aligned_garch['Portfolio ES'].tolist()\n",
    "\n",
    "    var_5d = calculate_multiday_var(vAlpha, 5, sample_size)\n",
    "    var_10d = calculate_multiday_var(vAlpha, 10, sample_size)\n",
    "\n",
    "    return var_results_df, es_results_df, var_5d, var_10d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_results_df, es_results_df, var_5d, var_10d = main()\n",
    "print(\"VaR results\")\n",
    "display(var_results_df.head())\n",
    "print(\"ES results\")\n",
    "display(es_results_df.head())\n",
    "print(\"5-day VaR results\")\n",
    "display(var_5d.head())\n",
    "print(\"10-day VaR results\")\n",
    "display(var_10d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform backtesting\n",
    "run_backtesting(main_df, var_results_df, es_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcbd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot VaR (and ES if available) vs actual returns with violations\n",
    "def plot_var_with_violations(var_type, var_data_column, es_data_column, var_results_df, es_results_df, main_df, title_prefix, window='1d'):\n",
    "    \"\"\"\n",
    "    Plots VaR (and ES if available) vs actual returns with violations.\n",
    "    For multi-day VaR (e.g., 5d, 10d), ES is not plotted.\n",
    "    - window: '1d', '5d', or '10d'\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Align dataframes by Date index\n",
    "    var_df_indexed = var_results_df.set_index('Date')\n",
    "    es_df_indexed = es_results_df.set_index('Date')\n",
    "    main_df_indexed = main_df.set_index('Date')\n",
    "    \n",
    "    # Use intersection of indices to ensure alignment\n",
    "    common_index = var_df_indexed.index.intersection(main_df_indexed.index)\n",
    "    if es_data_column in es_df_indexed.columns:\n",
    "        common_index = common_index.intersection(es_df_indexed.index)\n",
    "        \n",
    "    var_aligned = var_df_indexed.loc[common_index]\n",
    "    es_aligned = es_df_indexed.loc[common_index] if es_data_column in es_df_indexed.columns else None\n",
    "    main_aligned = main_df_indexed.loc[common_index]\n",
    "    \n",
    "    # Extract the values for different confidence levels from the arrays/lists\n",
    "    def safe_extract(series, index):\n",
    "        try:\n",
    "            return series.apply(lambda x: x[index] if isinstance(x, (list, np.ndarray)) and len(x) > index else np.nan)\n",
    "        except Exception:\n",
    "            return pd.Series(np.nan, index=series.index)\n",
    "            \n",
    "    var_95 = safe_extract(var_aligned[var_data_column], 0)\n",
    "    var_99 = safe_extract(var_aligned[var_data_column], 1)\n",
    "    es_95 = safe_extract(es_aligned[es_data_column], 0) if es_aligned is not None else pd.Series(np.nan, index=common_index)\n",
    "    es_99 = safe_extract(es_aligned[es_data_column], 1) if es_aligned is not None else pd.Series(np.nan, index=common_index)\n",
    "    \n",
    "    # Use negative portfolio loss as actual returns for comparison\n",
    "    actual_returns = -main_aligned['Portfolio_loss']\n",
    "    \n",
    "    # Combine into a single DataFrame for easier plotting and violation calculation\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Actual_Returns': actual_returns,\n",
    "        'VaR_95': var_95,\n",
    "        'VaR_99': var_99,\n",
    "        'ES_95': es_95,\n",
    "        'ES_99': es_99\n",
    "    }).dropna(subset=['Actual_Returns', 'VaR_95', 'VaR_99']) # Drop rows where essential data is missing\n",
    "\n",
    "    # Calculate violations for VaR (Actual Return < -VaR)\n",
    "    violations_95 = plot_df['Actual_Returns'] < -plot_df['VaR_95']\n",
    "    violations_99 = plot_df['Actual_Returns'] < -plot_df['VaR_99']\n",
    "\n",
    "    # Calculate violations for ES (Actual Return < -ES)\n",
    "    es_violations_95 = plot_df['Actual_Returns'] < -plot_df['ES_95']\n",
    "    es_violations_99 = plot_df['Actual_Returns'] < -plot_df['ES_99']\n",
    "\n",
    "    violation_pct_95 = violations_95.mean() * 100 if not plot_df.empty else 0\n",
    "    violation_pct_99 = violations_99.mean() * 100 if not plot_df.empty else 0\n",
    "    es_violation_pct_95 = es_violations_95.mean() * 100 if not plot_df.empty else 0\n",
    "    es_violation_pct_99 = es_violations_99.mean() * 100 if not plot_df.empty else 0\n",
    "\n",
    "    plt.plot(plot_df.index, plot_df['Actual_Returns'], label='Actual Portfolio Returns', color='blue', alpha=0.6)\n",
    "    plt.plot(plot_df.index, -plot_df['VaR_95'], \n",
    "                label=f'VaR {var_type} 95% ({violation_pct_95:.2f}% violations)', \n",
    "                color='red', linestyle='--', linewidth=1.5)\n",
    "    plt.plot(plot_df.index, -plot_df['VaR_99'], \n",
    "                label=f'VaR {var_type} 99% ({violation_pct_99:.2f}% violations)', \n",
    "                color='darkred', linestyle='-.', linewidth=1.5)\n",
    "    \n",
    "    # Only plot ES if data is available\n",
    "    if not plot_df['ES_95'].isna().all():\n",
    "        plt.plot(plot_df.index, -plot_df['ES_95'], \n",
    "                    label=f'ES {var_type} 95% ({es_violation_pct_95:.2f}% violations)', \n",
    "                    color='orange', linestyle='--', linewidth=1.5)\n",
    "    if not plot_df['ES_99'].isna().all():\n",
    "        plt.plot(plot_df.index, -plot_df['ES_99'], \n",
    "                    label=f'ES {var_type} 99% ({es_violation_pct_99:.2f}% violations)', \n",
    "                    color='purple', linestyle='-.', linewidth=1.5)\n",
    "\n",
    "    # Get violation points for scattering\n",
    "    violation_points_95 = plot_df[violations_95]\n",
    "    violation_points_99 = plot_df[violations_99]\n",
    "    es_violation_points_95 = plot_df[es_violations_95]\n",
    "    es_violation_points_99 = plot_df[es_violations_99]\n",
    "\n",
    "    plt.scatter(violation_points_95.index, violation_points_95['Actual_Returns'], \n",
    "                color='red', marker='o', s=50, label='95% VaR Violation', zorder=5)\n",
    "    plt.scatter(violation_points_99.index, violation_points_99['Actual_Returns'], \n",
    "                color='darkred', marker='x', s=80, label='99% VaR Violation', zorder=5)\n",
    "    if not plot_df['ES_95'].isna().all():\n",
    "        plt.scatter(es_violation_points_95.index, es_violation_points_95['Actual_Returns'],\n",
    "                    color='orange', marker='o', s=50, label='95% ES Violation', zorder=5)\n",
    "    if not plot_df['ES_99'].isna().all():\n",
    "        plt.scatter(es_violation_points_99.index, es_violation_points_99['Actual_Returns'],\n",
    "                    color='purple', marker='x', s=80, label='99% ES Violation', zorder=5)\n",
    "\n",
    "    plt.title(f'{title_prefix} and ES ({var_type} Distribution) vs Actual Portfolio Returns')\n",
    "    print(f\"95% VaR {var_type}: {violations_95.sum()} violations out of {len(plot_df)} days ({violation_pct_95:.2f}%)\")\n",
    "    print(f\"99% VaR {var_type}: {violations_99.sum()} violations out of {len(plot_df)} days ({violation_pct_99:.2f}%)\")\n",
    "    if not plot_df['ES_95'].isna().all():\n",
    "        print(f\"95% ES {var_type}: {es_violations_95.sum()} violations out of {len(plot_df)} days ({es_violation_pct_95:.2f}%)\")\n",
    "    if not plot_df['ES_99'].isna().all():\n",
    "        print(f\"99% ES {var_type}: {es_violations_99.sum()} violations out of {len(plot_df)} days ({es_violation_pct_99:.2f}%)\")\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_var_multiday_methods(var_df, main_df, horizon, confidence_levels=[0.95, 0.99]):\n",
    "    \"\"\"\n",
    "    Plot multi-day VaR (5d or 10d) for both 'reg' and 'sqrt' methods vs actual portfolio returns.\n",
    "    No violations are assessed or plotted.\n",
    "\n",
    "    Parameters:\n",
    "    - var_df: DataFrame with VaR columns (e.g., VaR_5d_reg, VaR_5d_sqrt, VaR_10d_reg, VaR_10d_sqrt)\n",
    "    - main_df: DataFrame with actual portfolio returns/losses and Date column\n",
    "    - horizon: int, 5 or 10 (number of days)\n",
    "    - confidence_levels: list of confidence levels (default [0.95, 0.99])\n",
    "    \"\"\"\n",
    "    methods = ['reg', 'sqrt']\n",
    "    colors = {'reg': ['red', 'darkred'], 'sqrt': ['orange', 'purple']}\n",
    "    linestyles = {'reg': '--', 'sqrt': '-.'}\n",
    "    labels = {'reg': 'Historical', 'sqrt': 'Sqrt Rule'}\n",
    "\n",
    "    # Align actual returns from main_df\n",
    "    # Use negative portfolio loss for consistency\n",
    "    actual_returns = -main_df.set_index('Date')['Portfolio_loss']\n",
    "    \n",
    "    # Align var_df\n",
    "    var_df_indexed = var_df.set_index('Date')\n",
    "    \n",
    "    # Use intersection of indices\n",
    "    common_index = actual_returns.index.intersection(var_df_indexed.index)\n",
    "    actual_returns_aligned = actual_returns.loc[common_index]\n",
    "    var_df_aligned = var_df_indexed.loc[common_index]\n",
    "    \n",
    "    if actual_returns_aligned.empty or var_df_aligned.empty:\n",
    "        print(f\"No common dates found for plotting {horizon}-day VaR. Skipping.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual_returns_aligned.index, actual_returns_aligned, label=f'Actual {horizon}d Portfolio Return', color='blue', alpha=0.6)\n",
    "\n",
    "    for method in methods:\n",
    "        var_col = f'VaR_{horizon}d_{method}'\n",
    "        if var_col not in var_df_aligned.columns:\n",
    "            continue\n",
    "            \n",
    "        # Define safe_extract locally or ensure it's accessible\n",
    "        def safe_extract(series, index):\n",
    "            try:\n",
    "                return series.apply(lambda x: x[index] if isinstance(x, (list, np.ndarray)) and len(x) > index else np.nan)\n",
    "            except Exception:\n",
    "                return pd.Series(np.nan, index=series.index)\n",
    "                \n",
    "        for idx, cl in enumerate(confidence_levels):\n",
    "            alpha_idx = 0 if cl == 0.95 else 1\n",
    "            # Apply safe_extract to the aligned DataFrame column\n",
    "            var_vals = safe_extract(var_df_aligned[var_col], alpha_idx)\n",
    "            plt.plot(var_df_aligned.index, var_vals, # Plot negative VaR\n",
    "                        label=f'VaR {horizon}d {int(cl*100)}% ({labels[method]})', \n",
    "                        color=colors[method][idx], linestyle=linestyles[method])\n",
    "\n",
    "    plt.title(f'{horizon}-Day VaR (Historical & Sqrt Rule) vs Actual Portfolio Returns')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4526d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot for Normal Distribution\n",
    "plot_var_with_violations(\"Normal\", \"VaR Normal\", \"ES Normal\", var_results_df, es_results_df, main_df, \"Parametric VaR\")\n",
    "\n",
    "# # Plot for T3 Distribution\n",
    "plot_var_with_violations(\"T3\", \"VaR T3\", \"ES T3\", var_results_df, es_results_df, main_df, \"Parametric VaR\")\n",
    "\n",
    "# # Plot for Historical Distribution\n",
    "plot_var_with_violations(\"Historical\", \"VaR Historical\", \"ES Historical\", var_results_df, es_results_df, main_df, \"Historical VaR\")\n",
    "\n",
    "# # Plot for GARCH Distribution\n",
    "plot_var_with_violations(\"GARCH\", \"VaR GARCH\", \"ES GARCH\", var_results_df, es_results_df, main_df, \"GARCH VaR\")\n",
    "\n",
    "# Plot for EWMA Distribution\n",
    "plot_var_with_violations(\"EWMA\", \"VaR FHS λ=0.94\", \"ES FHS λ=0.94\", var_results_df, es_results_df, main_df, \"EWMA VaR FHS λ=0.94\")\n",
    "\n",
    "# Plot for EWMA Distribution with λ=0.97\n",
    "plot_var_with_violations(\"EWMA\", \"VaR FHS λ=0.97\", \"ES FHS λ=0.97\", var_results_df, es_results_df, main_df, \"EWMA VaR FHS λ=0.97\")\n",
    "\n",
    "# Plot for 5-day VaR\n",
    "plot_var_multiday_methods(var_5d, main_df, 5, confidence_levels=[0.95, 0.99])\n",
    "\n",
    "# Plot for 10-day VaR\n",
    "plot_var_multiday_methods(var_10d, main_df, 10, confidence_levels=[0.95, 0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50edf1",
   "metadata": {},
   "source": [
    "# Stress Testing\n",
    "\n",
    "For stress testing, different extreme changes are tested to measure their effect on the VaR and ES in the portfolio. We simulate these stresses by modifying the historical data over short, randomly selected periods and then re-evaluating the risk measures.\n",
    "\n",
    "**Scenarios:**\n",
    "1.  **Equity Stress:** Equity index returns changing by +/- 20% and +/- 40%.\n",
    "2.  **Currency Stress:** USD/EUR and JPY/EUR exchange rates moving by +/- 10%.\n",
    "3.  **Commodity Stress:** Interpreted as equity index returns changing by +/- 20% and +/- 40% (proxy for commodity impact).\n",
    "4.  **Interest Rate Stress:** 10-year government bond yield shifting by +/- 2% and +/- 3% (absolute shift).\n",
    "\n",
    "**Methodology:**\n",
    "- Stresses are applied over randomly selected periods (max 4 consecutive days).\n",
    "- Each scenario type (e.g., Equity +20%) is applied 5 times to different random periods.\n",
    "- After applying stress to the underlying data, all dependent portfolio metrics (bond values, investment values, portfolio value, losses, returns) are recalculated.\n",
    "- The VaR and ES calculations (using the previously defined `main` function) are re-run on the stressed dataset.\n",
    "- Results are compared to the baseline (unstressed) VaR and ES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_stress_periods(df, num_periods=5, max_duration=4, min_separation=20, data_start_offset=500):\n",
    "    \"\"\"Selects random start indices for stress periods.\"\"\"\n",
    "    potential_starts = list(range(data_start_offset, len(df) - max_duration))\n",
    "    selected_indices = []\n",
    "    attempts = 0\n",
    "    max_attempts = num_periods * 10 # Prevent infinite loops\n",
    "    \n",
    "    while len(selected_indices) < num_periods and attempts < max_attempts:\n",
    "        start_index = random.choice(potential_starts)\n",
    "        duration = random.randint(1, max_duration)\n",
    "        valid = True\n",
    "        # Check for overlap/proximity with already selected periods\n",
    "        for existing_start, existing_duration in selected_indices:\n",
    "            if abs(start_index - existing_start) < min_separation:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            selected_indices.append((start_index, duration))\n",
    "        attempts += 1\n",
    "        \n",
    "    if len(selected_indices) < num_periods:\n",
    "        print(f\"Warning: Could only select {len(selected_indices)} out of {num_periods} stress periods.\")\n",
    "        \n",
    "    return selected_indices\n",
    "\n",
    "def recalculate_portfolio_metrics(df_stressed):\n",
    "    \"\"\"Recalculates all derived portfolio metrics after stressing base data.\"\"\"\n",
    "    df_recalc = df_stressed.copy()\n",
    "    \n",
    "    # --- Recalculate Bond Values (from cell 72b2903a) ---\n",
    "    days_per_annum = 365\n",
    "    interest_bond_initial = 1500000 # Initial investment in bond part of portfolio\n",
    "    interest_bond_vector = np.zeros(len(df_recalc))\n",
    "    interest_bond_profit_vector = np.zeros(len(df_recalc))\n",
    "    interest_bond_loss_vector = np.zeros(len(df_recalc))\n",
    "    daily_rates = np.zeros(len(df_recalc))\n",
    "    \n",
    "    if len(df_recalc) > 0:\n",
    "        interest_bond_vector[0] = interest_bond_initial\n",
    "        for i in range(len(df_recalc)):\n",
    "            # Ensure the column name matches exactly\n",
    "            yield_col_name = 'Yield curve spot rate, 10-year maturity - Government bond'\n",
    "            if yield_col_name in df_recalc.columns:\n",
    "                 # Adding 1.5% credit risk spread, converting to daily rate\n",
    "                daily_rate = (((df_recalc[yield_col_name].iloc[i] + 1.5) / days_per_annum) * (7/5)) / 100\n",
    "                daily_rates[i] = daily_rate\n",
    "                if i > 0:\n",
    "                    previous_value = interest_bond_vector[i-1]\n",
    "                    current_value = previous_value * (1 + daily_rate)\n",
    "                    interest_bond_vector[i] = current_value\n",
    "                    change = current_value - previous_value\n",
    "                    interest_bond_profit_vector[i] = change\n",
    "                    interest_bond_loss_vector[i] = -change\n",
    "                else: # First day calculation\n",
    "                     daily_rate_first = (((df_recalc[yield_col_name].iloc[0] + 1.5) / days_per_annum) * (7/5)) / 100\n",
    "                     daily_rates[0] = daily_rate_first\n",
    "                     # No profit/loss on day 0\n",
    "            else:\n",
    "                print(f\"Warning: Column '{yield_col_name}' not found during recalculation.\")\n",
    "                # Handle missing column case, e.g., set rates to 0 or raise error\n",
    "                daily_rates[i] = 0\n",
    "                if i > 0: interest_bond_vector[i] = interest_bond_vector[i-1]\n",
    "                \n",
    "        df_recalc['Interest_Bond'] = interest_bond_vector\n",
    "        df_recalc['Interest_Bond_Profit'] = interest_bond_profit_vector\n",
    "        df_recalc['Interest_Bond_Loss'] = interest_bond_loss_vector\n",
    "        df_recalc['Interest_Bond_daily_rate'] = daily_rates\n",
    "    \n",
    "    # --- Recalculate Investment Values (from cell b7a319cf) ---\n",
    "    # Assumes initial investment amounts are correctly set elsewhere or use the first row if available\n",
    "    # Need starting amounts (re-run cell b203e94a logic conceptually)\n",
    "    weights = {'S&P500': 0.4, 'DAX40': 0.3, 'NIKKEI': 0.15, 'EU-BOND': 0.15}\n",
    "    starting_investment = 10000000\n",
    "    starting_date = '2012-01-04'\n",
    "    start_idx_lookup = df_recalc[df_recalc['Date'] == starting_date].index\n",
    "    start_idx = None # Initialize start_idx\n",
    "    found_start_date = len(start_idx_lookup) > 0\n",
    "    \n",
    "    if found_start_date:\n",
    "        start_idx = start_idx_lookup[0] # Get the first index if found\n",
    "        starting_row = df_recalc.loc[start_idx]\n",
    "        usd_to_eur = float(starting_row['USD/EUR'])\n",
    "        jpy_to_eur = float(starting_row['JPY/EUR'])\n",
    "        invested_amount_SP500 = starting_investment * weights['S&P500'] / usd_to_eur\n",
    "        invested_amount_DAX40 = starting_investment * weights['DAX40']\n",
    "        invested_amount_NIKKEI = starting_investment * weights['NIKKEI'] / jpy_to_eur\n",
    "        # EU Bond initial investment is implicitly handled by Interest_Bond starting value\n",
    "        \n",
    "        df_recalc.loc[start_idx, 'SP500_Investment'] = invested_amount_SP500\n",
    "        df_recalc.loc[start_idx, 'DAX40_Investment'] = invested_amount_DAX40\n",
    "        df_recalc.loc[start_idx, 'NIKKEI_Investment'] = invested_amount_NIKKEI\n",
    "        # EU_BOND_Investment is directly from Interest_Bond\n",
    "        \n",
    "        for i in range(start_idx + 1, len(df_recalc)):\n",
    "            df_recalc.loc[i, 'SP500_Investment'] = df_recalc.loc[i-1, 'SP500_Investment'] * (1 + df_recalc.loc[i, 'C_S&P500_Returns'])\n",
    "            df_recalc.loc[i, 'DAX40_Investment'] = df_recalc.loc[i-1, 'DAX40_Investment'] * (1 + df_recalc.loc[i, 'C_Dax40_Returns'])\n",
    "            df_recalc.loc[i, 'NIKKEI_Investment'] = df_recalc.loc[i-1, 'NIKKEI_Investment'] * (1 + df_recalc.loc[i, 'C_Nikkei_Returns'])\n",
    "    \n",
    "    # EU Bond investment is directly the calculated bond value\n",
    "    df_recalc['EU_BOND_Investment'] = df_recalc['Interest_Bond']\n",
    "    \n",
    "    # --- Recalculate Portfolio Value/Change/Loss (from cell d340b392) ---\n",
    "    df_recalc['Portfolio_Value_EUR'] = (\n",
    "        df_recalc['SP500_Investment'].fillna(0) * df_recalc['USD/EUR'] +\n",
    "        df_recalc['DAX40_Investment'].fillna(0) +\n",
    "        df_recalc['NIKKEI_Investment'].fillna(0) * df_recalc['JPY/EUR'] +\n",
    "        df_recalc['EU_BOND_Investment'].fillna(0)\n",
    "    )\n",
    "    # Reset first day value if necessary\n",
    "    if found_start_date:\n",
    "         df_recalc.loc[start_idx, 'Portfolio_Value_EUR'] = starting_investment\n",
    "         \n",
    "    df_recalc['Portfolio_Change_EUR'] = df_recalc['Portfolio_Value_EUR'].diff()\n",
    "    df_recalc['Portfolio_loss'] = -df_recalc['Portfolio_Change_EUR']\n",
    "    # Set first day's change/loss to 0\n",
    "    if found_start_date:\n",
    "        df_recalc.loc[start_idx, 'Portfolio_Change_EUR'] = 0.0\n",
    "        df_recalc.loc[start_idx, 'Portfolio_loss'] = 0.0\n",
    "    else: # If start date not found, set first row in df\n",
    "        if len(df_recalc)>0:\n",
    "            df_recalc.loc[df_recalc.index[0], 'Portfolio_Change_EUR'] = 0.0 # Use index[0] for robustness\n",
    "            df_recalc.loc[df_recalc.index[0], 'Portfolio_loss'] = 0.0\n",
    "            \n",
    "    # --- Recalculate Portfolio Daily Returns (from cell 25566d3c) ---\n",
    "    df_recalc['Portfolio_Daily_Returns'] = (\n",
    "        weights['S&P500'] * df_recalc['C_S&P500_Returns'] +\n",
    "        weights['DAX40'] * df_recalc['C_Dax40_Returns'] +\n",
    "        weights['NIKKEI'] * df_recalc['C_Nikkei_Returns'] +\n",
    "        weights['EU-BOND'] * df_recalc['Interest_Bond_daily_rate']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    return df_recalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_equity(df, periods, shock_factor):\n",
    "    \"\"\"Applies additive shock to equity returns during specified periods.\"\"\"\n",
    "    df_stressed = df.copy()\n",
    "    equity_cols = ['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns']\n",
    "    for start_index, duration in periods:\n",
    "        for i in range(start_index, min(start_index + duration, len(df_stressed))):\n",
    "            for col in equity_cols:\n",
    "                # Apply additive shock: return' = return + shock\n",
    "                df_stressed.loc[i, col] = df_stressed.loc[i, col] + shock_factor\n",
    "    return df_stressed\n",
    "\n",
    "def stress_currency(df, periods, shock_factor_major, shock_factor_other):\n",
    "    \"\"\"Applies multiplicative shock to currency rates during specified periods.\"\"\"\n",
    "    df_stressed = df.copy()\n",
    "    # Assuming USD/EUR and JPY/EUR are major currencies\n",
    "    major_cols = ['USD/EUR', 'JPY/EUR']\n",
    "    # other_cols = [] # Add other currency columns if they exist\n",
    "    \n",
    "    for start_index, duration in periods:\n",
    "        for i in range(start_index, min(start_index + duration, len(df_stressed))):\n",
    "            for col in major_cols:\n",
    "                # Apply multiplicative shock: rate' = rate * shock\n",
    "                df_stressed.loc[i, col] = df_stressed.loc[i, col] * shock_factor_major\n",
    "            # for col in other_cols:\n",
    "            #     df_stressed.loc[i, col] = df_stressed.loc[i, col] * shock_factor_other\n",
    "    return df_stressed\n",
    "\n",
    "def stress_commodity(df, periods, shock_factor):\n",
    "    \"\"\"Applies additive shock to equity returns as a proxy for commodity stress.\"\"\"\n",
    "    # Reusing stress_equity logic as interpretation\n",
    "    return stress_equity(df, periods, shock_factor)\n",
    "\n",
    "def stress_interest_rate(df, periods, shock_shift):\n",
    "    \"\"\"Applies additive shift to interest rates during specified periods.\"\"\"\n",
    "    df_stressed = df.copy()\n",
    "    rate_col = 'Yield curve spot rate, 10-year maturity - Government bond'\n",
    "    for start_index, duration in periods:\n",
    "        for i in range(start_index, min(start_index + duration, len(df_stressed))):\n",
    "            # Apply additive shift: rate' = rate + shift\n",
    "            # Ensure rate doesn't go below a reasonable floor if necessary (e.g., 0 or a small negative number)\n",
    "            df_stressed.loc[i, rate_col] = max(-1.0, df_stressed.loc[i, rate_col] + shock_shift) # Example floor of -1%\n",
    "            \n",
    "    # IMPORTANT: Recalculate all dependent metrics after stressing interest rates\n",
    "    df_stressed = recalculate_portfolio_metrics(df_stressed)\n",
    "    return df_stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stress_scenario(original_df, stress_type, shock_params, scenario_name):\n",
    "    \"\"\"Runs a single stress scenario and recalculates VaR/ES.\"\"\"\n",
    "    print(f\"--- Running Stress Scenario: {scenario_name} ---\")\n",
    "    df_stressed = original_df.copy()\n",
    "    \n",
    "    # Select random periods for this specific scenario run\n",
    "    periods = select_stress_periods(df_stressed, num_periods=shock_params.get('num_repeats', 5), \n",
    "                                    max_duration=shock_params.get('duration', 4))\n",
    "    \n",
    "    if stress_type == 'equity':\n",
    "        df_stressed = stress_equity(df_stressed, periods, shock_params['shock'])\n",
    "        df_stressed = recalculate_portfolio_metrics(df_stressed) # Recalc needed as returns changed\n",
    "    elif stress_type == 'currency':\n",
    "        df_stressed = stress_currency(df_stressed, periods, shock_params['shock_major'], shock_params.get('shock_other', 1.0))\n",
    "        df_stressed = recalculate_portfolio_metrics(df_stressed) # Recalc needed as FX changed\n",
    "    elif stress_type == 'commodity':\n",
    "        df_stressed = stress_commodity(df_stressed, periods, shock_params['shock'])\n",
    "        df_stressed = recalculate_portfolio_metrics(df_stressed) # Recalc needed as returns changed\n",
    "    elif stress_type == 'interest_rate':\n",
    "        # Recalculation is already handled within stress_interest_rate\n",
    "        df_stressed = stress_interest_rate(df_stressed, periods, shock_params['shift'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown stress type: {stress_type}\")\n",
    "        \n",
    "    # Re-run VaR/ES calculation using the existing main() function logic\n",
    "    # Modify main() to accept a dataframe as input\n",
    "    def main_modified(input_df):\n",
    "        # Initialize lists to store results\n",
    "        VaR_results = []\n",
    "        ES_results = []\n",
    "        # Define time window\n",
    "        time_window = input_df[(input_df['Date'] >= '2012-01-05') & (input_df['Date'] <= '2021-12-31')]\n",
    "\n",
    "        # Define confidence levels\n",
    "        vAlpha = np.array([0.95, 0.99])\n",
    "        \n",
    "        # Define sample size and t-distribution degrees of freedom\n",
    "        sample_size = 500\n",
    "        degrees_of_freedom = [0, 3, 4, 5, 6]  # 0 represents normal distribution\n",
    "        \n",
    "        for i in range(sample_size, len(time_window)):\n",
    "            # Extract the rolling window\n",
    "            window = time_window.iloc[i - sample_size:i]\n",
    "            current_date = time_window.iloc[i]['Date']\n",
    "            \n",
    "            # Calculate loss statistics\n",
    "            loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "            mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "            portfolio_std_loss = loss_stats[\"Portfolio_std_loss\"]\n",
    "            \n",
    "            # Initialize result dictionaries for this date\n",
    "            var_row = {'Date': current_date}\n",
    "            es_row = {'Date': current_date}\n",
    "\n",
    "            # Calculate VaR and ES using various distributions\n",
    "            for df in degrees_of_freedom:\n",
    "                results = calculate_var_cov(current_date, vAlpha, mean_loss, portfolio_std_loss, df)\n",
    "                # Get the distribution label\n",
    "                if df == 0:\n",
    "                    dist_label = \"Normal\"\n",
    "                else:\n",
    "                    dist_label = f\"T{df}\"\n",
    "                # Add results to the dictionaries\n",
    "                var_row[f'VaR {dist_label}'] = results[f'VaR {dist_label}']\n",
    "                es_row[f'ES {dist_label}'] = results[f'ES {dist_label}']\n",
    "            \n",
    "            # Calculate VaR and ES using historical simulation\n",
    "            hist_results = calculate_historical_var_es(window, current_date, vAlpha)\n",
    "            var_row['VaR Historical'] = hist_results['VaR Historical']\n",
    "            es_row['ES Historical'] = hist_results['ES Historical']\n",
    "            \n",
    "            # Add results for this date\n",
    "            VaR_results.append(var_row)\n",
    "            ES_results.append(es_row)\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        var_results_df = pd.DataFrame(VaR_results)\n",
    "        es_results_df = pd.DataFrame(ES_results)\n",
    "\n",
    "        # Example usage\n",
    "        returns_df = main_df[['Date', 'C_S&P500_Returns', 'C_Dax40_Returns', \n",
    "                            'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "        #set Date as index\n",
    "        returns_df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Calculate VaR and ES using rolling FHS\n",
    "        var_df, es_df = rolling_fhs_multiday_var_es(\n",
    "            returns_df=returns_df,  # DataFrame containing returns\n",
    "            weights=weights,        # Portfolio weights\n",
    "            window_size=500,       # Base window size\n",
    "            horizons=[1],   # Horizons for VaR calculation\n",
    "            confidence_levels=[0.95, 0.99],  # Confidence levels\n",
    "            n_simulations=1000,    # Number of simulations\n",
    "            lambdas=[0.94, 0.97]   # EWMA lambda values\n",
    "        )\n",
    "\n",
    "        # Align indices for merging\n",
    "        var_results_df = var_results_df.copy()\n",
    "        es_results_df = es_results_df.copy()\n",
    "\n",
    "        # Ensure 'Date' is datetime and set as index for alignment\n",
    "        var_results_df['Date'] = pd.to_datetime(var_results_df['Date'])\n",
    "        es_results_df['Date'] = pd.to_datetime(es_results_df['Date'])\n",
    "        var_results_df.set_index('Date', inplace=True)\n",
    "        es_results_df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Add FHS VaR and ES columns as lists [95, 99] for each lambda\n",
    "        for lambda_ in [0.94, 0.97]:\n",
    "            var_col_95 = f'VaR_95_lambda_{lambda_}_h1'\n",
    "            var_col_99 = f'VaR_99_lambda_{lambda_}_h1'\n",
    "            es_col_95 = f'ES_95_lambda_{lambda_}_h1'\n",
    "            es_col_99 = f'ES_99_lambda_{lambda_}_h1'\n",
    "\n",
    "            # compute monetary values\n",
    "            var_df_corrected, es_df_corrected = calculate_monetary_values(var_df, es_df, input_df)\n",
    "\n",
    "            # Align var_df/es_df to var_results_df by Date\n",
    "            aligned_var = var_df_corrected.loc[var_results_df.index, [var_col_95, var_col_99]]\n",
    "            aligned_es = es_df_corrected.loc[es_results_df.index, [es_col_95, es_col_99]]\n",
    "\n",
    "            # Combine [95,99] into a list for each row\n",
    "            var_results_df[f'VaR FHS λ={lambda_}'] = aligned_var.values.tolist()\n",
    "            es_results_df[f'ES FHS λ={lambda_}'] = aligned_es.values.tolist()\n",
    "\n",
    "        # Reset index to restore 'Date' as a column\n",
    "        var_results_df.reset_index(inplace=True)\n",
    "        es_results_df.reset_index(inplace=True)\n",
    "\n",
    "        # Insert the GARCH values into var_results_df and es_results_df using the main_analysis function\n",
    "        garch_results_df = main_analysis(500, vAlpha=[0.95, 0.99])\n",
    "\n",
    "        # Ensure 'Date' is datetime for alignment\n",
    "        garch_results_df['Date'] = pd.to_datetime(garch_results_df['Date'])\n",
    "\n",
    "        # convert to monetary values by multiplying by the portfolio value\n",
    "        # portfolio value is Portfolio_Value_EUR in main_df, each row for Portfolio VaR and ES contains a list of [95, 99] which is the VaR and ES for each confidence level\n",
    "        # it is converted by multiplying the portfolio value by the VaR and ES values\n",
    "        portfolio_value = input_df['Portfolio_Value_EUR'].iloc[-len(garch_results_df):].values\n",
    "        # Multiply each VaR/ES value by the corresponding portfolio value for each row\n",
    "        garch_results_df['Portfolio VaR'] = [\n",
    "            [v * pv for v in var_list] for var_list, pv in zip(garch_results_df['Portfolio VaR'], portfolio_value)\n",
    "        ]\n",
    "        garch_results_df['Portfolio ES'] = [\n",
    "            [v * pv for v in es_list] for es_list, pv in zip(garch_results_df['Portfolio ES'], portfolio_value)\n",
    "        ]\n",
    "\n",
    "        # Align GARCH results to var_results_df by Date\n",
    "        aligned_garch = garch_results_df.set_index('Date').reindex(var_results_df['Date'])\n",
    "\n",
    "        # Add GARCH VaR and ES columns as lists [95, 99] for each row\n",
    "        var_results_df['VaR GARCH'] = aligned_garch['Portfolio VaR'].tolist()\n",
    "        es_results_df['ES GARCH'] = aligned_garch['Portfolio ES'].tolist()\n",
    "\n",
    "        var_5d = calculate_multiday_var(vAlpha, 5, sample_size)\n",
    "        var_10d = calculate_multiday_var(vAlpha, 10, sample_size)\n",
    "\n",
    "        return var_results_df, es_results_df, var_5d, var_10d\n",
    "        \n",
    "    stressed_var, stressed_es, _, _ = main_modified(df_stressed)\n",
    "    print(f\"--- Finished Stress Scenario: {scenario_name} ---\")\n",
    "    return stressed_var, stressed_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_testing_main(original_df, baseline_var, baseline_es):\n",
    "    \"\"\"Defines and runs all stress test scenarios and compares all VaR/ES columns.\"\"\"\n",
    "    scenarios = {\n",
    "        'Equity +20%': {'type': 'equity', 'params': {'shock': 0.20}},\n",
    "        'Equity -20%': {'type': 'equity', 'params': {'shock': -0.20}},\n",
    "        'Equity +40%': {'type': 'equity', 'params': {'shock': 0.40}},\n",
    "        'Equity -40%': {'type': 'equity', 'params': {'shock': -0.40}},\n",
    "        'Currency +10%': {'type': 'currency', 'params': {'shock_major': 1.10}},\n",
    "        'Currency -10%': {'type': 'currency', 'params': {'shock_major': 0.90}},\n",
    "        'Commodity +20%': {'type': 'commodity', 'params': {'shock': 0.20}},\n",
    "        'Commodity -20%': {'type': 'commodity', 'params': {'shock': -0.20}},\n",
    "        'Commodity +40%': {'type': 'commodity', 'params': {'shock': 0.40}},\n",
    "        'Commodity -40%': {'type': 'commodity', 'params': {'shock': -0.40}},\n",
    "        'Interest Rate +2%': {'type': 'interest_rate', 'params': {'shift': 2.0}},\n",
    "        'Interest Rate -2%': {'type': 'interest_rate', 'params': {'shift': -2.0}},\n",
    "        'Interest Rate +3%': {'type': 'interest_rate', 'params': {'shift': 3.0}},\n",
    "        'Interest Rate -3%': {'type': 'interest_rate', 'params': {'shift': -3.0}},\n",
    "    }\n",
    "    \n",
    "    results_summary = {}\n",
    "\n",
    "    # Helper to compute average for list-valued columns\n",
    "    def avg_list_col(df, col, idx):\n",
    "        return np.mean([v[idx] for v in df[col] if isinstance(v, (list, np.ndarray)) and len(v) > idx])\n",
    "\n",
    "    for name, config in scenarios.items():\n",
    "        stressed_var, stressed_es = run_stress_scenario(original_df, config['type'], config['params'], name)\n",
    "        summary = {}\n",
    "\n",
    "        if not stressed_var.empty and not baseline_var.empty:\n",
    "            # Loop through all VaR columns\n",
    "            for var_col in baseline_var.columns:\n",
    "                if var_col == 'Date' or var_col not in stressed_var.columns:\n",
    "                    continue\n",
    "                # If column contains lists (for 95/99%), compare both\n",
    "                if isinstance(baseline_var[var_col].iloc[0], (list, np.ndarray)):\n",
    "                    for idx, cl in enumerate(['95', '99']):\n",
    "                        base_avg = avg_list_col(baseline_var, var_col, idx)\n",
    "                        stress_avg = avg_list_col(stressed_var, var_col, idx)\n",
    "                        summary[f'Avg {var_col} {cl}% Change'] = stress_avg - base_avg\n",
    "                        summary[f'Avg Stressed {var_col} {cl}%'] = stress_avg\n",
    "                else:\n",
    "                    base_avg = baseline_var[var_col].mean()\n",
    "                    stress_avg = stressed_var[var_col].mean()\n",
    "                    summary[f'Avg {var_col} Change'] = stress_avg - base_avg\n",
    "                    summary[f'Avg Stressed {var_col}'] = stress_avg\n",
    "\n",
    "            # Loop through all ES columns\n",
    "            for es_col in baseline_es.columns:\n",
    "                if es_col == 'Date' or es_col not in stressed_es.columns:\n",
    "                    continue\n",
    "                if isinstance(baseline_es[es_col].iloc[0], (list, np.ndarray)):\n",
    "                    for idx, cl in enumerate(['95', '99']):\n",
    "                        base_avg = avg_list_col(baseline_es, es_col, idx)\n",
    "                        stress_avg = avg_list_col(stressed_es, es_col, idx)\n",
    "                        summary[f'Avg {es_col} {cl}% Change'] = stress_avg - base_avg\n",
    "                        summary[f'Avg Stressed {es_col} {cl}%'] = stress_avg\n",
    "                else:\n",
    "                    base_avg = baseline_es[es_col].mean()\n",
    "                    stress_avg = stressed_es[es_col].mean()\n",
    "                    summary[f'Avg {es_col} Change'] = stress_avg - base_avg\n",
    "                    summary[f'Avg Stressed {es_col}'] = stress_avg\n",
    "        else:\n",
    "            summary = {'Error': 'Calculation failed or insufficient data'}\n",
    "\n",
    "        results_summary[name] = summary\n",
    "\n",
    "    return pd.DataFrame.from_dict(results_summary, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ccc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = stress_testing_main(main_df, var_results_df, es_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each VaR/ES method's changes under stress scenarios\n",
    "for col in results_summary.columns:\n",
    "    if 'Change' in col:\n",
    "        results_summary.plot(\n",
    "            y=col,\n",
    "            kind='bar',\n",
    "            figsize=(12, 5),\n",
    "            title=f'Change in {col} under Stress Scenarios'\n",
    "        )\n",
    "        plt.ylabel('Change in Value (EUR)')\n",
    "        plt.xlabel('Stress Scenario')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
