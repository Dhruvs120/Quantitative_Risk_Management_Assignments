{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc2ba2c",
   "metadata": {},
   "source": [
    "### Initial package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f45638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c440bd4",
   "metadata": {},
   "source": [
    "## Initial files being read in and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9ce118",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "# Read the data\n",
    "main_df = pd.read_csv('Data/Cleaned_Indices_Assignment1.csv', sep=';')\n",
    "\n",
    "# Read the interest rate data\n",
    "#interest_rate_df = pd.read_csv('Data/ECB_Rates_2012_to_2022.csv', sep=';')\n",
    "interest_rate_bond_df = pd.read_csv('Data/ECB_Data_10yr_Treasury_bond.csv', sep=',')\n",
    "\n",
    "# Convert date columns to datetime format for proper merging\n",
    "main_df['Date'] = pd.to_datetime(main_df['Date'], format='%d-%m-%Y')\n",
    "#interest_rate_df['Date'] = pd.to_datetime(interest_rate_df['Date'], format='%d-%m-%Y')\n",
    "#gov_bond_investment_df['Date'] = pd.to_datetime(gov_bond_investment_df['Date'], format='%Y-%m-%d')\n",
    "interest_rate_bond_df['Date'] = pd.to_datetime(interest_rate_bond_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Merge the dataframes on the Date column\n",
    "main_df = pd.merge(main_df, interest_rate_bond_df, on='Date', how='left')\n",
    "#main_df = pd.merge(main_df, gov_bond_investment_df, on='Date', how='left')\n",
    "\n",
    "# Remove rows where the bond does not have a yield curve spot rate (Market closed?)\n",
    "main_df = main_df.dropna(axis=0, subset=['Yield curve spot rate, 10-year maturity - Government bond'])\n",
    "\n",
    "# Filter the dataframe to start from 2012-01-04\n",
    "main_df = main_df[main_df['Date'] >= '2012-01-04']\n",
    "main_df = main_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c10b0c",
   "metadata": {},
   "source": [
    "### government bond column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e954a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "# Add a column for the interest bond value per day\n",
    "days_per_annum = 365\n",
    "interest_bond = 1500000\n",
    "\n",
    "# Initialize the arrays with appropriate lengths matching the DataFrame\n",
    "interest_bond_vector = np.zeros(len(main_df))\n",
    "interest_bond_profit_vector = np.zeros(len(main_df))\n",
    "interest_bond_loss_vector = np.zeros(len(main_df))\n",
    "daily_rates = np.zeros(len(main_df))\n",
    "\n",
    "# Set initial value\n",
    "interest_bond_vector[0] = interest_bond\n",
    "\n",
    "\n",
    "# Calculate bond values day by day based on the daily yield rate\n",
    "for i in range(len(main_df)):\n",
    "    # Adding 1.5% to account for the credit risk spread\n",
    "    daily_rate = (((main_df['Yield curve spot rate, 10-year maturity - Government bond'].iloc[i] + 1.5) / (days_per_annum)) * (7/5)) / 100\n",
    "    daily_rates[i] = daily_rate\n",
    "    \n",
    "    if i > 0:\n",
    "        previous_value = interest_bond_vector[i-1]\n",
    "        current_value = previous_value * (1 + daily_rate)\n",
    "        interest_bond_vector[i] = current_value\n",
    "        \n",
    "        # Calculate change, profit/loss and return\n",
    "        change = current_value - previous_value\n",
    "        interest_bond_profit_vector[i] = change\n",
    "        interest_bond_loss_vector[i] = -change\n",
    "\n",
    "# Add vectors to the dataframe\n",
    "main_df['Interest_Bond'] = interest_bond_vector\n",
    "main_df['Interest_Bond_Profit'] = interest_bond_profit_vector\n",
    "main_df['Interest_Bond_Loss'] = interest_bond_loss_vector\n",
    "main_df['Interest_Bond_daily_rate'] = daily_rates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de917d",
   "metadata": {},
   "source": [
    "## Portfolio details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3557320",
   "metadata": {},
   "source": [
    "### details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d95594",
   "metadata": {},
   "source": [
    "#### Instruments:\n",
    "- **S&P500**\n",
    "- **DAX40**\n",
    "- **NIKKEI**\n",
    "- **EU Government Bond (10-year maturity, AAA-rated)**\n",
    "\n",
    "#### Invested amount:\n",
    "- **10,000,000 EURO**\n",
    "\n",
    "#### Period:\n",
    "- **01/01/2012 - 31/12/2022**\n",
    "\n",
    "#### Weights:\n",
    "- **S&P500**: 0.4  \n",
    "- **DAX40**: 0.3  \n",
    "- **NIKKEI**: 0.15  \n",
    "- **EU Government Bond**: 0.15  \n",
    "\n",
    "#### Measures:\n",
    "- **Value at Risk (VaR)**: 1, 5, 10 days  \n",
    "- **Expected Shortfall (ES)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d60941",
   "metadata": {},
   "source": [
    "### weights and currency correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688bd314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5177993.527508091, 3000000.0, 148957298.90764648, 1500000.0]\n"
     ]
    }
   ],
   "source": [
    "# initial investment \n",
    "weights = {\n",
    "    'S&P500': 0.4,\n",
    "    'DAX40': 0.3,\n",
    "    'NIKKEI': 0.15,\n",
    "    'EU-BOND': 0.15,\n",
    "}\n",
    "\n",
    "starting_investment = 10000000  # 10 million euros\n",
    "starting_date = '2012-01-04'\n",
    "\n",
    "# Filter the main_df for the starting date\n",
    "starting_row = main_df[main_df['Date'] == starting_date]\n",
    "\n",
    "# Extract the exchange rates for the starting date\n",
    "usd_to_eur = float(starting_row['USD/EUR'].iloc[0])\n",
    "jpy_to_eur = float(starting_row['JPY/EUR'].iloc[0])\n",
    "\n",
    "# Calculate the invested amounts\n",
    "invested_amount_SP500 = starting_investment * weights['S&P500'] / usd_to_eur\n",
    "invested_amount_DAX40 = starting_investment * weights['DAX40']\n",
    "invested_amount_NIKKEI = starting_investment * weights['NIKKEI'] / jpy_to_eur\n",
    "invested_amount_EU_BOND = starting_investment * weights['EU-BOND']\n",
    "\n",
    "invested_amounts = [\n",
    "    invested_amount_SP500, #in USD\n",
    "    invested_amount_DAX40, #in EUR\n",
    "    invested_amount_NIKKEI, #in JPY\n",
    "    invested_amount_EU_BOND #in EUR\n",
    "]\n",
    "\n",
    "print(invested_amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a6bef",
   "metadata": {},
   "source": [
    "### Returns Portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49ef3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to track investments for each asset\n",
    "# Initialize the first day with the initial invested amounts\n",
    "main_df.loc[0, 'SP500_Investment'] = invested_amount_SP500\n",
    "main_df.loc[0, 'DAX40_Investment'] = invested_amount_DAX40\n",
    "main_df.loc[0, 'NIKKEI_Investment'] = invested_amount_NIKKEI\n",
    "main_df.loc[0, 'EU_BOND_Investment'] = invested_amount_EU_BOND\n",
    "\n",
    "# Calculate daily investment values for subsequent days\n",
    "# This uses cumulative returns to track the value growth\n",
    "for i in range(1, len(main_df)):\n",
    "    # S&P 500 in USD\n",
    "    main_df.loc[i, 'SP500_Investment'] = main_df.loc[i-1, 'SP500_Investment'] * (1 + main_df.loc[i, 'C_S&P500_Returns'])\n",
    "    \n",
    "    # DAX 40 in EUR\n",
    "    main_df.loc[i, 'DAX40_Investment'] = main_df.loc[i-1, 'DAX40_Investment'] * (1 + main_df.loc[i, 'C_Dax40_Returns'])\n",
    "    \n",
    "    # NIKKEI in JPY\n",
    "    main_df.loc[i, 'NIKKEI_Investment'] = main_df.loc[i-1, 'NIKKEI_Investment'] * (1 + main_df.loc[i, 'C_Nikkei_Returns'])\n",
    "    \n",
    "# EU Government Bond value is already calculated in the Interest_Bond column\n",
    "main_df['EU_BOND_Investment'] = main_df['Interest_Bond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fc2cb",
   "metadata": {},
   "source": [
    "# Methods input values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd3e01",
   "metadata": {},
   "source": [
    "### Portfolio change Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d45a427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500_Investment</th>\n",
       "      <th>DAX40_Investment</th>\n",
       "      <th>NIKKEI_Investment</th>\n",
       "      <th>EU_BOND_Investment</th>\n",
       "      <th>USD/EUR</th>\n",
       "      <th>JPY/EUR</th>\n",
       "      <th>Portfolio_Value_EUR</th>\n",
       "      <th>Portfolio_Change_EUR</th>\n",
       "      <th>Portfolio_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>5177993.527508</td>\n",
       "      <td>3000000.000000</td>\n",
       "      <td>148957298.907646</td>\n",
       "      <td>1500000.000000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>10000000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>5189897.180583</td>\n",
       "      <td>2992352.262000</td>\n",
       "      <td>148944667.328699</td>\n",
       "      <td>1500246.523142</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>10058937.403442</td>\n",
       "      <td>58937.403442</td>\n",
       "      <td>-58937.403442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>5179533.786296</td>\n",
       "      <td>2973606.159562</td>\n",
       "      <td>148926943.509066</td>\n",
       "      <td>1500493.291886</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.010210</td>\n",
       "      <td>10066275.054084</td>\n",
       "      <td>7337.650642</td>\n",
       "      <td>-7337.650642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>5188699.359796</td>\n",
       "      <td>2953565.582481</td>\n",
       "      <td>148926943.509066</td>\n",
       "      <td>1500738.323863</td>\n",
       "      <td>0.783300</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>10036177.669230</td>\n",
       "      <td>-30097.384854</td>\n",
       "      <td>30097.384854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>5224622.209432</td>\n",
       "      <td>3024254.466714</td>\n",
       "      <td>148932706.981779</td>\n",
       "      <td>1500982.736072</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>10131650.828032</td>\n",
       "      <td>95473.158802</td>\n",
       "      <td>-95473.158802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>5225894.770664</td>\n",
       "      <td>3019028.766693</td>\n",
       "      <td>148937338.788967</td>\n",
       "      <td>1501223.999292</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>10156138.336831</td>\n",
       "      <td>24487.508799</td>\n",
       "      <td>-24487.508799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>5235403.197359</td>\n",
       "      <td>3032185.494802</td>\n",
       "      <td>148926138.701090</td>\n",
       "      <td>1501460.534432</td>\n",
       "      <td>0.779600</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>10128255.931098</td>\n",
       "      <td>-27882.405733</td>\n",
       "      <td>27882.405733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>5214914.028114</td>\n",
       "      <td>3014404.192042</td>\n",
       "      <td>148946848.667790</td>\n",
       "      <td>1501695.498805</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>10158861.526361</td>\n",
       "      <td>30605.595263</td>\n",
       "      <td>-30605.595263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-01-16</td>\n",
       "      <td>5214914.028114</td>\n",
       "      <td>3051919.263087</td>\n",
       "      <td>148924774.744817</td>\n",
       "      <td>1501930.470054</td>\n",
       "      <td>0.789500</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>10201971.042714</td>\n",
       "      <td>43109.516353</td>\n",
       "      <td>-43109.516353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-01-17</td>\n",
       "      <td>5229430.909452</td>\n",
       "      <td>3106827.837990</td>\n",
       "      <td>148940684.676353</td>\n",
       "      <td>1502166.395712</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>10235748.351923</td>\n",
       "      <td>33777.309209</td>\n",
       "      <td>-33777.309209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>5274345.466564</td>\n",
       "      <td>3117425.957849</td>\n",
       "      <td>148955611.958593</td>\n",
       "      <td>1502403.866448</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>10229553.773692</td>\n",
       "      <td>-6194.578231</td>\n",
       "      <td>6194.578231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-01-19</td>\n",
       "      <td>5294387.103796</td>\n",
       "      <td>3147543.890112</td>\n",
       "      <td>148971047.185971</td>\n",
       "      <td>1502643.509093</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>10222862.760034</td>\n",
       "      <td>-6691.013658</td>\n",
       "      <td>6691.013658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-01-20</td>\n",
       "      <td>5297126.329679</td>\n",
       "      <td>3141715.579943</td>\n",
       "      <td>148992818.261720</td>\n",
       "      <td>1502885.831405</td>\n",
       "      <td>0.773100</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>10235697.672170</td>\n",
       "      <td>12834.912136</td>\n",
       "      <td>-12834.912136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>5299041.659320</td>\n",
       "      <td>3157486.568023</td>\n",
       "      <td>148992740.338476</td>\n",
       "      <td>1503131.732769</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>10211136.630721</td>\n",
       "      <td>-24561.041449</td>\n",
       "      <td>24561.041449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>5294865.797232</td>\n",
       "      <td>3148939.431860</td>\n",
       "      <td>148996003.279489</td>\n",
       "      <td>1503380.337308</td>\n",
       "      <td>0.767800</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>10189351.252674</td>\n",
       "      <td>-21785.378047</td>\n",
       "      <td>21785.378047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012-01-25</td>\n",
       "      <td>5329733.806928</td>\n",
       "      <td>3150229.309877</td>\n",
       "      <td>149012273.643047</td>\n",
       "      <td>1503627.843063</td>\n",
       "      <td>0.762700</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>10180357.507375</td>\n",
       "      <td>-8993.745298</td>\n",
       "      <td>8993.745298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>5306292.070704</td>\n",
       "      <td>3207588.656799</td>\n",
       "      <td>149006611.176649</td>\n",
       "      <td>1503871.191231</td>\n",
       "      <td>0.763200</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>10229086.083093</td>\n",
       "      <td>48728.575717</td>\n",
       "      <td>-48728.575717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-01-27</td>\n",
       "      <td>5299897.352003</td>\n",
       "      <td>3193890.097735</td>\n",
       "      <td>149005240.315826</td>\n",
       "      <td>1504112.070161</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>10173916.235525</td>\n",
       "      <td>-55169.847568</td>\n",
       "      <td>55169.847568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>5289710.716097</td>\n",
       "      <td>3160596.150557</td>\n",
       "      <td>148997119.530229</td>\n",
       "      <td>1504350.590111</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>10176744.842645</td>\n",
       "      <td>2828.607120</td>\n",
       "      <td>-2828.607120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-01-31</td>\n",
       "      <td>5287862.089155</td>\n",
       "      <td>3167679.925176</td>\n",
       "      <td>148998728.699120</td>\n",
       "      <td>1504590.958661</td>\n",
       "      <td>0.764600</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>10208337.498770</td>\n",
       "      <td>31592.656125</td>\n",
       "      <td>-31592.656125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  SP500_Investment  DAX40_Investment  NIKKEI_Investment  \\\n",
       "0  2012-01-04    5177993.527508    3000000.000000   148957298.907646   \n",
       "1  2012-01-05    5189897.180583    2992352.262000   148944667.328699   \n",
       "2  2012-01-06    5179533.786296    2973606.159562   148926943.509066   \n",
       "3  2012-01-09    5188699.359796    2953565.582481   148926943.509066   \n",
       "4  2012-01-10    5224622.209432    3024254.466714   148932706.981779   \n",
       "5  2012-01-11    5225894.770664    3019028.766693   148937338.788967   \n",
       "6  2012-01-12    5235403.197359    3032185.494802   148926138.701090   \n",
       "7  2012-01-13    5214914.028114    3014404.192042   148946848.667790   \n",
       "8  2012-01-16    5214914.028114    3051919.263087   148924774.744817   \n",
       "9  2012-01-17    5229430.909452    3106827.837990   148940684.676353   \n",
       "10 2012-01-18    5274345.466564    3117425.957849   148955611.958593   \n",
       "11 2012-01-19    5294387.103796    3147543.890112   148971047.185971   \n",
       "12 2012-01-20    5297126.329679    3141715.579943   148992818.261720   \n",
       "13 2012-01-23    5299041.659320    3157486.568023   148992740.338476   \n",
       "14 2012-01-24    5294865.797232    3148939.431860   148996003.279489   \n",
       "15 2012-01-25    5329733.806928    3150229.309877   149012273.643047   \n",
       "16 2012-01-26    5306292.070704    3207588.656799   149006611.176649   \n",
       "17 2012-01-27    5299897.352003    3193890.097735   149005240.315826   \n",
       "18 2012-01-30    5289710.716097    3160596.150557   148997119.530229   \n",
       "19 2012-01-31    5287862.089155    3167679.925176   148998728.699120   \n",
       "\n",
       "    EU_BOND_Investment  USD/EUR  JPY/EUR  Portfolio_Value_EUR  \\\n",
       "0       1500000.000000 0.772500 0.010070      10000000.000000   \n",
       "1       1500246.523142 0.782100 0.010120      10058937.403442   \n",
       "2       1500493.291886 0.786100 0.010210      10066275.054084   \n",
       "3       1500738.323863 0.783300 0.010190      10036177.669230   \n",
       "4       1500982.736072 0.782600 0.010190      10131650.828032   \n",
       "5       1501223.999292 0.786900 0.010230      10156138.336831   \n",
       "6       1501460.534432 0.779600 0.010160      10128255.931098   \n",
       "7       1501695.498805 0.789000 0.010260      10158861.526361   \n",
       "8       1501930.470054 0.789500 0.010280      10201971.042714   \n",
       "9       1502166.395712 0.784900 0.010220      10235748.351923   \n",
       "10      1502403.866448 0.777500 0.010130      10229553.773692   \n",
       "11      1502643.509093 0.771300 0.009996      10222862.760034   \n",
       "12      1502885.831405 0.773100 0.010040      10235697.672170   \n",
       "13      1503131.732769 0.767300 0.009964      10211136.630721   \n",
       "14      1503380.337308 0.767800 0.009877      10189351.252674   \n",
       "15      1503627.843063 0.762700 0.009808      10180357.507375   \n",
       "16      1503871.191231 0.763200 0.009851      10229086.083093   \n",
       "17      1504112.070161 0.756000 0.009860      10173916.235525   \n",
       "18      1504350.590111 0.761100 0.009972      10176744.842645   \n",
       "19      1504590.958661 0.764600 0.010020      10208337.498770   \n",
       "\n",
       "    Portfolio_Change_EUR  Portfolio_loss  \n",
       "0               0.000000        0.000000  \n",
       "1           58937.403442   -58937.403442  \n",
       "2            7337.650642    -7337.650642  \n",
       "3          -30097.384854    30097.384854  \n",
       "4           95473.158802   -95473.158802  \n",
       "5           24487.508799   -24487.508799  \n",
       "6          -27882.405733    27882.405733  \n",
       "7           30605.595263   -30605.595263  \n",
       "8           43109.516353   -43109.516353  \n",
       "9           33777.309209   -33777.309209  \n",
       "10          -6194.578231     6194.578231  \n",
       "11          -6691.013658     6691.013658  \n",
       "12          12834.912136   -12834.912136  \n",
       "13         -24561.041449    24561.041449  \n",
       "14         -21785.378047    21785.378047  \n",
       "15          -8993.745298     8993.745298  \n",
       "16          48728.575717   -48728.575717  \n",
       "17         -55169.847568    55169.847568  \n",
       "18           2828.607120    -2828.607120  \n",
       "19          31592.656125   -31592.656125  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate total portfolio value in EUR for each day\n",
    "main_df['Portfolio_Value_EUR'] = (\n",
    "    main_df['SP500_Investment'].fillna(0) * main_df['USD/EUR'] +\n",
    "    main_df['DAX40_Investment'].fillna(0) +\n",
    "    main_df['NIKKEI_Investment'].fillna(0) * main_df['JPY/EUR'] +\n",
    "    main_df['EU_BOND_Investment'].fillna(0)\n",
    ")\n",
    "\n",
    "# First day should be the initial investment amount\n",
    "main_df.loc[0, 'Portfolio_Value_EUR'] = starting_investment\n",
    "\n",
    "# Calculate the daily change in portfolio value (profit/loss)\n",
    "main_df['Portfolio_Change_EUR'] = main_df['Portfolio_Value_EUR'].diff()\n",
    "main_df.loc[0, 'Portfolio_Change_EUR'] = 0.0  # Set the first day's change to 0\n",
    "\n",
    "# Portfolio loss is the negative of the daily change\n",
    "main_df['Portfolio_loss'] = -main_df['Portfolio_Change_EUR']\n",
    "\n",
    "# Set the first day's loss to 0 (there's no previous day to compare with)\n",
    "main_df.loc[0, 'Portfolio_loss'] = 0.0\n",
    "\n",
    "# Display the relevant columns to verify\n",
    "display(main_df[['Date', 'SP500_Investment', 'DAX40_Investment', 'NIKKEI_Investment', \n",
    "                'EU_BOND_Investment', 'USD/EUR', 'JPY/EUR', 'Portfolio_Value_EUR', \n",
    "                'Portfolio_Change_EUR', 'Portfolio_loss']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26176760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Loss Statistics:\n",
      "Minimum Loss: -850226.1277\n",
      "Maximum Loss: 1342732.3566\n",
      "Mean Loss: -3159.9667\n",
      "Number of valid loss values: 2803 out of 2805\n"
     ]
    }
   ],
   "source": [
    "loss_values = main_df['Portfolio_loss'].values\n",
    "\n",
    "# Calculate and print the minimum, maximum, and mean of portfolio loss values\n",
    "min_loss = np.nanmin(loss_values)\n",
    "max_loss = np.nanmax(loss_values)\n",
    "mean_loss = np.nanmean(loss_values)\n",
    "\n",
    "print(f\"Portfolio Loss Statistics:\")\n",
    "print(f\"Minimum Loss: {min_loss:.4f}\")\n",
    "print(f\"Maximum Loss: {max_loss:.4f}\")\n",
    "print(f\"Mean Loss: {mean_loss:.4f}\")\n",
    "\n",
    "# Also print the number of valid loss values (non-NaN)\n",
    "valid_count = np.sum(~np.isnan(loss_values))\n",
    "print(f\"Number of valid loss values: {valid_count} out of {len(loss_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62edd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_loss_variables(time_window, current_date):\n",
    "    # Calculate the mean and standard deviation of portfolio loss from the time windows\n",
    "    loss_dict = {\n",
    "        \"Date\": current_date,\n",
    "        \"Portfolio_mean_loss\": np.nanmean(time_window['Portfolio_loss']),\n",
    "        \"Portfolio_std_loss\": np.nanstd(time_window['Portfolio_loss'])\n",
    "    }\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72986b3d",
   "metadata": {},
   "source": [
    "### Portfolio variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2842c0",
   "metadata": {},
   "source": [
    "## Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab29edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Get the VaR of the normal model\n",
    "\n",
    "    Inputs:\n",
    "        alpha   double, level\n",
    "        r       double, expected return\n",
    "        s       double, volatility\n",
    "        df      (optional) double, degrees of freedom for student-t\n",
    "\n",
    "    Return value:\n",
    "        dVaR    double, VaR\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        dVaR0= st.norm.ppf(alpha)\n",
    "        dVaR = r + s*dVaR0\n",
    "    else:\n",
    "        dVaR0= st.t.ppf(alpha, df= df)\n",
    "\n",
    "        dS2t= df/(df-2)\n",
    "\n",
    "        c = s / np.sqrt(dS2t)\n",
    "        dVaR= r + c*dVaR0\n",
    "    return dVaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12755bb3",
   "metadata": {},
   "source": [
    "## Expected Shortfall (ES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb43cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES(alpha, r= 0, s= 1, df= 0):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Get the ES of the normal/student model\n",
    "\n",
    "    Inputs:\n",
    "        alpha   double, level\n",
    "        r       double, expected return\n",
    "        s       double, volatility\n",
    "        df      (optional, default= 0/normal) double, df\n",
    "\n",
    "    Return value:\n",
    "        dES     double, ES\n",
    "    \"\"\"\n",
    "    if (df == 0):\n",
    "        dVaR0= st.norm.ppf(alpha)\n",
    "        dES0= st.norm.pdf(dVaR0) / (1-alpha)\n",
    "        dES= r + s*dES0\n",
    "    else:\n",
    "        dVaR0= st.t.ppf(alpha, df= df)\n",
    "        dES0= st.t.pdf(dVaR0, df= df)*((df + dVaR0**2)/(df-1)) / (1-alpha)\n",
    "\n",
    "        dS2t= df/(df-2)\n",
    "        c= s / np.sqrt(dS2t)\n",
    "        dES= r + c*dES0\n",
    "    return dES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0d4d6",
   "metadata": {},
   "source": [
    "# performing different methods\n",
    "\n",
    "write method for variance covariance where the sample period is an input parameter alongside other parameters that are needed for the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d7a40",
   "metadata": {},
   "source": [
    "## 1. var/cov multivar normal dist & T-distribution & Historical\n",
    "\n",
    "4 code blocks with functions to calculate components of Var/cov method, 1 code block with for loop to iterate through set window for daily VaR and ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8ad66",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e631c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var_cov(window, current_date, vAlpha, mean_loss, portfolio_std_loss, df=0):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk using variance-covariance method.\n",
    "    \n",
    "    Parameters:\n",
    "    - window: DataFrame containing the time window\n",
    "    - current_date: Current date for reporting\n",
    "    - vAlpha: Confidence levels (array)\n",
    "    - mean_loss: Mean loss for the portfolio\n",
    "    - portfolio_std_loss: Standard deviation of losses\n",
    "    - df: Degrees of freedom for t-distribution (0 for normal distribution)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with date, VaR and ES values\n",
    "    \"\"\"\n",
    "    # Calculate VaR and ES\n",
    "    var_result = VaR(vAlpha, mean_loss, portfolio_std_loss, df=df)\n",
    "    es_result = ES(vAlpha, mean_loss, portfolio_std_loss, df=df)\n",
    "    \n",
    "    # Set label for distribution type\n",
    "    if df == 0:\n",
    "        dist_label = \"Normal\"\n",
    "    else:\n",
    "        dist_label = f\"T{df}\"\n",
    "        \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        f'VaR {dist_label}': var_result,\n",
    "        f'ES {dist_label}': es_result\n",
    "    }\n",
    "\n",
    "def calculate_historical_var_es(window, current_date, vAlpha):\n",
    "    \"\"\"\n",
    "    Calculate VaR and ES using historical simulation method.\n",
    "    \"\"\"\n",
    "    # Extract portfolio loss values from the window\n",
    "    historical_losses = window['Portfolio_loss'].dropna()\n",
    "    \n",
    "    # Sort losses in ascending order\n",
    "    sorted_losses = np.sort(historical_losses)\n",
    "    \n",
    "    # Calculate VaR for alpha levels\n",
    "    var_hist = np.percentile(sorted_losses, 100*vAlpha)\n",
    "    \n",
    "    # Calculate ES for each alpha level\n",
    "    es_hist = []\n",
    "    for i, alpha in enumerate(vAlpha):\n",
    "        es_hist.append(sorted_losses[sorted_losses >= var_hist[i]].mean())\n",
    "    \n",
    "    return {\n",
    "        'Date': current_date,\n",
    "        'VaR Historical': var_hist,\n",
    "        'ES Historical': es_hist\n",
    "    }\n",
    "\n",
    "def calculate_multiday_var(vAlpha, interval, var_results):\n",
    "    \"\"\"\n",
    "    Calculate multi-day VaR using the historical simulation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - vAlpha: Confidence levels (array)\n",
    "    - interval: Number of days for the multi-day calculation (e.g., 5 or 10)\n",
    "    \n",
    "    Returns:\n",
    "    - var_multi: VaR values at specified confidence levels\n",
    "    \"\"\"\n",
    "    # Filter data for the period we want to analyze\n",
    "    time_window_multi = main_df[(main_df['Date'] >= '2012-01-05') & (main_df['Date'] <= '2021-12-31')]\n",
    "    \n",
    "    # Create a new DataFrame for storing multi-day losses\n",
    "    multi_day_losses = []\n",
    "    \n",
    "    # Calculate rolling sum of losses over the interval\n",
    "    for i in range(0, len(time_window_multi) - interval + 1, interval):\n",
    "        window = time_window_multi.iloc[i:i+interval]\n",
    "        if len(window) == interval:  # Ensure we have a complete window\n",
    "            total_loss = window['Portfolio_loss'].sum()\n",
    "            date = window['Date'].iloc[-1]\n",
    "            multi_day_losses.append({'Date': date, 'Portfolio_loss': total_loss})\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    multi_day_losses_df = pd.DataFrame(multi_day_losses)\n",
    "\n",
    "    # Calculate VaR as percentiles of the multi-day losses for each row\n",
    "    var_list = []\n",
    "    for idx, row in multi_day_losses_df.iterrows():\n",
    "        var_95 = np.percentile(multi_day_losses_df['Portfolio_loss'][:idx+1], vAlpha[0]*100)\n",
    "        var_99 = np.percentile(multi_day_losses_df['Portfolio_loss'][:idx+1], vAlpha[1]*100)\n",
    "        var_list.append([var_95, var_99])\n",
    "    multi_day_losses_df['VaR'] = var_list\n",
    "\n",
    "    return multi_day_losses_df[['Date', 'Portfolio_loss', 'VaR']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5067a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize lists to store results\n",
    "    VaR_results = []\n",
    "    ES_results = []\n",
    "    losses = []\n",
    "    # Define time window\n",
    "    time_window = main_df[(main_df['Date'] >= '2012-01-05') & (main_df['Date'] <= '2021-12-31')]\n",
    "\n",
    "    # Define confidence levels\n",
    "    vAlpha = np.array([0.95, 0.99])\n",
    "    \n",
    "    # Define sample size and t-distribution degrees of freedom\n",
    "    sample_size = 500\n",
    "    degrees_of_freedom = [0, 3, 4, 5, 6]  # 0 represents normal distribution\n",
    "    \n",
    "    for i in range(sample_size, len(time_window)):\n",
    "        # Extract the rolling window\n",
    "        window = time_window.iloc[i - sample_size:i]\n",
    "        current_date = time_window.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate loss statistics\n",
    "        loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "        mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "        portfolio_std_loss = loss_stats[\"Portfolio_std_loss\"]\n",
    "        \n",
    "        # Initialize result dictionaries for this date\n",
    "        var_row = {'Date': current_date}\n",
    "        es_row = {'Date': current_date}\n",
    "\n",
    "        \n",
    "        # Calculate VaR and ES using various distributions\n",
    "        for df in degrees_of_freedom:\n",
    "            results = calculate_var_cov(window, current_date, vAlpha, mean_loss, portfolio_std_loss, df)\n",
    "            \n",
    "            # Get the distribution label\n",
    "            if df == 0:\n",
    "                dist_label = \"Normal\"\n",
    "            else:\n",
    "                dist_label = f\"T{df}\"\n",
    "                \n",
    "            # Add results to the dictionaries\n",
    "            var_row[f'VaR {dist_label}'] = results[f'VaR {dist_label}']\n",
    "            es_row[f'ES {dist_label}'] = results[f'ES {dist_label}']\n",
    "        \n",
    "        # Calculate VaR and ES using historical simulation\n",
    "        hist_results = calculate_historical_var_es(window, current_date, vAlpha)\n",
    "        var_row['VaR Historical'] = hist_results['VaR Historical']\n",
    "        es_row['ES Historical'] = hist_results['ES Historical']\n",
    "        \n",
    "        # Add results for this date\n",
    "        VaR_results.append(var_row)\n",
    "        ES_results.append(es_row)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    var_results_df = pd.DataFrame(VaR_results)\n",
    "    es_results_df = pd.DataFrame(ES_results)\n",
    "\n",
    "    var_5d = calculate_multiday_var(vAlpha, 5, var_results_df)\n",
    "    var_10d = calculate_multiday_var(vAlpha, 10, var_results_df)\n",
    "\n",
    "    return var_results_df, es_results_df, var_5d, var_10d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1641b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>VaR Normal</th>\n",
       "      <th>VaR T3</th>\n",
       "      <th>VaR T4</th>\n",
       "      <th>VaR T5</th>\n",
       "      <th>VaR T6</th>\n",
       "      <th>VaR Historical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-18</td>\n",
       "      <td>[81302.63724237992, 116557.82173332498]</td>\n",
       "      <td>[66500.06233789833, 131830.61781966424]</td>\n",
       "      <td>[74194.10217215758, 133274.76768026515]</td>\n",
       "      <td>[76956.93381443433, 131048.8178940729]</td>\n",
       "      <td>[78289.05285881073, 128954.41137094288]</td>\n",
       "      <td>[76635.87745611469, 124145.5344469194]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-19</td>\n",
       "      <td>[81540.3660070524, 116948.8599203884]</td>\n",
       "      <td>[66673.4211597873, 132288.0707363653]</td>\n",
       "      <td>[74400.91901787695, 133738.50057490906]</td>\n",
       "      <td>[77175.76501008144, 131502.87110384434]</td>\n",
       "      <td>[78513.67685923811, 129399.35692012627]</td>\n",
       "      <td>[76635.87745611469, 124145.5344469194]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-20</td>\n",
       "      <td>[81583.82165663296, 117072.39289918296]</td>\n",
       "      <td>[66683.25478523278, 132446.29378239065]</td>\n",
       "      <td>[74428.22860238179, 133900.0038098057]</td>\n",
       "      <td>[77209.34998874959, 131659.3183982767]</td>\n",
       "      <td>[78550.28756410255, 129551.04705662336]</td>\n",
       "      <td>[76635.87745611469, 124145.5344469194]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-23</td>\n",
       "      <td>[81449.94297045414, 116946.14401575756]</td>\n",
       "      <td>[66546.17257797203, 132323.3501836578]</td>\n",
       "      <td>[74292.81151210057, 133777.37274889508]</td>\n",
       "      <td>[77074.53082075343, 131536.20560515978]</td>\n",
       "      <td>[78415.75668861427, 129427.48099942949]</td>\n",
       "      <td>[76635.87745611469, 124145.5344469194]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-24</td>\n",
       "      <td>[81344.2515656486, 116759.42526174705]</td>\n",
       "      <td>[66474.5020816722, 132101.5298070153]</td>\n",
       "      <td>[74203.4577257698, 133552.23326793505]</td>\n",
       "      <td>[76978.82719034994, 131316.18204724038]</td>\n",
       "      <td>[78316.99143545798, 129212.27103733084]</td>\n",
       "      <td>[76635.87745611469, 124145.5344469194]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                               VaR Normal  \\\n",
       "0 2013-12-18  [81302.63724237992, 116557.82173332498]   \n",
       "1 2013-12-19    [81540.3660070524, 116948.8599203884]   \n",
       "2 2013-12-20  [81583.82165663296, 117072.39289918296]   \n",
       "3 2013-12-23  [81449.94297045414, 116946.14401575756]   \n",
       "4 2013-12-24   [81344.2515656486, 116759.42526174705]   \n",
       "\n",
       "                                    VaR T3  \\\n",
       "0  [66500.06233789833, 131830.61781966424]   \n",
       "1    [66673.4211597873, 132288.0707363653]   \n",
       "2  [66683.25478523278, 132446.29378239065]   \n",
       "3   [66546.17257797203, 132323.3501836578]   \n",
       "4    [66474.5020816722, 132101.5298070153]   \n",
       "\n",
       "                                    VaR T4  \\\n",
       "0  [74194.10217215758, 133274.76768026515]   \n",
       "1  [74400.91901787695, 133738.50057490906]   \n",
       "2   [74428.22860238179, 133900.0038098057]   \n",
       "3  [74292.81151210057, 133777.37274889508]   \n",
       "4   [74203.4577257698, 133552.23326793505]   \n",
       "\n",
       "                                    VaR T5  \\\n",
       "0   [76956.93381443433, 131048.8178940729]   \n",
       "1  [77175.76501008144, 131502.87110384434]   \n",
       "2   [77209.34998874959, 131659.3183982767]   \n",
       "3  [77074.53082075343, 131536.20560515978]   \n",
       "4  [76978.82719034994, 131316.18204724038]   \n",
       "\n",
       "                                    VaR T6  \\\n",
       "0  [78289.05285881073, 128954.41137094288]   \n",
       "1  [78513.67685923811, 129399.35692012627]   \n",
       "2  [78550.28756410255, 129551.04705662336]   \n",
       "3  [78415.75668861427, 129427.48099942949]   \n",
       "4  [78316.99143545798, 129212.27103733084]   \n",
       "\n",
       "                           VaR Historical  \n",
       "0  [76635.87745611469, 124145.5344469194]  \n",
       "1  [76635.87745611469, 124145.5344469194]  \n",
       "2  [76635.87745611469, 124145.5344469194]  \n",
       "3  [76635.87745611469, 124145.5344469194]  \n",
       "4  [76635.87745611469, 124145.5344469194]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ES Normal</th>\n",
       "      <th>ES T3</th>\n",
       "      <th>ES T4</th>\n",
       "      <th>ES T5</th>\n",
       "      <th>ES T6</th>\n",
       "      <th>ES Historical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-18</td>\n",
       "      <td>[102919.40446331202, 134088.11726814165]</td>\n",
       "      <td>[111925.80079269716, 205375.9477153037]</td>\n",
       "      <td>[113372.33536064536, 187180.662555394]</td>\n",
       "      <td>[112022.79145338114, 174626.62119015018]</td>\n",
       "      <td>[110710.06207567592, 166541.3106481568]</td>\n",
       "      <td>[111708.34702453166, 164482.1976745602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-19</td>\n",
       "      <td>[103251.13512144932, 134555.38706543748]</td>\n",
       "      <td>[112296.696340737, 206153.21719823903]</td>\n",
       "      <td>[113749.52125668032, 187878.80867822265]</td>\n",
       "      <td>[112394.1087715062, 175270.17526196092]</td>\n",
       "      <td>[111075.67090622509, 167149.7052304643]</td>\n",
       "      <td>[111708.34702453166, 164482.1976745602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-20</td>\n",
       "      <td>[103343.69030093697, 134718.73771266945]</td>\n",
       "      <td>[112409.70831755288, 206478.48840127286]</td>\n",
       "      <td>[113865.81883890402, 188162.75177984586]</td>\n",
       "      <td>[112507.341049341, 175525.6035758192]</td>\n",
       "      <td>[111185.92149883926, 167386.76886737032]</td>\n",
       "      <td>[111708.34702453166, 164482.1976745602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-23</td>\n",
       "      <td>[103214.48983932125, 134596.2826740096]</td>\n",
       "      <td>[112282.45698848333, 206371.46122358963]</td>\n",
       "      <td>[113738.88056374664, 188051.78684268647]</td>\n",
       "      <td>[112380.11071064833, 175411.92173728038]</td>\n",
       "      <td>[111058.40706388204, 167271.33723442463]</td>\n",
       "      <td>[111708.34702453166, 164482.1976745602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-24</td>\n",
       "      <td>[103059.11639848091, 134369.27386341666]</td>\n",
       "      <td>[112106.38405532726, 205980.61086699113]</td>\n",
       "      <td>[113559.4830454759, 187702.7548950915]</td>\n",
       "      <td>[112203.81486287006, 175091.7428706811]</td>\n",
       "      <td>[110885.12827538834, 166969.74091930466]</td>\n",
       "      <td>[111708.34702453166, 164482.1976745602]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                                 ES Normal  \\\n",
       "0 2013-12-18  [102919.40446331202, 134088.11726814165]   \n",
       "1 2013-12-19  [103251.13512144932, 134555.38706543748]   \n",
       "2 2013-12-20  [103343.69030093697, 134718.73771266945]   \n",
       "3 2013-12-23   [103214.48983932125, 134596.2826740096]   \n",
       "4 2013-12-24  [103059.11639848091, 134369.27386341666]   \n",
       "\n",
       "                                      ES T3  \\\n",
       "0   [111925.80079269716, 205375.9477153037]   \n",
       "1    [112296.696340737, 206153.21719823903]   \n",
       "2  [112409.70831755288, 206478.48840127286]   \n",
       "3  [112282.45698848333, 206371.46122358963]   \n",
       "4  [112106.38405532726, 205980.61086699113]   \n",
       "\n",
       "                                      ES T4  \\\n",
       "0    [113372.33536064536, 187180.662555394]   \n",
       "1  [113749.52125668032, 187878.80867822265]   \n",
       "2  [113865.81883890402, 188162.75177984586]   \n",
       "3  [113738.88056374664, 188051.78684268647]   \n",
       "4    [113559.4830454759, 187702.7548950915]   \n",
       "\n",
       "                                      ES T5  \\\n",
       "0  [112022.79145338114, 174626.62119015018]   \n",
       "1   [112394.1087715062, 175270.17526196092]   \n",
       "2     [112507.341049341, 175525.6035758192]   \n",
       "3  [112380.11071064833, 175411.92173728038]   \n",
       "4   [112203.81486287006, 175091.7428706811]   \n",
       "\n",
       "                                      ES T6  \\\n",
       "0   [110710.06207567592, 166541.3106481568]   \n",
       "1   [111075.67090622509, 167149.7052304643]   \n",
       "2  [111185.92149883926, 167386.76886737032]   \n",
       "3  [111058.40706388204, 167271.33723442463]   \n",
       "4  [110885.12827538834, 166969.74091930466]   \n",
       "\n",
       "                             ES Historical  \n",
       "0  [111708.34702453166, 164482.1976745602]  \n",
       "1  [111708.34702453166, 164482.1976745602]  \n",
       "2  [111708.34702453166, 164482.1976745602]  \n",
       "3  [111708.34702453166, 164482.1976745602]  \n",
       "4  [111708.34702453166, 164482.1976745602]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-day VaR results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Portfolio_loss</th>\n",
       "      <th>VaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>-156138.336831</td>\n",
       "      <td>[-156138.33683110587, -156138.33683110587]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>-73415.436861</td>\n",
       "      <td>[-77551.58185954868, -74242.66586074626]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-25</td>\n",
       "      <td>49196.266317</td>\n",
       "      <td>[36935.09599924124, 46744.03225348897]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>-98575.554752</td>\n",
       "      <td>[30804.510840336377, 45517.915221707975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-02-08</td>\n",
       "      <td>-69012.145047</td>\n",
       "      <td>[25554.584044278392, 44467.9298624964]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Portfolio_loss                                         VaR\n",
       "0 2012-01-11  -156138.336831  [-156138.33683110587, -156138.33683110587]\n",
       "1 2012-01-18   -73415.436861    [-77551.58185954868, -74242.66586074626]\n",
       "2 2012-01-25    49196.266317      [36935.09599924124, 46744.03225348897]\n",
       "3 2012-02-01   -98575.554752    [30804.510840336377, 45517.915221707975]\n",
       "4 2012-02-08   -69012.145047      [25554.584044278392, 44467.9298624964]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-day VaR results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Portfolio_loss</th>\n",
       "      <th>VaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>-229553.773692</td>\n",
       "      <td>[-229553.77369215153, -229553.77369215153]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>-49379.288435</td>\n",
       "      <td>[-58388.012698122126, -51181.033287847196]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-02-15</td>\n",
       "      <td>-112686.663738</td>\n",
       "      <td>[-55710.02596558259, -50645.43594133929]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-02-29</td>\n",
       "      <td>56884.906958</td>\n",
       "      <td>[40945.27764888533, 53696.981096061405]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-03-14</td>\n",
       "      <td>-256065.927532</td>\n",
       "      <td>[35632.06787922865, 52634.33914213009]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Portfolio_loss                                         VaR\n",
       "0 2012-01-18  -229553.773692  [-229553.77369215153, -229553.77369215153]\n",
       "1 2012-02-01   -49379.288435  [-58388.012698122126, -51181.033287847196]\n",
       "2 2012-02-15  -112686.663738    [-55710.02596558259, -50645.43594133929]\n",
       "3 2012-02-29    56884.906958     [40945.27764888533, 53696.981096061405]\n",
       "4 2012-03-14  -256065.927532      [35632.06787922865, 52634.33914213009]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_results_df, es_results_df, var_5d, var_10d = main()\n",
    "print(\"VaR results\")\n",
    "display(var_results_df.head())\n",
    "print(\"ES results\")\n",
    "display(es_results_df.head())\n",
    "print(\"5-day VaR results\")\n",
    "display(var_5d.head())\n",
    "print(\"10-day VaR results\")\n",
    "display(var_10d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Historical VaR and ES over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract the values for different confidence levels from the arrays\n",
    "var_95 = [row[0] for row in var_results_df['VaR Historical']]\n",
    "var_99 = [row[1] for row in var_results_df['VaR Historical']]\n",
    "es_95 = [row[0] for row in es_results_df['ES Historical']]\n",
    "es_99 = [row[1] for row in es_results_df['ES Historical']]\n",
    "\n",
    "# Plot VaR Historical 95% and 99%\n",
    "plt.plot(var_results_df['Date'], var_95, label='VaR Historical 95%', color='blue')\n",
    "plt.plot(var_results_df['Date'], var_99, label='VaR Historical 99%', color='red')\n",
    "\n",
    "# Plot ES Historical 95% and 99%\n",
    "plt.plot(es_results_df['Date'], es_95, label='ES Historical 95%', color='green', linestyle='--')\n",
    "plt.plot(es_results_df['Date'], es_99, label='ES Historical 99%', color='orange', linestyle='--')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Historical VaR and ES Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for all indices\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# S&P 500\n",
    "sp500_returns = main_df['C_S&P500_Returns'].dropna()\n",
    "mu_sp500 = sp500_returns.mean()\n",
    "sigma_sp500 = sp500_returns.std()\n",
    "x_sp500 = np.linspace(mu_sp500 - 4*sigma_sp500, mu_sp500 + 4*sigma_sp500, 100)\n",
    "ax1.hist(sp500_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax1.plot(x_sp500, st.norm.pdf(x_sp500, mu_sp500, sigma_sp500), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_sp500 / np.sqrt(df/(df-2))\n",
    "    ax1.plot(x_sp500, st.t.pdf((x_sp500-mu_sp500)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax1.set_title('S&P500 Returns Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# DAX40\n",
    "dax_returns = main_df['C_Dax40_Returns'].dropna()\n",
    "mu_dax = dax_returns.mean()\n",
    "sigma_dax = dax_returns.std()\n",
    "x_dax = np.linspace(mu_dax - 4*sigma_dax, mu_dax + 4*sigma_dax, 100)\n",
    "ax2.hist(dax_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax2.plot(x_dax, st.norm.pdf(x_dax, mu_dax, sigma_dax), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_dax / np.sqrt(df/(df-2))\n",
    "    ax2.plot(x_dax, st.t.pdf((x_dax-mu_dax)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax2.set_title('DAX40 Returns Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# NIKKEI\n",
    "nikkei_returns = main_df['C_Nikkei_Returns'].dropna()\n",
    "mu_nikkei = nikkei_returns.mean()\n",
    "sigma_nikkei = nikkei_returns.std()\n",
    "x_nikkei = np.linspace(mu_nikkei - 4*sigma_nikkei, mu_nikkei + 4*sigma_nikkei, 100)\n",
    "ax3.hist(nikkei_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax3.plot(x_nikkei, st.norm.pdf(x_nikkei, mu_nikkei, sigma_nikkei), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_nikkei / np.sqrt(df/(df-2))\n",
    "    ax3.plot(x_nikkei, st.t.pdf((x_nikkei-mu_nikkei)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax3.set_title('NIKKEI Returns Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# EU Bond\n",
    "bond_returns = main_df['Interest_Bond_daily_rate'].dropna()\n",
    "mu_bond = bond_returns.mean()\n",
    "sigma_bond = bond_returns.std()\n",
    "x_bond = np.linspace(mu_bond - 4*sigma_bond, mu_bond + 4*sigma_bond, 100)\n",
    "ax4.hist(bond_returns, bins=500, density=True, alpha=0.3, color='grey', label='Histogram')\n",
    "ax4.plot(x_bond, st.norm.pdf(x_bond, mu_bond, sigma_bond), 'r-', lw=2, label='Normal')\n",
    "# Add t-distributions\n",
    "for df in [3, 4, 5, 6]:\n",
    "    s = sigma_bond / np.sqrt(df/(df-2))\n",
    "    ax4.plot(x_bond, st.t.pdf((x_bond-mu_bond)/s, df)/s, '--', lw=1, label=f't-dist (df={df})')\n",
    "ax4.set_title('EU Bond Returns Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d31b7",
   "metadata": {},
   "source": [
    "## GARCH(1,1) with constant conditional correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830587c3",
   "metadata": {},
   "source": [
    "## EWMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef66365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ewma_volatility(loss_series, lambda_=0.94):\n",
    "    weights = (1 - lambda_) * (lambda_ ** np.arange(len(loss_series)-1, -1, -1))\n",
    "    weighted_squared_losses = weights * (loss_series ** 2)\n",
    "    ewma_variance = weighted_squared_losses.sum()\n",
    "    return np.sqrt(ewma_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bb975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ewma_vol_series(loss_series, lambda_=0.94):\n",
    "    ewma_var = [loss_series.iloc[0] ** 2]  # Seed with squared first value\n",
    "    for t in range(1, len(loss_series)):\n",
    "        prev_var = ewma_var[-1]\n",
    "        new_var = lambda_ * prev_var + (1 - lambda_) * (loss_series.iloc[t - 1] ** 2)\n",
    "        ewma_var.append(new_var)\n",
    "    return pd.Series(np.sqrt(ewma_var), index=loss_series.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_ewma():\n",
    "    # Initialize lists to store results\n",
    "    mean_losses = []\n",
    "    portfolio_std_losses = []\n",
    "    ewma_vol_94 = []  # For λ = 0.94\n",
    "    ewma_vol_97 = []  # For λ = 0.97\n",
    "    VaR_results = []\n",
    "    ES_results = []\n",
    "\n",
    "    time_window = main_df[(main_df['Date'] >= '2012-01-04') & (main_df['Date'] <= '2021-12-31')]\n",
    "    vAlpha = np.array([0.95, 0.99])\n",
    "    sample_size = 500\n",
    "    degrees_of_freedom = [0, 3, 4, 5, 6]\n",
    "\n",
    "    fhs_results = []\n",
    "\n",
    "\n",
    "    for i in range(sample_size, len(time_window)):\n",
    "        window = time_window.iloc[i - sample_size:i]\n",
    "        current_date = time_window.iloc[i]['Date']\n",
    "        \n",
    "        loss_stats = calculate_daily_loss_variables(window, current_date)\n",
    "        mean_loss = loss_stats[\"Portfolio_mean_loss\"]\n",
    "        std_loss = loss_stats[\"Portfolio_std_loss\"]\n",
    "        \n",
    "        # Calculate EWMA volatilities for both lambda values\n",
    "        ewma_94 = calculate_ewma_volatility(window['Portfolio_loss'], lambda_=0.94)\n",
    "        ewma_97 = calculate_ewma_volatility(window['Portfolio_loss'], lambda_=0.97)\n",
    "\n",
    "        # --- Filtered Historical Simulation (FHS) using EWMA filtered losses ---\n",
    "        factors = ['C_S&P500_Loss', 'C_Dax40_Loss', 'C_Nikkei_Loss', 'Interest_Bond_Loss']\n",
    "        filtered_losses = []\n",
    "\n",
    "        for factor in factors:\n",
    "            series = window[factor]\n",
    "            vol_series = calculate_ewma_vol_series(series, lambda_=0.94)\n",
    "            current_vol = vol_series.iloc[-1]\n",
    "\n",
    "            normalized = series / vol_series\n",
    "            normalized = normalized.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "            filtered = normalized * current_vol\n",
    "            filtered_losses.append(filtered)\n",
    "\n",
    "        # Combine into filtered portfolio loss\n",
    "        filtered_portfolio_loss = np.sum(filtered_losses, axis=0)\n",
    "\n",
    "        # Calculate FHS VaR and ES\n",
    "        FHS_VaR_95 = np.percentile(filtered_portfolio_loss, 5)\n",
    "        FHS_VaR_99 = np.percentile(filtered_portfolio_loss, 1)\n",
    "        FHS_ES_95 = filtered_portfolio_loss[filtered_portfolio_loss <= FHS_VaR_95].mean()\n",
    "        FHS_ES_99 = filtered_portfolio_loss[filtered_portfolio_loss <= FHS_VaR_99].mean()\n",
    "\n",
    "        # Store results\n",
    "        fhs_results.append({\n",
    "            'Date': current_date,\n",
    "            'FHS_VaR_95': FHS_VaR_95,\n",
    "            'FHS_VaR_99': FHS_VaR_99,\n",
    "            'FHS_ES_95': FHS_ES_95,\n",
    "            'FHS_ES_99': FHS_ES_99\n",
    "        })\n",
    "\n",
    "        \n",
    "        # Append everything\n",
    "        mean_losses.append(mean_loss)\n",
    "        portfolio_std_losses.append(std_loss)\n",
    "        ewma_vol_94.append(ewma_94)\n",
    "        ewma_vol_97.append(ewma_97)\n",
    "        \n",
    "        var_row = {'Date': current_date}\n",
    "        es_row = {'Date': current_date}\n",
    "        \n",
    "        for df in degrees_of_freedom:\n",
    "            current_var = VaR(vAlpha, mean_loss, std_loss, df)\n",
    "            current_es = ES(vAlpha, mean_loss, std_loss, df)\n",
    "            \n",
    "            var_key = f\"VaR {'Normal' if df == 0 else f'T{df}'}\"\n",
    "            es_key = f\"ES {'Normal' if df == 0 else f'T{df}'}\"\n",
    "            var_row[var_key] = current_var\n",
    "            es_row[es_key] = current_es\n",
    "\n",
    "        # Historical simulation\n",
    "        # Extract the portfolio loss values from the window\n",
    "        historical_losses = window['Portfolio_loss'].dropna()  # Remove NaN values directly\n",
    "        \n",
    "        # Sort the losses in ascending order\n",
    "        sorted_losses = np.sort(historical_losses)\n",
    "\n",
    "        # Calculate VaR for alpha levels\n",
    "        var_95 = np.percentile(sorted_losses, 95) \n",
    "        var_99 = np.percentile(sorted_losses, 99)  \n",
    "\n",
    "        # Calculate ES\n",
    "        es_95 = sorted_losses[sorted_losses >= var_95].mean()  # Mean of losses below VaR 95\n",
    "        es_99 = sorted_losses[sorted_losses >= var_99].mean()  # Mean of losses below VaR 99\n",
    "\n",
    "        # Add to the row dictionaries\n",
    "        var_row['VaR Historical'] = [var_95, var_99]\n",
    "        es_row['ES Historical'] = [es_95, es_99]\n",
    "        \n",
    "        VaR_results.append(var_row)\n",
    "        ES_results.append(es_row)\n",
    "\n",
    "    # Final DataFrames\n",
    "    mean_losses_df = pd.DataFrame(mean_losses, columns=['Mean_Loss'])\n",
    "    portfolio_std_losses_df = pd.DataFrame(portfolio_std_losses, columns=['Std_Loss'])\n",
    "    ewma_94_df = pd.DataFrame(ewma_vol_94, columns=['EWMA_0.94'])\n",
    "    ewma_97_df = pd.DataFrame(ewma_vol_97, columns=['EWMA_0.97'])\n",
    "    var_results_df = pd.DataFrame(VaR_results)\n",
    "    es_results_df = pd.DataFrame(ES_results)\n",
    "    fhs_df = pd.DataFrame(fhs_results)\n",
    "\n",
    "\n",
    "    return mean_losses_df, portfolio_std_losses_df, ewma_94_df, ewma_97_df, var_results_df, es_results_df, fhs_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b019ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the returned DataFrames\n",
    "mean_loss_df, portfolio_std_losses_df, var_results_df, es_results_df, ewma_94_df, ewma_97_df, fhs_df  = main_ewma()\n",
    "\n",
    "fhs_df[['FHS_VaR_95', 'FHS_VaR_99', 'FHS_ES_95', 'FHS_ES_99']] = fhs_df[\n",
    "    ['FHS_VaR_95', 'FHS_VaR_99', 'FHS_ES_95', 'FHS_ES_99']].abs()\n",
    "\n",
    "\n",
    "# Display the results\n",
    "\n",
    "print(\"Mean Loss Results Head:\")\n",
    "display(mean_loss_df.head())\n",
    "print(\"\\nPortfolio Std Loss Results Head:\")\n",
    "display(portfolio_std_losses_df.head())\n",
    "print(\"\\nFHS Results Head:\")\n",
    "display(fhs_df.head())\n",
    "print(\"\\nVaR Results Head:\")\n",
    "display(var_results_df.head())\n",
    "print(\"\\nES Results Head:\")\n",
    "display(es_results_df.head())\n",
    "print(\"\\nEWMA Volatility Results Head:\")\n",
    "display(ewma_94_df.head())\n",
    "display(ewma_97_df.head())\n",
    "\n",
    "# Extract the actual losses aligned with fhs_df\n",
    "aligned_losses = main_df.loc[main_df['Date'].isin(fhs_df['Date']), 'Portfolio_loss'].values\n",
    "\n",
    "# Create a figure for comparing VaR, ES and actual portfolio losses\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Plot actual portfolio losses\n",
    "ax.plot(fhs_df['Date'], aligned_losses, label='Actual Portfolio Losses', color='black', alpha=0.5)\n",
    "\n",
    "\n",
    "# Plot VaR values\n",
    "ax.plot(fhs_df['Date'], fhs_df['FHS_VaR_95'], label='95% VaR', color='blue')\n",
    "ax.plot(fhs_df['Date'], fhs_df['FHS_VaR_99'], label='99% VaR', color='red')\n",
    "\n",
    "# Plot ES values\n",
    "ax.plot(fhs_df['Date'], fhs_df['FHS_ES_95'], label='95% ES', color='blue', linestyle='--')\n",
    "ax.plot(fhs_df['Date'], fhs_df['FHS_ES_99'], label='99% ES', color='red', linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Portfolio Losses with VaR and ES Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum likelihood estimation of GARCH(1,1) parameters\n",
    "def garch_likelihood(params, returns):\n",
    "    omega, alpha, beta = params\n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "    var[0] = omega / (1 - alpha - beta)\n",
    "    ll = 0\n",
    "    for t in range(2, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "        ll += 0.5 * (np.log(2 * np.pi) + np.log(var[t]) + returns[t]**2 / var[t])\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ff295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) parameter estimation using MLE\n",
    "# This function estimates the parameters of a GARCH(1,1) model using maximum likelihood estimation (MLE).\n",
    "def parameter_estimation_GARCH(returns):\n",
    "    # Run the optimization\n",
    "    result = minimize(\n",
    "        garch_likelihood,\n",
    "        x0=[0.00002, 0.05, 0.9],\n",
    "        args=(returns,),\n",
    "        method='SLSQP',\n",
    "        bounds=[(1e-6, None), (0, 0.99), (0, 0.99)],\n",
    "        constraints=[\n",
    "            {'type': 'ineq', 'fun': lambda x: 0.999 - x[1] - x[2]}\n",
    "        ],\n",
    "        options={'disp': True}\n",
    "    )\n",
    "\n",
    "    # Return the optimization result\n",
    "    return result\n",
    "\n",
    "# # Check the optimization result\n",
    "# if result.success:\n",
    "#     print(f\"Optimized parameters: omega={result.x[0]}, alpha={result.x[1]}, beta={result.x[2]}\")\n",
    "# else:\n",
    "#     print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the GARCH(1,1) volatility for returns t\n",
    "def garch_volatility(returns):\n",
    "    param = parameter_estimation_GARCH(returns)\n",
    "    omega, alpha, beta = param.x\n",
    "    T = len(returns)\n",
    "    var = np.zeros(T)\n",
    "    var[0]= param.x[0] / (1 - param.x[1] - param.x[2])\n",
    "    for t in range(2, T):\n",
    "        var[t] = omega + alpha * returns[t-1]**2 + beta * var[t-1]\n",
    "    return np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d45979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation matrix of the 4 time series\n",
    "# the method must take the returns as matrix of 4 vectors and return the correlation matrix\n",
    "def correlation_matrix(returns):\n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.cov(returns.T)\n",
    "    \n",
    "    # Calculate the standard deviations of each asset\n",
    "    std_devs = np.sqrt(np.diag(cov_matrix))\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = cov_matrix / np.outer(std_devs, std_devs)\n",
    "    \n",
    "    return corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fdfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the diagonal matrix with the garch volatility of the 4 time series\n",
    "def diagonal_matrix(returns):\n",
    "    # Initialize an empty dictionary to store volatilities for each column\n",
    "    volatilities_dict = {}\n",
    "\n",
    "    # Iterate through each column in the returns DataFrame\n",
    "    for column in returns.columns:\n",
    "        # Reset the index of the column to ensure numeric indexing\n",
    "        column_returns = returns[column].dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the GARCH(1,1) volatility for the column\n",
    "        volatilities_dict[column] = garch_volatility(column_returns)\n",
    "        \n",
    "    # Combine the volatilities into a single array\n",
    "    volatilities = np.array([vol[-1] for vol in volatilities_dict.values()])\n",
    "\n",
    "    # Create a diagonal matrix with the volatilities\n",
    "    #diag_matrix = np.diag(volatilities)\n",
    "    \n",
    "    return volatilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8799372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the covariance matrix of the 4 time series\n",
    "def covariance_matrix(returns):\n",
    "    # Calculate the GARCH(1,1) volatilities\n",
    "    volatilities = diagonal_matrix(returns)\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = correlation_matrix(returns)\n",
    "    \n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.outer(volatilities, volatilities) * corr_matrix\n",
    "    \n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the portfolio variance and volatility\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    # Calculate the portfolio variance\n",
    "    port_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    \n",
    "    # Calculate the portfolio volatility\n",
    "    port_volatility = np.sqrt(port_variance)\n",
    "    \n",
    "    return port_variance, port_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62efc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the VaR and ES of the portfolio using the normal distribution\n",
    "def portfolio_VaR_ES(weights, cov_matrix, alpha=0.95):\n",
    "    # Calculate the portfolio variance and volatility\n",
    "    _, port_volatility = portfolio_variance(weights, cov_matrix)\n",
    "    \n",
    "    # Calculate the VaR using the normal distribution\n",
    "    VaR = -port_volatility * st.norm.ppf(1 - alpha)\n",
    "    \n",
    "    # Calculate the ES using the normal distribution\n",
    "    ES = -port_volatility * (st.norm.pdf(st.norm.ppf(1 - alpha)) / (1 - alpha))\n",
    "    \n",
    "    return VaR, ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to run the analysis\n",
    "def main_analysis(time_window_size):\n",
    "    # Initialize lists to store results\n",
    "    portfolio_VaR_list = []\n",
    "    portfolio_ES_list = []\n",
    "    dates = []\n",
    "\n",
    "    # Iterate through the dataset with a rolling time window\n",
    "    for i in range(time_window_size, len(main_df)):\n",
    "        # Extract the rolling time window\n",
    "        window = main_df.iloc[i - time_window_size:i]\n",
    "        current_date = main_df.iloc[i]['Date']\n",
    "        \n",
    "        # Calculate the returns for each asset in the window\n",
    "        returns = window[['C_S&P500_Returns', 'C_Dax40_Returns', 'C_Nikkei_Returns', 'Interest_Bond_daily_rate']].dropna()\n",
    "\n",
    "        # Define the weights for the portfolio\n",
    "        weights = np.array([0.4, 0.3, 0.15, 0.15])\n",
    "\n",
    "        # Calculate the covariance matrix\n",
    "        cov_matrix = covariance_matrix(returns)\n",
    "\n",
    "        # Calculate the portfolio VaR and ES\n",
    "        VaR, ES = portfolio_VaR_ES(weights, cov_matrix)\n",
    "\n",
    "        # Append results\n",
    "        portfolio_VaR_list.append(VaR)\n",
    "        portfolio_ES_list.append(ES)\n",
    "        dates.append(current_date)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Portfolio VaR': portfolio_VaR_list,\n",
    "        'Portfolio ES': portfolio_ES_list\n",
    "    })\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results_df['Date'], results_df['Portfolio VaR'], label='Portfolio VaR', color='red')\n",
    "    plt.plot(results_df['Date'], results_df['Portfolio ES'], label='Portfolio ES', color='blue')\n",
    "    plt.title('Portfolio VaR and ES Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "time_window_size = 500\n",
    "\n",
    "main_analysis(time_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only the column names and the first 3 rows of the main dataframe\n",
    "print(main_df.columns)\n",
    "print(main_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19740b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking assumption on taking mean = 0 \n",
    "# Calculate portfolio daily returns\n",
    "main_df['Portfolio_Daily_Returns'] = (\n",
    "    weights['S&P500'] * main_df['C_S&P500_Returns'] +\n",
    "    weights['DAX40'] * main_df['C_Dax40_Returns'] +\n",
    "    weights['NIKKEI'] * main_df['C_Nikkei_Returns'] +\n",
    "    weights['EU-BOND'] * main_df['Interest_Bond_daily_rate']\n",
    ")\n",
    "\n",
    "# make a graph of the portfolio daily returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(main_df['Date'], main_df['Portfolio_Daily_Returns'], label='Portfolio Daily Returns', color='blue')\n",
    "plt.title('Portfolio Daily Returns Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.show()\n",
    "\n",
    "# Calculate the mean of the returns for each stock over a subset of the data defined on the time range\n",
    "# Define the sample size\n",
    "sample_size = 500\n",
    "\n",
    "# Initialize variables to store the mean returns for each stock for each time window\n",
    "mean_sp500_returns_list = []\n",
    "mean_dax40_returns_list = []\n",
    "mean_nikkei_returns_list = []\n",
    "mean_eu_bond_returns_list = []\n",
    "\n",
    "# Loop through the data with a fixed sample size\n",
    "for i in range(sample_size, len(main_df)):\n",
    "    time_range_start = main_df['Date'].iloc[i - sample_size]\n",
    "    time_range_end = main_df['Date'].iloc[i]\n",
    "    time_range_df = main_df[(main_df['Date'] >= time_range_start) & (main_df['Date'] <= time_range_end)]\n",
    "    \n",
    "    # Calculate mean returns for each stock\n",
    "    mean_sp500_returns = time_range_df['C_S&P500_Returns'].mean()\n",
    "    mean_dax40_returns = time_range_df['C_Dax40_Returns'].mean()\n",
    "    mean_nikkei_returns = time_range_df['C_Nikkei_Returns'].mean()\n",
    "    mean_eu_bond_returns = time_range_df['Interest_Bond_daily_rate'].mean()\n",
    "    \n",
    "    # Append the results to the respective lists\n",
    "    mean_sp500_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean S&P500 Returns': mean_sp500_returns\n",
    "    })\n",
    "    mean_dax40_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean DAX40 Returns': mean_dax40_returns\n",
    "    })\n",
    "    mean_nikkei_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean Nikkei Returns': mean_nikkei_returns\n",
    "    })\n",
    "    mean_eu_bond_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean EU Bond Returns': mean_eu_bond_returns\n",
    "    })\n",
    "\n",
    "# Convert the results to DataFrames for easier analysis\n",
    "mean_sp500_returns_df = pd.DataFrame(mean_sp500_returns_list)\n",
    "mean_dax40_returns_df = pd.DataFrame(mean_dax40_returns_list)\n",
    "mean_nikkei_returns_df = pd.DataFrame(mean_nikkei_returns_list)\n",
    "mean_eu_bond_returns_df = pd.DataFrame(mean_eu_bond_returns_list)\n",
    "\n",
    "# Display the results\n",
    "display(mean_sp500_returns_df.head())\n",
    "display(mean_dax40_returns_df.head())\n",
    "display(mean_nikkei_returns_df.head())\n",
    "display(mean_eu_bond_returns_df.head())\n",
    "\n",
    "# Plot the mean returns for each stock over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_sp500_returns_df['End Date'], mean_sp500_returns_df['Mean S&P500 Returns'], label='Mean S&P500 Returns', color='blue')\n",
    "plt.plot(mean_dax40_returns_df['End Date'], mean_dax40_returns_df['Mean DAX40 Returns'], label='Mean DAX40 Returns', color='orange')\n",
    "plt.plot(mean_nikkei_returns_df['End Date'], mean_nikkei_returns_df['Mean Nikkei Returns'], label='Mean Nikkei Returns', color='green')\n",
    "plt.plot(mean_eu_bond_returns_df['End Date'], mean_eu_bond_returns_df['Mean EU Bond Returns'], label='Mean EU Bond Returns', color='red')\n",
    "plt.title('Mean Returns Over Time for Each Stock')\n",
    "plt.xlabel('End Date')\n",
    "plt.ylabel('Mean Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b52406",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Backtesting VaR and ES\n",
    "\n",
    "In this section, we perform backtesting on the calculated Value at Risk (VaR) and Expected Shortfall (ES) measures. Backtesting helps assess the accuracy and reliability of the risk models.\n",
    "\n",
    "We will:\n",
    "1.  **Calculate Violations:** Identify the days where the actual portfolio loss exceeded the predicted VaR.\n",
    "2.  **Compare Actual vs. Expected Violations (VaR):** Group violations by year and compare the observed number of violations against the number expected based on the confidence level (alpha).\n",
    "3.  **Compare Actual Shortfall vs. Predicted ES (ES):** For the days a violation occurred, compare the average actual loss (shortfall) against the predicted ES, grouped by year.\n",
    "4.  **Visualize Violations:** Plot the occurrences of violations over time to visually inspect for clustering or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e41335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_violations(actual_losses, var_predictions):\n",
    "    \"\"\"Checks for VaR violations.\"\"\"\n",
    "    return actual_losses > var_predictions\n",
    "\n",
    "def backtest_var(violations, alpha, dates):\n",
    "    \"\"\"Compares actual vs. expected VaR violations yearly.\"\"\"\n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date': # Ensure index is Date for grouping\n",
    "         violations = violations.set_index(dates)\n",
    "            \n",
    "    violations_df = pd.DataFrame({'Violations': violations, 'Year': violations.index.year})\n",
    "    yearly_violations = violations_df.groupby('Year')['Violations'].sum()\n",
    "    yearly_counts = violations_df.groupby('Year')['Violations'].count()\n",
    "    \n",
    "    expected_violations = yearly_counts * (1 - alpha)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Actual Violations': yearly_violations,\n",
    "        'Expected Violations': expected_violations,\n",
    "        'Total Observations': yearly_counts\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def backtest_es(actual_losses, violations, es_predictions, dates):\n",
    "    \"\"\"Compares actual average shortfall vs. predicted ES yearly.\"\"\"\n",
    "    # Ensure inputs are pandas Series with Date index\n",
    "    if not isinstance(actual_losses, pd.Series):\n",
    "        actual_losses = pd.Series(actual_losses, index=dates)\n",
    "    elif actual_losses.index.name != 'Date':\n",
    "        actual_losses = actual_losses.set_index(dates)\n",
    "        \n",
    "    if not isinstance(violations, pd.Series):\n",
    "        violations = pd.Series(violations, index=dates)\n",
    "    elif violations.index.name != 'Date':\n",
    "        violations = violations.set_index(dates)\n",
    "        \n",
    "    if not isinstance(es_predictions, pd.Series):\n",
    "        es_predictions = pd.Series(es_predictions, index=dates)\n",
    "    elif es_predictions.index.name != 'Date':\n",
    "        es_predictions = es_predictions.set_index(dates)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_Loss': actual_losses,\n",
    "        'Violation': violations,\n",
    "        'Predicted_ES': es_predictions,\n",
    "        'Year': actual_losses.index.year\n",
    "    })\n",
    "    \n",
    "    # Filter for violations\n",
    "    violation_data = results_df[results_df['Violation']]\n",
    "    \n",
    "    # Calculate yearly averages\n",
    "    yearly_avg_actual_shortfall = violation_data.groupby('Year')['Actual_Loss'].mean()\n",
    "    yearly_avg_predicted_es = violation_data.groupby('Year')['Predicted_ES'].mean()\n",
    "    yearly_violation_count = violation_data.groupby('Year').size()\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Avg Actual Shortfall': yearly_avg_actual_shortfall,\n",
    "        'Avg Predicted ES': yearly_avg_predicted_es,\n",
    "        'Violation Count': yearly_violation_count\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def plot_violations(violations, dates, title):\n",
    "    \"\"\"Plots VaR violations over time.\"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(dates, violations, 'ro', markersize=4, alpha=0.7, label='Violation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Violation (1=Yes, 0=No)')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "def run_backtesting(main_df, var_results_df, es_results_df):\n",
    "    \"\"\"Runs the backtesting process for VaR and ES models.\"\"\"\n",
    "    # Align main_df with var/es results (which start after the initial window)\n",
    "    backtest_dates = var_results_df['Date']\n",
    "    backtest_data = main_df[main_df['Date'].isin(backtest_dates)].set_index('Date')\n",
    "    actual_losses = backtest_data['Portfolio_loss']\n",
    "\n",
    "    # Confidence levels used\n",
    "    alphas = [0.95, 0.99]\n",
    "    alpha_indices = {0.95: 0, 0.99: 1} # Index mapping for results arrays\n",
    "\n",
    "    # Iterate through models (columns in var_results_df/es_results_df)\n",
    "    var_model_cols = [col for col in var_results_df.columns if col != 'Date']\n",
    "    es_model_cols = [col for col in es_results_df.columns if col != 'Date']\n",
    "\n",
    "    for i, model_name in enumerate(var_model_cols):\n",
    "        print(f\"\\n--- Backtesting for Model: {model_name} ---\")\n",
    "        \n",
    "        # Extract predictions for this model\n",
    "        # Need to handle the fact that predictions are stored as arrays [pred_95, pred_99]\n",
    "        var_preds_list = var_results_df[model_name].tolist()\n",
    "        # Ensure alignment between var and es model columns\n",
    "        if i < len(es_model_cols):\n",
    "            es_preds_list = es_results_df[es_model_cols[i]].tolist()\n",
    "        else:\n",
    "            print(f\"  Warning: No matching ES column found for {model_name}. Skipping ES backtest.\")\n",
    "            es_preds_list = None\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            alpha_idx = alpha_indices[alpha]\n",
    "            print(f\"\\nConfidence Level: {alpha*100}%\")\n",
    "            \n",
    "            # Extract predictions for the specific alpha\n",
    "            # Handle potential errors if data isn't as expected (e.g., not a list/array)\n",
    "            try:\n",
    "                var_predictions = pd.Series([p[alpha_idx] for p in var_preds_list], index=backtest_dates)\n",
    "                if es_preds_list:\n",
    "                    es_predictions = pd.Series([p[alpha_idx] for p in es_preds_list], index=backtest_dates)\n",
    "                else:\n",
    "                    es_predictions = None\n",
    "            except (TypeError, IndexError) as e:\n",
    "                print(f\"  Error extracting predictions for alpha={alpha}: {e}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # 1. Calculate Violations\n",
    "            violations = calculate_violations(actual_losses, var_predictions)\n",
    "            \n",
    "            # 2. Backtest VaR\n",
    "            var_summary = backtest_var(violations, alpha, backtest_dates)\n",
    "            print(\"\\nVaR Backtest Summary (Yearly):\")\n",
    "            display(var_summary)\n",
    "            \n",
    "            # 3. Backtest ES\n",
    "            if es_predictions is not None:\n",
    "                es_summary = backtest_es(actual_losses, violations, es_predictions, backtest_dates)\n",
    "                print(\"\\nES Backtest Summary (Yearly):\")\n",
    "                display(es_summary)\n",
    "            \n",
    "            # 4. Plot Violations\n",
    "            plot_violations(violations, backtest_dates, f'VaR Violations for {model_name} (alpha={alpha})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ad9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the backtesting function with the required dataframes\n",
    "run_backtesting(main_df, var_results_df, es_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bde3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fced84f1",
   "metadata": {},
   "source": [
    "### Backtesting Interpretation\n",
    "\n",
    "Review the tables and plots above:\n",
    "\n",
    "*   **VaR Backtest:** Compare 'Actual Violations' to 'Expected Violations' each year. Significant deviations might indicate issues with the VaR model's calibration. If actual violations consistently exceed expected, the model underestimates risk. If they are consistently lower, it might be too conservative.\n",
    "*   **ES Backtest:** Compare 'Avg Actual Shortfall' to 'Avg Predicted ES'. If the actual average shortfall during violations is consistently higher than the predicted ES, the model underestimates the severity of tail losses.\n",
    "*   **Violation Plots:** Look for patterns. Ideally, violations should be randomly distributed. Clustering of violations suggests the model fails to adapt quickly to changing market volatility (violation dependence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking assumption on taking mean = 0 \n",
    "# Calculate portfolio daily returns\n",
    "main_df['Portfolio_Daily_Returns'] = (\n",
    "    weights['S&P500'] * main_df['C_S&P500_Returns'] +\n",
    "    weights['DAX40'] * main_df['C_Dax40_Returns'] +\n",
    "    weights['NIKKEI'] * main_df['C_Nikkei_Returns'] +\n",
    "    weights['EU-BOND'] * main_df['Interest_Bond_daily_rate']\n",
    ")\n",
    "\n",
    "# make a graph of the portfolio daily returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(main_df['Date'], main_df['Portfolio_Daily_Returns'], label='Portfolio Daily Returns', color='blue')\n",
    "plt.title('Portfolio Daily Returns Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.show()\n",
    "\n",
    "# Calculate the mean of the returns for each stock over a subset of the data defined on the time range\n",
    "# Define the sample size\n",
    "sample_size = 500\n",
    "\n",
    "# Initialize variables to store the mean returns for each stock for each time window\n",
    "mean_sp500_returns_list = []\n",
    "mean_dax40_returns_list = []\n",
    "mean_nikkei_returns_list = []\n",
    "mean_eu_bond_returns_list = []\n",
    "\n",
    "# Loop through the data with a fixed sample size\n",
    "for i in range(sample_size, len(main_df)):\n",
    "    time_range_start = main_df['Date'].iloc[i - sample_size]\n",
    "    time_range_end = main_df['Date'].iloc[i]\n",
    "    time_range_df = main_df[(main_df['Date'] >= time_range_start) & (main_df['Date'] <= time_range_end)]\n",
    "    \n",
    "    # Calculate mean returns for each stock\n",
    "    mean_sp500_returns = time_range_df['C_S&P500_Returns'].mean()\n",
    "    mean_dax40_returns = time_range_df['C_Dax40_Returns'].mean()\n",
    "    mean_nikkei_returns = time_range_df['C_Nikkei_Returns'].mean()\n",
    "    mean_eu_bond_returns = time_range_df['Interest_Bond_daily_rate'].mean()\n",
    "    \n",
    "    # Append the results to the respective lists\n",
    "    mean_sp500_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean S&P500 Returns': mean_sp500_returns\n",
    "    })\n",
    "    mean_dax40_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean DAX40 Returns': mean_dax40_returns\n",
    "    })\n",
    "    mean_nikkei_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean Nikkei Returns': mean_nikkei_returns\n",
    "    })\n",
    "    mean_eu_bond_returns_list.append({\n",
    "        'Start Date': time_range_start,\n",
    "        'End Date': time_range_end,\n",
    "        'Mean EU Bond Returns': mean_eu_bond_returns\n",
    "    })\n",
    "\n",
    "# Convert the results to DataFrames for easier analysis\n",
    "mean_sp500_returns_df = pd.DataFrame(mean_sp500_returns_list)\n",
    "mean_dax40_returns_df = pd.DataFrame(mean_dax40_returns_list)\n",
    "mean_nikkei_returns_df = pd.DataFrame(mean_nikkei_returns_list)\n",
    "mean_eu_bond_returns_df = pd.DataFrame(mean_eu_bond_returns_list)\n",
    "\n",
    "# Display the results\n",
    "display(mean_sp500_returns_df.head())\n",
    "display(mean_dax40_returns_df.head())\n",
    "display(mean_nikkei_returns_df.head())\n",
    "display(mean_eu_bond_returns_df.head())\n",
    "\n",
    "# Plot the mean returns for each stock over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_sp500_returns_df['End Date'], mean_sp500_returns_df['Mean S&P500 Returns'], label='Mean S&P500 Returns', color='blue')\n",
    "plt.plot(mean_dax40_returns_df['End Date'], mean_dax40_returns_df['Mean DAX40 Returns'], label='Mean DAX40 Returns', color='orange')\n",
    "plt.plot(mean_nikkei_returns_df['End Date'], mean_nikkei_returns_df['Mean Nikkei Returns'], label='Mean Nikkei Returns', color='green')\n",
    "plt.plot(mean_eu_bond_returns_df['End Date'], mean_eu_bond_returns_df['Mean EU Bond Returns'], label='Mean EU Bond Returns', color='red')\n",
    "plt.title('Mean Returns Over Time for Each Stock')\n",
    "plt.xlabel('End Date')\n",
    "plt.ylabel('Mean Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
